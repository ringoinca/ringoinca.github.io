<!DOCTYPE html>
<!-- Akai (pandoc HTML5 template)
     designer:     soimort
     last updated: 2016-05-06
     last adapted: 2016-11-01 -->
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <meta name="author" content="Mort Yao">
    <meta name="dcterms.date" content="2017-01-11">
    <title>The Measurable Entropy</title>
    <link rel="canonical" href="https://www.soimort.org/mst/4">
    <style type="text/css">code { white-space: pre; }</style>
    <link rel="stylesheet" href="//cdn.soimort.org/normalize/5.0.0/normalize.min.css">
    <link rel="stylesheet" href="//cdn.soimort.org/fonts/latest/URW-Palladio-L.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/__/css/style.css">
    <link rel="stylesheet" href="/__/css/pygments.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
    <![endif]-->
    <script src="//cdn.soimort.org/jk/20160504/jk.min.js"></script>
    <script src="/__/js/main.js"></script>
    <link rel="icon" href="/favicon.png">
    <link rel="apple-touch-icon" href="/favicon.png">
    <link rel="alternate" type="application/atom+xml" href="/feed.atom">
  </head>
  <body>
    <article>
      <header>
        <h1 class="title"><a href="https://www.soimort.org/mst/4">The Measurable Entropy</a></h1>
        <h1 class="subtitle"><a href="https://www.soimort.org/mst/4">Maximum entropy, uniformity and normality.</a></h1>
        <address class="author">Mort Yao</address>
        <h3 class="date">2017-01-11</h3>
      </header>
      <div id="content">
<p>A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (quantitative properties about real-world data including central tendency, dispersion and shape). The maximization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known).</p>
<ul>
<li>What is the measure for <strong>information</strong>?
<ul>
<li>Intuitively, if a sample appears to be more naturally “random”, then it may contain more “information” of interest since it takes a greater size of data (more bits) to describe. But how to measure this quantitatively?</li>
<li>Probability-theoretic view: <em>Shannon entropy</em>.</li>
<li>Algorithmic view: <em>Kolmogorov complexity</em>. (TBD in February or March 2017)</li>
</ul></li>
<li><a href="https://wiki.soimort.org/info/">Basic information theory</a>
<ul>
<li><strong>Shannon entropy</strong>
<ul>
<li>For discrete random variable <span class="math inline">\(X\)</span> with pmf <span class="math inline">\(p(x)\)</span>: <span class="math display">\[\operatorname{H}(X) = -\sum_{x\in\mathcal{X}} p(x) \log p(x).\]</span></li>
<li>For continuous random variable <span class="math inline">\(X\)</span> with pdf <span class="math inline">\(f(x)\)</span>: <span class="math display">\[\operatorname{H}(X) = -\int_\mathcal{X} f(x) \log f(x) dx.\]</span> (also referred to as <em>differential entropy</em>)</li>
<li>The notion of entropy is an extension of the one in statistical thermodynamics (Gibbs entropy) and the <a href="https://wiki.soimort.org/math/dynamical-systems/ergodic/">measure-theoretic entropy of dynamical systems</a>.</li>
<li>Obviously, the entropy is determined by the pmf/pdf, which depends on the parameters of the specific probability distribution.</li>
<li>In the context of Computer Science, the logarithm in the formula is often taken to the base <span class="math inline">\(2\)</span>. Assume that we take a uniform binary string of length <span class="math inline">\(\ell\)</span>, then <span class="math display">\[p(x) = 2^{-\ell}\]</span> Thus, the entropy of the distribution is <span class="math display">\[\operatorname{H}(X) = -\sum_{x\in\mathcal{X}} p(x) \log p(x) = - (2^\ell \cdot 2^{-\ell} \log_2 2^{-\ell}) = \ell\]</span> which is just the length of this (<span class="math inline">\(\ell\)</span>-bit) binary string. Therefore, the unit of information (when applying binary logarithm) is often called a <em>bit</em> (also <em>shannon</em>).</li>
<li>For the <strong>joint entropy</strong> <span class="math inline">\(\operatorname{H}(X,Y)\)</span> and the <strong>conditional entropy</strong> <span class="math inline">\(\operatorname{H}(X\,|\,Y)\)</span>, the following equation holds: <span class="math display">\[\operatorname{H}(X,Y) = \operatorname{H}(X\,|\,Y) + \operatorname{H}(Y) = \operatorname{H}(Y\,|\,X) + \operatorname{H}(X)\]</span> Notice that if <span class="math inline">\(\operatorname{H}(X\,|\,Y) = \operatorname{H}(X)\)</span>, then <span class="math inline">\(\operatorname{H}(X,Y) = \operatorname{H}(X) + \operatorname{H}(Y)\)</span> and <span class="math inline">\(\operatorname{H}(Y\,|\,X) = \operatorname{H}(Y)\)</span>. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be independent of each other in this case.</li>
<li><strong>Mutual information</strong> <span class="math inline">\(\operatorname{I}(X;Y) = \operatorname{H}(X) + \operatorname{H}(Y) - \operatorname{H}(X,Y) \geq 0\)</span> (equality holds iff <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent). Unlike the joint entropy or the conditional entropy, this notion does not reflect an actual probabilistic event thus it is referred to as <em>information</em> (sometimes <em>correlation</em>) rather than entropy.</li>
</ul></li>
<li><strong>Kullback-Leibler divergence (relative entropy)</strong> <span class="math display">\[\operatorname{D}_\mathrm{KL}(p\|q) = \sum_{x\in\mathcal{X}} p(x) \log \frac{p(x)}{q(x)}\]</span> KL divergence is a measurement of the distance of two probability distributions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>.
<ul>
<li>If <span class="math inline">\(p = q\)</span>, <span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) = 0\)</span>. (Any distribution has a KL divergence of 0 with itself.)</li>
<li><span class="math inline">\(\operatorname{I}(X;Y) = \operatorname{D}_\mathrm{KL}(p(x,y)\|p(x)p(y))\)</span>.</li>
<li><strong>Cross entropy</strong> <span class="math display">\[\operatorname{H}(p,q) = \operatorname{H}(p) + \operatorname{D}_\mathrm{KL}(p\|q)\]</span> Notice that cross entropy is defined on two distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> rather than two random variables taking one distribution <span class="math inline">\(p\)</span> (given by joint entropy <span class="math inline">\(\operatorname{H}(X,Y)\)</span>).</li>
</ul></li>
</ul></li>
<li>Basic probability theory
<ul>
<li><a href="https://wiki.soimort.org/math/probability/distributions/normal/">Normal (Gaussian) distribution</a>.
<ul>
<li>(Univariate) <span class="math inline">\(X \sim \mathcal{N}(\mu,\sigma^2)\)</span> <span class="math display">\[f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span> where <span class="math inline">\(\mu\)</span> is the mean, <span class="math inline">\(\sigma^2\)</span> is the variance of the distribution.<br />
(Multivariate) <span class="math inline">\(\boldsymbol x \sim \mathcal{N}_k(\boldsymbol\mu,\mathbf\Sigma)\)</span> <span class="math display">\[f(\boldsymbol x) = (2\pi)^{-k/2} |\mathbf\Sigma|^{-1/2} e^{-\frac{1}{2} (\boldsymbol x - \boldsymbol\mu)^\mathrm{T} \Sigma^{-1}(\boldsymbol x - \boldsymbol\mu)}\]</span> where <span class="math inline">\(\boldsymbol\mu\)</span> is the mean vector, <span class="math inline">\(\mathbf\Sigma\)</span> is the covariance matrix of the distribution.</li>
<li><em>Maximum entropy</em>: normal distribution is the probability distribution that maximizes the entropy when the mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span> are fixed.</li>
<li>The normal distribution does not have any shape parameter. Moreover, its skewness and excess kurtosis are always 0.</li>
</ul></li>
</ul></li>
<li><a href="https://wiki.soimort.org/math/statistics/">Basic descriptive statistics</a>
<ul>
<li><em>Descriptive statistics</em> describe the properties of data sets quantitatively, without making any further inference.</li>
<li><em>Population</em> vs. <em>sample</em>.</li>
<li>Three major descriptive statistics:
<ol type="1">
<li><em>Central tendency</em>: sample means (<strong>arithmetic mean</strong> <span class="math inline">\(\mu\)</span>, geometric, harmonic), <strong>median</strong>, <strong>mode</strong>, mid-range.
<ul>
<li>Arithmetic mean is an unbiased estimator of the population mean (expectation).</li>
<li>Median and mode are most robust in the presence of outliers.</li>
</ul></li>
<li><em>Dispersion (or variability)</em>: minimum, maximum, range, IQR (interquartile range), maximum absolute deviation, MAD (mean absolute deviation), <strong>sample variance</strong> <span class="math inline">\(s^2\)</span> with Bessel’s correction, <strong>CV (coefficient of variance)</strong>, <strong>VMR (index of dispersion)</strong>.
<ul>
<li>IQR and MAD are robust in the presence of outliers.</li>
<li>Sample variance (with Bessel’s correction) is an unbiased estimator of the population variance.</li>
<li>CV and VMR are sample standard deviation and sample variance normalized by the mean respectively, thus they are sometimes called <em>relative standard deviation</em> and <em>relative variance</em>; they are <em>not</em> unbiased though.</li>
</ul></li>
<li><em>Shape</em>: sample skewness, sample excess kurtosis.
<ul>
<li>These statistics show how a sample deviates from normality, since the skewness and the excess kurtosis of a normal distribution are 0. The estimators could vary under different circumstances.</li>
</ul></li>
</ol></li>
</ul></li>
</ul>
<section id="entropy-as-a-measure" class="level2">
<h2>Entropy as a measure<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></h2>
<p>For random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we define sets <span class="math inline">\(\tilde X\)</span> and <span class="math inline">\(\tilde Y\)</span>. Then the information entropy <span class="math inline">\(\operatorname{H}\)</span> may be viewed as a signed measure <span class="math inline">\(\mu\)</span> over <a href="https://wiki.soimort.org/math/set/">sets</a>: <span class="math display">\[\begin{align*}
\operatorname{H}(X) &amp;= \mu(\tilde X) \\
\operatorname{H}(Y) &amp;= \mu(\tilde Y) \\
\operatorname{H}(X,Y) &amp;= \mu(\tilde X \cup \tilde Y) \qquad \text{(Joint entropy is the measure of a set union)} \\
\operatorname{H}(X\,|\,Y) &amp;= \mu(\tilde X \setminus \tilde Y) \qquad \text{(Conditional entropy is the measure of a set difference)} \\
\operatorname{I}(X;Y) &amp;= \mu(\tilde X \cap \tilde Y) \qquad \text{(Mutual information is the measure of a set intersection)}
\end{align*}\]</span> The inclusion–exclusion principle: <span class="math display">\[\begin{align*}
\operatorname{H}(X,Y) &amp;= \operatorname{H}(X) + \operatorname{H}(Y) - \operatorname{I}(X;Y) \\
\mu(\tilde X \cup \tilde Y) &amp;= \mu(\tilde X) + \mu(\tilde Y) - \mu(\tilde X \cap \tilde Y)
\end{align*}\]</span> Bayes’ theorem: <span class="math display">\[\begin{align*}
\operatorname{H}(X\,|\,Y) &amp;= \operatorname{H}(Y\,|\,X) + \operatorname{H}(X) - \operatorname{H}(Y) \\
\mu(\tilde X \setminus \tilde Y) &amp;= \mu(\tilde Y \setminus \tilde X) + \mu(\tilde X) - \mu(\tilde Y)
\end{align*}\]</span></p>
</section>
<section id="entropy-and-data-coding" class="level2">
<h2>Entropy and data coding</h2>
<p><em>Absolute entropy (Shannon entropy)</em> quantifies how much information is contained in some data. For data compression, the entropy gives the minimum size that is needed to reconstruct original data (losslessly). Assume that we want to store a random binary string of length <span class="math inline">\(\ell\)</span> (by “random”, we do not have yet any prior knowledge on what data to be stored). Under the <em>principle of maximum entropy</em>, the entropy of its distribution <span class="math inline">\(p(x)\)</span> should be maximized: <span class="math display">\[\max \operatorname{H}(X) = \max \left\{ -\sum_{x\in\mathcal{X}} p(x) \log p(x) \right\}\]</span> given the only constraint <span class="math display">\[\sum_{x\in\mathcal{X}} p(x) = 1\]</span> Let <span class="math inline">\(\lambda\)</span> be the Lagrange multiplier, set <span class="math display">\[\mathcal{L} = - \sum_{x\in\mathcal{X}} p(x) \log p(x) - \lambda\left( \sum_{x\in\mathcal{X}} p(x) - 1 \right)\]</span> We get <span class="math display">\[\begin{align*}
\frac{\partial\mathcal{L}}{\partial x} = -p(x)(\log p(x) + 1 + \lambda) &amp;= 0 \\
\log p(x) &amp;= - \lambda - 1 \\
p(x) &amp;= c \qquad \text{(constant)}
\end{align*}\]</span> That is, the <a href="https://wiki.soimort.org/math/probability/#discrete-uniform-distribution">discrete uniform distribution</a> maximizes the entropy for a random string. Since <span class="math inline">\(|\mathcal{X}| = 2^\ell\)</span>, we have <span class="math inline">\(p(x) = 2^{-\ell}\)</span> and <span class="math inline">\(\operatorname{H}(X) = -\sum_{x\in\mathcal{X}} 2^{-\ell} \log_2 2^{-\ell} = \ell\)</span> (bits). We conclude that the information that can be represented in a <span class="math inline">\(\ell\)</span>-bit string is at most <span class="math inline">\(\ell\)</span> bits. Some practical results include</p>
<ul>
<li>In general, pseudorandom data (assume no prior knowledge) cannot be losslessly compressed, e.g., the uniform key used in one-time pad must have <span class="math inline">\(\log_2 |\mathcal{M}|\)</span> bits (lower bound) so as not to compromise the perfect secrecy. (Further topic: <em>Shannon’s source coding theorem</em>)</li>
<li>Fully correct encoding/decoding of data, e.g., <span class="math inline">\(\mathsf{Enc}(m)\)</span> and <span class="math inline">\(\mathsf{Dec}(c)\)</span> algorithms in a private-key encryption scheme, must ensure that the probability distributions of <span class="math inline">\(m \in \mathcal{M}\)</span> and <span class="math inline">\(c \in \mathcal{C}\)</span> have the same entropy.</li>
<li>An algorithm with finite input cannot generate randomness infinitely. Consider a circuit that takes the encoded algorithm with some input (<span class="math inline">\(\ell\)</span> bits in total) and outputs some randomness, the entropy of the output data is at most <span class="math inline">\(\ell\)</span> bits. (Further topic: <em>Kolmogorov complexity</em>)</li>
</ul>
<p><em>Relative entropy (KL divergence)</em> quantifies how much information diverges between two sets of data. For data differencing, the KL divergence gives the minimum patch size that is needed to reconstruct target data (with distribution <span class="math inline">\(p(x)\)</span>) given source data (with distribution <span class="math inline">\(q(x)\)</span>).</p>
<p>In particular, if <span class="math inline">\(p(x) = q(x)\)</span>, which means that the two distributions are identical, we have <span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) = 0\)</span>. This follows our intuition that no information is gained or lost during data encoding/decoding. If <span class="math inline">\(p(x_0) = 0\)</span> at <span class="math inline">\(x=x_0\)</span>, we take <span class="math inline">\(p(x) \log \frac{p(x)}{q(x)} = 0\)</span>, to justify the fact that the target data is trivial to reconstruct at this point, no matter how much information <span class="math inline">\(q(x)\)</span> contains. However, if <span class="math inline">\(q(x_0) = 0\)</span> at <span class="math inline">\(x=x_0\)</span>, we should take <span class="math inline">\(p(x) \log \frac{p(x)}{q(x)} = \infty\)</span>, so that the target data is impossible to reconstruct if we have only trivial <span class="math inline">\(q(x)\)</span> at some point (unless <span class="math inline">\(p(x_0) = 0\)</span>).</p>
<p><strong>Lemma 4.1. (Gibbs’ inequality)</strong><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> <em>The KL divergence is always non-negative: <span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) \geq 0\)</span>.</em></p>
<p>Informally, Lemma 4.1 simply states that in order to reconstruct target data from source data, either more information (<span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) &gt; 0\)</span>) or no further information (<span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) = 0\)</span>) is needed.</p>
</section>
<section id="maximum-entropy-and-normality" class="level2">
<h2>Maximum entropy and normality</h2>
<p><strong>Theorem 4.2.</strong> <em>Normal distribution <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span> maximizes the differential entropy for given mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</em></p>
<p><strong>Proof.</strong><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Let <span class="math inline">\(g(x)\)</span> be a pdf of the normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(f(x)\)</span> be an arbitrary pdf with the same mean and variance.</p>
<p>Consider the KL divergence between <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span>. By Lemma 4.1 (Gibbs’ inequality): <span class="math display">\[\operatorname{D}_\mathrm{KL}(f\|g) = \int_{-\infty}^\infty f(x) \log \frac{f(x)}{g(x)} dx = \operatorname{H}(f,g) - \operatorname{H}(f) \geq 0\]</span></p>
<p>Notice that <span class="math display">\[\begin{align*}
\operatorname{H}(f,g) &amp;= - \int_{-\infty}^\infty f(x) \log g(x) dx \\
&amp;= - \int_{-\infty}^\infty f(x) \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \right) dx \\
&amp;= \frac{1}{2} \left( \log(2\pi\sigma^2) + 1 \right) \\
&amp;= \operatorname{H}(g)
\end{align*}\]</span> Therefore, <span class="math display">\[\operatorname{H}(g) \geq \operatorname{H}(f)\]</span> That is, the distribution of <span class="math inline">\(g(x)\)</span> (Gaussian) always has the maximum entropy. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>It is also possible to derive the normal distribution directly from the principle of maximum entropy, under the constraint such that <span class="math inline">\(\int_{-\infty}^\infty (x-\mu)^2f(x)dx = \sigma^2\)</span>.</p>
<p>The well-known central limit theorem (CLT) which states that the sum of independent random variables <span class="math inline">\(\{X_1,\dots,X_n\}\)</span> tends toward a normal distribution may be alternatively expressed as the monotonicity of the entropy of the normalized sum: <span class="math display">\[\operatorname{H}\left( \frac{\sum_{i=1}^n X_i}{\sqrt{n}} \right)\]</span> which is an increasing function of <span class="math inline">\(n\)</span>. <span class="citation" data-cites="artstein2004solution">[1]</span></p>
</section>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Books:</strong></p>
<p>T. M. Cover and J. A. Thomas. <em>Elements of Information Theory</em>, 2nd ed.</p>
<p><strong>Papers:</strong></p>
<div id="refs" class="references">
<div id="ref-artstein2004solution">
<p>[1] S. Artstein, K. Ball, F. Barthe, and A. Naor, “Solution of shannon’s problem on the monotonicity of entropy,” <em>Journal of the American Mathematical Society</em>, vol. 17, no. 4, pp. 975–982, 2004. </p>
</div>
</div>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Information_theory_and_measure_theory" class="uri">https://en.wikipedia.org/wiki/Information_theory_and_measure_theory</a><a href="#fnref1" class="footnoteBack">↩</a></p></li>
<li id="fn2"><p><a href="https://en.wikipedia.org/wiki/Gibbs&#39;_inequality" class="uri">https://en.wikipedia.org/wiki/Gibbs'_inequality</a><a href="#fnref2" class="footnoteBack">↩</a></p></li>
<li id="fn3"><p><a href="https://en.wikipedia.org/wiki/Differential_entropy#Maximization_in_the_normal_distribution" class="uri">https://en.wikipedia.org/wiki/Differential_entropy#Maximization_in_the_normal_distribution</a><a href="#fnref3" class="footnoteBack">↩</a></p></li>
</ol>
</section>
      </div>
      <!-- (www.soimort.org) last updated: 2016-05-07 -->
      <aside id="soimort-toolbar">
        <a href="/"><i class="fa fa-home" aria-hidden="true"></i></a>
      </aside>
    </article>
  </body>
</html>
