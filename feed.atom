<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Through the Looking-Glass</title>
  <subtitle>Mort’s random homepage.</subtitle>
  <link rel="alternate" type="text/html" href="https://www.soimort.org/" />
  <link rel="self" type="application/atom+xml" href="https://www.soimort.org/feed.atom" />
  <id>tag:www.soimort.org,2017:/</id>
  <updated>2017-11-09T00:00:00+01:00</updated>
  <author>
    <name>Mort Yao</name>
    <email>soi@mort.ninja</email>
  </author>

  <entry>
    <title>Underrated Gems (1)</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/notes/171109" />
    <id>tag:www.soimort.org,2017:/notes/171109</id>
    <published>2017-11-09T00:00:00+01:00</published>
    <updated>2017-11-11T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="logic" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p><small></p>
<blockquote>
<p>In a listless afternoon I questioned myself: What are some of the most neglected theorems that deserve more attention? Many famous old theorems in mathematics as I recalled, are about some kinds of <em>impossibilities</em>: <span class="math inline">\(\sqrt{2}\)</span> is irrational, <span class="math inline">\(\pi\)</span> is transcendental, polynomial equations of degree 5 or higher are generally insolvable, squaring the circle is impossible, Fermat’s last theorem, etc; by arguing about impossibility they’ve landscaped lots of math subfields in a revolutionary way. Compared to mathematics, formal logic is a relatively young field, not really fruitful before Frege, and the study on the theory of computation is even less cultivated, with quite a bunch of landmarking results yet underrated or poorly informed. Among those neglected theorems in logic and computation, I decided to do a write-up on each one I ever heard of, to help myself understand them better. Undoubtedly, they ought to be brought to the limelight and made better understood by working mathematicians and programmers nowadays. </small></p>
</blockquote>
<hr />
<p>It should be put in the first paragraph of the preface to <em><a href="https://en.wikipedia.org/wiki/Paul_Erdős#Personality">The Book</a> on logic</em>, if there could be one, that <em>truth in any language cannot be defined in the same language</em>. We have the classical <em>liar’s sentence</em> which says that “This sentence is false”. If this kind of inconsistency does not seem too disastrous to bug you yet, as you may guarantee not to involve yourself into any circular reasoning, then think about the following <em>truth-teller’s sentence</em>:</p>
<p style="text-align:center !important;text-indent:0 !important">If this sentence is true, then God exists.</p>
<p>Symbolically,</p>
<p><span class="math display">\[\sigma = (\text{True}(\sigma) \to \exists x\,\text{Godlike}(x))
\]</span></p>
<p>If you take this sentence to be false, then since it is a material conditional the only way it can ever be false is that the antecedent (<span class="math inline">\(\text{True}(\sigma)\)</span>) is true but the consequent (<span class="math inline">\(\exists x\,\text{Godlike}(x)\)</span>) is false. However, from that <span class="math inline">\(\text{True}(\sigma)\)</span> is true we would immediately get that <span class="math inline">\(\sigma\)</span> is true, which contradicts our assumption that <span class="math inline">\(\sigma\)</span> is false. On the other hand, if you take this sentence to be true, then everything should work perfectly: <span class="math inline">\(\text{True}(\sigma)\)</span> is true, <span class="math inline">\(\exists x\,\text{Godlike}(x)\)</span> is also true, therefore the whole sentence <span class="math inline">\(\sigma\)</span> is true, which just verifies the fact that <span class="math inline">\(\text{True}(\sigma)\)</span> is true. Therefore, to maintain logical consistency in our reasoning, the only choice for a rational person is to accept the truth of <span class="math inline">\(\exists x\,\text{Godlike}(x)\)</span>, i.e., “God exists.”</p>
<p>This is not really about the ontological argument of God’s existence; it’s about how the inherent inconsistency of an unrestricted language can lead to any random truth trivially (<em>ex contradictione quodlibet</em>). If you believe that the above argument is all reasonable and God indeed exists, try yourself on the following sentence:</p>
<p style="text-align:center !important;text-indent:0 !important">If this sentence is true, then Satan rules the world.</p>
<p>Let’s say, get it over since natural language is so tricky and facile but no one is really hurt by its expressive power.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> The sure thing is, no serious mathematician wants to derive ridiculous random facts (e.g., if some self-referential proposition is true, then “<span class="math inline">\(2 + 2 = 3\)</span>”), so in a <em>formal language</em> we are obliged to have a metatheorem like:</p>
<p><strong>Undefinability theorem (Tarski 1936).</strong> Let <span class="math inline">\(\mathcal{L}\)</span> be a formal language, and let <span class="math inline">\(\mathfrak{A}\)</span> be an <span class="math inline">\(\mathcal{L}\)</span>-structure, with its theory <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span> capable of representing all computable functions. A Gödel numbering <span class="math inline">\(\mathfrak{g}(\phi)\)</span> is defined for every <span class="math inline">\(\mathcal{L}\)</span>-formula <span class="math inline">\(\phi\)</span>. There is no formula <span class="math inline">\(\text{True}(n)\)</span> such that for every <span class="math inline">\(\mathcal{L}\)</span>-formula <span class="math inline">\(\varphi\)</span>, <span class="math inline">\(\text{True}(\mathfrak{g}(\varphi)) \leftrightarrow \varphi\)</span>.</p>
<p>Before proceeding with the proof of Tarski’s undefinability theorem, we get our terminology straight first and prove a very important lemma, namely the <em>diagonal lemma</em>. The definitions of language, structure and theory are already made in <a href="/mst/7/">Mst. #7</a>. The capability of representing all computable functions can be informally understood as, for every Turing-computable function <span class="math inline">\(f : x \mapsto y\)</span>, there is an <span class="math inline">\(\mathcal{L}\)</span>-formula <span class="math inline">\(\gamma_f\)</span> such that <span class="math inline">\(\gamma_f(x, y) \in \operatorname{Th}\mathfrak{A} \leftrightarrow y = f(x)\)</span>.</p>
<p>A Gödel numbering in a formal language <span class="math inline">\(\mathcal{L}\)</span> is an injective function <span class="math inline">\(\mathfrak{g} : \Phi \to \mathbb{N}\)</span> that maps every <span class="math inline">\(\mathcal{L}\)</span>-formula to a unique natural number <span class="math inline">\(n \in \mathbb{N}\)</span>. It may be defined in any way that satisfies the desired injective property. For example, in a first-order language <span class="math inline">\(\mathcal{L}_\text{Set}\)</span> consisting of only variables <span class="math inline">\(x_i\)</span> (<span class="math inline">\(i = 0, 1, 2, \dots\)</span>), two connective symbols <span class="math inline">\(\{ \to, \neg \}\)</span>, the equality symbol <span class="math inline">\(=\)</span>, the quantifier symbol <span class="math inline">\(\forall\)</span> and a predicate symbol <span class="math inline">\(\in\)</span>, one possible Gödel numbering function may be defined inductively as</p>
<span class="math display">\[\begin{align*}
\mathfrak{g}(x_i) &amp;= 2^0 \cdot 3^i \\
\mathfrak{g}(= t_1 t_2) &amp;= 2^1 \cdot 3^{\mathfrak{g}(t_1)} \cdot 5^{\mathfrak{g}(t_2)} \\
\mathfrak{g}(\in t_1 t_2) &amp;= 2^2 \cdot 3^{\mathfrak{g}(t_1)} \cdot 5^{\mathfrak{g}(t_2)} \\
\mathfrak{g}((\neg \psi)) &amp;= 2^3 \cdot 3^{\mathfrak{g}(\psi)} \\
\mathfrak{g}((\psi \to \theta)) &amp;= 2^4 \cdot 3^{\mathfrak{g}(\psi)} \cdot 5^{\mathfrak{g}(\theta)} \\
\mathfrak{g}(\forall x_i \psi) &amp;= 2^5 \cdot 3^i \cdot 5^{\mathfrak{g}(\psi)} \\
\end{align*}\]</span>
<p style="text-align:right !important;text-indent:0 !important">(*)</p>
<p><strong>Claim 1.</strong> (*) defines a Gödel numbering <span class="math inline">\(\mathfrak{g}\)</span> on the language <span class="math inline">\(\mathcal{L}_\text{Set}\)</span>. That is, for any <span class="math inline">\(\mathcal{L}_\text{Set}\)</span>-formulas <span class="math inline">\(\varphi_1\)</span> and <span class="math inline">\(\varphi_2\)</span>, if <span class="math inline">\(\mathfrak{g}(\varphi_1) = \mathfrak{g}(\varphi_2)\)</span>, then <span class="math inline">\(\varphi_1 \cong \varphi_2\)</span>.</p>
<p><strong>Proof idea.</strong> By structural induction on the formula and the <a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_arithmetic">fundamental theorem of arithmetic</a> (every integer greater than 1 has a unique prime-factorization).</p>
<p><strong>Example 2.</strong> The Gödel number of formula <span class="math inline">\(\forall x_0 (\neg \in x_0 x_0)\)</span> using (*) is</p>
<span class="math display">\[\begin{align*}
\mathfrak{g}(\forall x_0 (\neg \in x_0 x_0))
&amp;= 2^5 \cdot 3^0 \cdot 5^{\mathfrak{g}(\neg \in x_0 x_0)}
= 2^5 \cdot 3^0 \cdot 5^{2^3 \cdot 3^{\mathfrak{g}(\in x_0 x_0)}}
= 2^5 \cdot 3^0 \cdot 5^{2^3 \cdot 3^{2^2 \cdot 3^{2^0 \cdot 3^0} \cdot 5^{2^0 \cdot 3^0}}} \\
&amp;= 2^5 \cdot 3^0 \cdot 5^{2^3 \cdot 3^{60}} \\
&amp;= 32 \cdot 5^{339129266201729628114355465608}
\end{align*}\]</span>
<p>Note that the choice of our numbering function is purely technical; it doesn’t make any difference if we choose another set of prime numbers <span class="math inline">\(\{5, 7, 11\}\)</span> instead of <span class="math inline">\(\{2, 3, 5\}\)</span> as basis, as long as the decomposition of a Gödel number is unique. (Of course, such numbers would be even bigger.)</p>
<p>So, what is the whole point of this numbering thing, you might ask? As is shown in Example 2, a simple formula can yield a Gödel number which is insanely large (no kidding, even <em>much</em> greater than the total number of atoms in the universe!). We don’t actually use these numbers; we construct them just to convince ourselves that we <em>can</em> encode virtually any theory into the theory of arithmetic, or literally, strings into natural numbers. The overwhelming cost (complexity) is not of our concern for now. Hence, from now on, we can virtually reduce any problem in a formal language (set theory, type theory, analysis, geometry, …) to an arithmetical problem, i.e., a problem simply about manipulating and deciding on natural numbers.</p>
<p><strong>Diagonal lemma (Carnap 1934).</strong> Let <span class="math inline">\(\mathcal{L}_\text{A}\)</span> be a first-order language of arithmetic, and let <span class="math inline">\(\mathfrak{A}\)</span> be an <span class="math inline">\(\mathcal{L}_\text{A}\)</span>-structure, with its theory <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span> capable of representing all computable functions. Let <span class="math inline">\(\phi\)</span> be an <span class="math inline">\(\mathcal{L}_\text{A}\)</span>-formula with one free variable. There is a sentence <span class="math inline">\(\psi\)</span> such that <span class="math inline">\(\psi \leftrightarrow \phi(\mathfrak{g}(\psi))\)</span> in <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span>.</p>
<p><strong>Proof.</strong> Let <span class="math inline">\(f\)</span> be a total function defined on <span class="math inline">\(|\mathfrak{A}| = \mathbb{N}\)</span>: <span class="math display">\[f(\mathfrak{g}(\theta)) = \mathfrak{g}(\theta(\mathfrak{g}(\theta)))\]</span> for each <span class="math inline">\(\mathcal{L}_\text{A}\)</span>-formula <span class="math inline">\(\theta\)</span> with one free variable; otherwise, define <span class="math inline">\(f(n) = 0\)</span>.</p>
<p>Since <span class="math inline">\(f\)</span> is a computable function, there is a formula <span class="math inline">\(\gamma_f\)</span> representing <span class="math inline">\(f\)</span> in <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span>. That is, for each formula <span class="math inline">\(\theta\)</span>, we have in <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span> <span class="math display">\[\forall y\,(\gamma_f(\mathfrak{g}(\theta), y) \leftrightarrow y = f(\mathfrak{g}(\theta)))\]</span> which is just <span class="math display">\[\forall y\,(\gamma_f(\mathfrak{g}(\theta), y) \leftrightarrow y = \mathfrak{g}(\theta(\mathfrak{g}(\theta))))\]</span> Define the formula <span class="math inline">\(\beta\)</span> (with one free variable) as <span class="math display">\[\beta(z) = \forall y\,(\gamma_f(z,y) \to \phi(y))\]</span> Then we have in <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span> <span class="math display">\[\beta(\mathfrak{g}(\theta)) \leftrightarrow \forall y\,(y = \mathfrak{g}(\theta(\mathfrak{g}(\theta))) \to \phi(y))\]</span> which is just <span class="math display">\[\beta(\mathfrak{g}(\theta)) \leftrightarrow \phi(\mathfrak{g}(\theta(\mathfrak{g}(\theta))))\]</span> Since <span class="math inline">\(\theta\)</span> can be any <span class="math inline">\(\mathcal{L}_\text{A}\)</span>-formula, let <span class="math inline">\(\theta = \beta\)</span>. Then we have in <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span> <span class="math display">\[\beta(\mathfrak{g}(\beta)) \leftrightarrow \phi(\mathfrak{g}(\beta(\mathfrak{g}(\beta))))\]</span> Let <span class="math inline">\(\psi = \beta(\mathfrak{g}(\beta))\)</span>. Thus we have in <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span> <span class="math display">\[\psi \leftrightarrow \phi(\mathfrak{g}(\psi))\]</span> Therefore we obtain <span class="math inline">\(\psi\)</span> which is the required sentence (“fixed point”). <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>Now we are all set to prove the undefinability theorem:</p>
<p><strong>Proof.</strong> By reductio ad absurdum: Suppose that there is an <span class="math inline">\(\mathcal{L}\)</span>-formula <span class="math inline">\(\text{True}(n)\)</span> such that for every <span class="math inline">\(\mathcal{L}\)</span>-formula <span class="math inline">\(\varphi\)</span>, <span class="math inline">\(\text{True}(\mathfrak{g}(\varphi)) \leftrightarrow \varphi\)</span>. In particular, if <span class="math inline">\(\sigma\)</span> is a sentence, then <span class="math inline">\(\text{True}(\mathfrak{g}(\sigma))\)</span> holds in <span class="math inline">\(\mathfrak{A}\)</span> iff <span class="math inline">\(\sigma\)</span> is true in <span class="math inline">\(\mathfrak{A}\)</span>. Thus for all <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\text{True}(\mathfrak{g}(\sigma)) \leftrightarrow \sigma\)</span> holds in <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span>. However, by the diagonal lemma, there is also a sentence <span class="math inline">\(\psi\)</span> such that <span class="math inline">\(\psi \leftrightarrow \neg \text{True}(\mathfrak{g}(\mathfrak{\psi}))\)</span> in <span class="math inline">\(\operatorname{Th}\mathfrak{A}\)</span>. That is a contradiction. Therefore, such a formula <span class="math inline">\(\text{True}(n)\)</span> does not exist. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>Tarski’s undefinability theorem, along with the auxiliary diagonal lemma, showed some really fundamental limitative results in formal logic, and even more. The diagonal lemma obtained its name from the ingenious <a href="https://ncatlab.org/nlab/show/Cantor%27s+theorem">Cantor’s diagonal argument</a>, which reveals the well-known fact that the set <span class="math inline">\(\mathbb{R}\)</span> of all real numbers is uncountable. The similar trick of diagonalization has been exploited in multiple contextually different but essentially interwoven arguments, e.g., Gödel’s incompleteness theorems, and that the halting problem is undecidable. The category-theoretic generalization of this lemma is known as <a href="https://ncatlab.org/nlab/show/Lawvere%27s+fixed+point+theorem">Lawvere’s fixed-point theorem</a>, applicable in any cartesian closed category.</p>
<p>Let’s consider an informal but intuitive connection between the undefinability theorem and the (renowned) incompleteness theorem. Given the fact that the universal truth-telling formula <span class="math inline">\(\text{True}(n)\)</span> is undefinable in a formal language, one may wonder, “If some statement is true, then by the completeness theorem of first-order logic it can always be proven true. But if truth is not even definable at all, how do I know if any statement could as well be <em>proven false</em>?”</p>
<p>Here’s the thing. We don’t. If we do, we would be able to tell truth from falsehood completely and <em>provably</em>. That is to say, for every formula <span class="math inline">\(\varphi\)</span> in our language, there would be either a proof (<span class="math inline">\(\vdash \varphi\)</span>) or a disproof (<span class="math inline">\(\vdash \varphi \to \bot\)</span> or <span class="math inline">\(\vdash \neg \varphi\)</span>) of it; the class of all such proofs would just make a perfect <span class="math inline">\(\text{True}(n)\)</span>, while <span class="math inline">\(\text{True}(\mathfrak{g}(\varphi))\)</span> holds if and only if <span class="math inline">\(\varphi\)</span> has a proof! That would contradict the undefinability theorem. So necessarily, we must take for granted that there could be some falsehood in a logical system (which is powerful enough to represent all computable functions via Gödel numbering, e.g., a number theory of elementary arithmetic) of which existence we can never disprove (because otherwise we could have defined that <span class="math inline">\(\text{True}(n)\)</span>). Plug this metastatement into Löb’s theorem: (Let the modal operator <span class="math inline">\(\square\)</span> denote the modality of provability, and let the constant <span class="math inline">\(\bot\)</span> denote the falsehood.) <span class="math display">\[\begin{align*}
&amp; \square (\square \bot \to \bot) \to \square \bot \\
\iff &amp; \square (\neg \square \bot) \to \square \bot &amp;\qquad\text{(Definition of negation)} \\
\iff &amp; \square (\neg \square \bot) \to \bot &amp;\qquad\text{(Axiom T and transitivity)}\\
\iff &amp; \neg \square (\neg \square \bot) &amp;\qquad\text{(Definition of negation)}\\
\end{align*}\]</span> which informally says “It is not provable that falsehood is not provable.” which is just another way of claiming that <span class="math inline">\(\mathfrak{N} \not\vdash \operatorname{Cons}\mathfrak{N}\)</span>, the consistency of the logical system cannot be proved from within the system itself, a.k.a. Gödel’s second incompleteness theorem.</p>
<p>Digressing a little from our main topic, formal logic, let’s consider the application of the undefinability theorem to programming language theory as a dessert. It is well understood that, by the <a href="https://en.wikipedia.org/wiki/Curry–Howard_correspondence">Curry-Howard correspondence</a>, a logical formula corresponds to a type, and a type checker of a programming language guarantees that every well-typed program consists of only types that correspond to valid formulas. If you write some sophisticated type and its type checking succeeds, the underlying formula must be valid as you have just proved that (so-called “programs are proofs”). Now, I challenge you, given a hypothetical, reasonably expressive programming language with full dependent types, can you write a type-checked <a href="https://en.wikipedia.org/wiki/Meta-circular_evaluator">self-interpreter</a> in it? Once you succeed, you would have proved that you can actually tell valid formulas (types) from false ones in all cases (by writing a self-interpreter that accepts only well-typed programs), thus effectively define the truth in the language itself. By the undefinability theorem, that would be impossible (otherwise your underlying logical system can’t be consistent!). A slightly different but related result is that <em>a total programming language can’t have a self-interpreter</em>; since a total language is not necessarily equipped with a type system or alike proof-theoretic constructions, the undefinability theorem may not directly apply, but a computability-theoretic argument of this is analogously diagonal (see <a href="https://cstheory.stackexchange.com/a/24994/21291">[5]</a>).<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<section id="references" class="level2">
<h2>References</h2>
<p>[1] Wikipedia, “Diagonal lemma”. <a href="https://en.wikipedia.org/wiki/Diagonal_lemma" class="uri">https://en.wikipedia.org/wiki/Diagonal_lemma</a></p>
<p>[2] Wikipedia, “Tarski’s undefinability theorem”. <a href="https://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem#General_form_of_the_theorem">https://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem#General_form_of_the_theorem</a></p>
<p>[3] nLab, “Lawvere’s fixed point theorem”. <a href="https://ncatlab.org/nlab/show/Lawvere%27s+fixed+point+theorem">https://ncatlab.org/nlab/show/Lawvere%27s+fixed+point+theorem</a></p>
<p>[4] nLab, “Löb’s theorem”. <a href="https://ncatlab.org/nlab/show/Löb%27s+theorem">https://ncatlab.org/nlab/show/Löb%27s+theorem</a></p>
<p>[5] Andrej Bauer, “Answer to: A total language that only a Turing complete language can interpret”. <a href="https://cstheory.stackexchange.com/a/24994/21291" class="uri">https://cstheory.stackexchange.com/a/24994/21291</a></p>
<p>[6] Matt Brown and Jens Palsberg, “Breaking Through the Normalization Barrier: A Self-Interpreter for F-omega”. <a href="http://compilers.cs.ucla.edu/popl16/popl16-full.pdf" class="uri">http://compilers.cs.ucla.edu/popl16/popl16-full.pdf</a></p>
<p>[7] Jason Gross, Jack Gallagher and Benya Fallenstein, “Löb’s Theorem – A functional pearl of dependently typed quining”. <a href="https://jasongross.github.io/lob-paper/nightly/lob.pdf" class="uri">https://jasongross.github.io/lob-paper/nightly/lob.pdf</a></p>
<p>[8] Herbert B. Enderton, <em>A Mathematical Introduction to Logic</em>, 2nd ed.</p>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>One may find it intuitive to notice that, whether <span class="math inline">\(\text{True}(\sigma)\)</span> is true or not, it has nothing to do with the existence of God or whatever other claims of real interest. Indeed, instead of restricting our language as we do here, we could as well devise a restrictive set of inference rules for an alternative logic (e.g., <a href="https://plato.stanford.edu/entries/logic-relevance/">relevance logic</a>) so that explosions can be expelled.<a href="#fnref1" class="footnoteBack">↩</a></p></li>
<li id="fn2"><p>I am aware of the “self-interpreter” for System <span class="math inline">\(\text{F}_\omega\)</span> proposed in a POPL’16 paper by Brown &amp; Palsberg <a href="http://compilers.cs.ucla.edu/popl16/popl16-full.pdf">[6]</a>. According to a footnote in <a href="https://jasongross.github.io/lob-paper/nightly/lob.pdf">[7]</a> and also from my unrefined understanding, the interpreter does not come with a separate syntax for types thus fails to capture the underlying logical consistency; therefore neither Löb’s theorem nor the undefinability theorem may apply in that case.<a href="#fnref2" class="footnoteBack">↩</a></p></li>
</ol>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>Go Away, tracker-store</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/notes/171103" />
    <id>tag:www.soimort.org,2017:/notes/171103</id>
    <published>2017-11-03T00:00:00+01:00</published>
    <updated>2017-11-04T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="tooling" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>Long story short, after an Arch update the annoying process called <code>tracker-store</code> started to hog my CPU and disk space (again). For some really peculiar reason, GNOME developers decided that everyone should want to use their <del>awesome</del> <a href="https://wiki.gnome.org/Projects/Tracker">Tracker</a> smarty-ware to index everything in pants, and you should not need to turn it off so easily…</p>
<p>Well, I had had enough of this due to concerns of performance and privacy, and surely uninstalled the <code>tracker</code> package a long time ago, but now it seems GNOME’s canonical file manager <code>nautilus</code> starts to depend on <code>tracker</code> (one of its old dependencies, <code>libtracker-sparql</code>, has now been fused with the <code>tracker</code> package since version 2.0 in Arch’s downstream repository). That explains why it’s back, uninvitedly.</p>
<p>A few quirks noted (in Tracker 2.0.x):</p>
<ol type="1">
<li>The GUI configuration program <code>tracker-preferences</code> has been summarily removed (<a href="https://github.com/GNOME/tracker/commit/d4a8d6e45e991758440276b4ca3ad6e821dfdab2">d4a8d6e</a>), while <code>tracker</code> provides <em>no</em> command-line alternative for intuitive gsettings configuration yet. (Then why would they remove this very helpful GUI?)</li>
<li>The old trick that appends <code>Hidden=true</code> to an overriding autostart file such as <code>~/.config/autostart/tracker-store.desktop</code> (in [1] [2]) no longer works in 2.0, because….</li>
<li><code>tracker</code> ships with a systemd user service now (<code>/usr/lib/systemd/user/tracker-store.service</code>), since they obviously think a desktop-level autostart is not enough. While Arch Linux ships wisely with a <code>/usr/lib/systemd/system-preset/99-default.preset</code> containing “<code>disable *</code>” which disables all new units by default [3], there is no equivalent <code>user-preset</code> file doing the same, which means that <code>tracker-store.service</code>, as a user service, is enabled by default and still gets to run on every boot.</li>
</ol>
<p>Now that uninstalling <code>tracker</code> is not an option, one has to take both measures to block <code>tracker-store</code> from running:</p>
<ol type="1">
<li>Copy the autostart file and override it with a user-specific one: <code>$ cp /etc/xdg/autostart/tracker-store.desktop ~/.config/autostart/tracker-store.desktop</code><br />
and append to it:<br />
<code>Hidden=true</code><br />
(The same step also applies for <code>tracker</code>’s friends, such as <code>tracker-miner-fs</code>, etc.)</li>
<li>Mask (which is the strongest yet nondestructive way to “disable” a static systemd service completely [4]) all <code>tracker</code>-related services:</li>
</ol>
<pre><code>        $ systemctl --user mask tracker-store</code></pre>
<p>Look ma, no more hogs. Back to work!</p>
<section id="references" class="level2">
<h2>References</h2>
<p>[1] Ask Ubuntu, “tracker-store and tracker-miner-fs eating up my CPU on every startup”. <a href="https://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup" class="uri">https://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup</a></p>
<p>[2] “Disabling GNOME Tracker and Other Info”. <a href="https://gist.github.com/vancluever/d34b41eb77e6d077887c" class="uri">https://gist.github.com/vancluever/d34b41eb77e6d077887c</a></p>
<p>[3] ArchWiki, “systemd”. <a href="https://wiki.archlinux.org/index.php/Systemd" class="uri">https://wiki.archlinux.org/index.php/Systemd</a></p>
<p>[4] FreeDesktop.org, “systemctl(1)”. <a href="https://www.freedesktop.org/software/systemd/man/systemctl.html" class="uri">https://www.freedesktop.org/software/systemd/man/systemctl.html</a></p>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>The Semantic Truth</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/9" />
    <id>tag:www.soimort.org,2017:/mst/9</id>
    <published>2017-10-07T00:00:00+02:00</published>
    <updated>2017-10-17T00:00:00+02:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<blockquote>
<p>“This is clear, in the first place, if we define what the true and the false are. To say of what is that it is not, or of what is not that it is, is <em>false</em>, while to say of what is that it is, and of what is not that it is not, is <em>true</em>.”</p>
</blockquote>
<p style="text-align:right !important;text-indent:0 !important">– Aristotle, <em>Metaphysics</em></p>
<hr />
<p><strong>On truth-bearers.</strong> A <em>truth-bearer</em> is something of what we can talk about the truth-value: a <em>statement</em>, a <em>sentence</em>, a <em>proposition</em>, etc. We will use the three aforementioned notions indistinguishably, but avoid the use of subjective notions such like <em>assertion</em>, <em>belief</em>, <em>intuition</em> and <em>judgment</em> as truth-bearers. (These words however, can occur with their originally intended meanings.)</p>
<p>In <a href="/mst/7/">Mst. #7</a> we defined that (in a formal language) a sentence is just a well-formed formula with no free variable. Furthermore, in <a href="/mst/8/">Mst. #8</a> we defined a statement, naïvely, as a sentence that says something is true. But what makes a sentence true, or oppositely, not true?</p>
<p><strong>Aristotle’s definition of truth.</strong> Aristotle’s original truth definition in <em>Metaphysics</em> (360s B.C.) can be denoted using the modern device of Quine’s quasi-quotation, as follows:</p>
<p style="text-align:center !important;text-indent:0 !important">⌜<span class="math inline">\(\psi\)</span>⌝ is <em>true</em> if and only if <span class="math inline">\(\psi\)</span>.<br />
⌜<span class="math inline">\(\psi\)</span>⌝ is <em>false</em> if and only if it is not the case that <span class="math inline">\(\psi\)</span>.</p>
<p>Note that &quot; ⌜<span class="math inline">\(\psi\)</span>⌝&quot; is a <em>mention</em> of some statement while “<span class="math inline">\(\psi\)</span>” is the <em>use</em> of that statement in the metalanguage, so the above definition does make sense (instead of some “be whatever you be” nonsense as it might seem to an untrained eye).</p>
<p>An implication of this definition, is that “false” is the opposite of “true”. The bivalence of truth and falsity is an essential property of classical logic. Thus, we have the following tautology: <span class="math display">\[\psi \lor \lnot\psi \qquad\text{(Law of Excluded Middle)}\]</span> that is, given any interpretation, whether <span class="math inline">\(\psi\)</span> is true or false, the above proposition is always true (or said to be <em>valid</em>). This fact may be justified via a truth table.</p>
<p><strong>Theories of truth.</strong> From a metaphysical perspective, several theories of truth have been proposed and well argued:</p>
<ul>
<li><strong>Correspondence theory</strong>: The truth is determined solely by how it relates to the world and whether it describes a fact about that world.
<ul>
<li>For a statement to be true, there must be an isomorphism to it from the state of the real world that makes it true. Hence, the correspondence view is mainly accepted by realists/Platonists but rejected by most idealists.</li>
</ul></li>
<li><strong>Coherence theory</strong>: The truth is determined by how consistent it is with our existing, coherent set of statements or beliefs.</li>
<li><strong>Pragmatic theory</strong>: The truth is determined by how well it enables us to achieve our goals.</li>
<li><strong>Deflationary theory</strong> (alternatively under the labels of <strong>redundancy theory</strong>, <strong>disquotational theory</strong>, <strong>prosentential theory</strong> or <strong>minimalist theory</strong>): The assertion of truth of a statement is nothing but an assertion of the statement itself; it does not attribute a property called “truth” to such a statement.
<ul>
<li>The most widely accepted definitions of truth in modern treatment of formal logic (by Frege, <a href="https://en.wikipedia.org/wiki/Frank_P._Ramsey">F. P. Ramsey</a>, Quine, etc.), are deflationary in essence.</li>
</ul></li>
</ul>
<blockquote style="background:gainsboro; border-radius:1em; padding:.25em 1em; font-size:.8em">
<p style="text-align:center !important;text-indent:0 !important"><strong><em>Reading list</em></strong></p>
<p>On the notion of truth and truth-bearers:</p>
<ul>
<li>Michael Glanzberg, “Truth,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/truth/" class="uri">https://plato.stanford.edu/entries/truth/</a>.</li>
<li>Matthew McGrath, “Propositions,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/propositions/" class="uri">https://plato.stanford.edu/entries/propositions/</a>.</li>
</ul>
<p>Correspondence theory:</p>
<ul>
<li>Marian David, “The Correspondence Theory of Truth,” Stanford Encyclopedia of Philosophy, <a href="http://plato.stanford.edu/entries/truth-correspondence/" class="uri">http://plato.stanford.edu/entries/truth-correspondence/</a>.</li>
</ul>
<p>Coherence theory:</p>
<ul>
<li>James O. Young, “The Coherence Theory of Truth,” Stanford Encyclopedia of Philosophy, <a href="http://plato.stanford.edu/entries/truth-coherence/" class="uri">http://plato.stanford.edu/entries/truth-coherence/</a>.</li>
</ul>
<p>Pragmatic theory:</p>
<ul>
<li>Christopher Hookway, “Pragmatism,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/pragmatism/#PraTheTru" class="uri">https://plato.stanford.edu/entries/pragmatism/#PraTheTru</a>.</li>
</ul>
<p>Deflationary theory:</p>
<ul>
<li>Daniel Stoljar and Nic Damnjanovic, “The Deflationary Theory of Truth,” Stanford Encyclopedia of Philosophy, <a href="http://plato.stanford.edu/entries/truth-deflationary/" class="uri">http://plato.stanford.edu/entries/truth-deflationary/</a>.</li>
</ul>
<p>Ramsey’s redundancy theory of truth (a deflationary theory):</p>
<ul>
<li>Frank P. Ramsey, “Facts and Propositions.” <a href="https://www.aristoteliansociety.org.uk/pdf/ramsey.pdf">[PDF]</a></li>
</ul>
</blockquote>
<p>Now we consider a (semi-)formal language where we are able to state truth self-referentially.</p>
<p><strong>Antinomies.</strong></p>
<ol type="1">
<li><em>Liar’s paradox.</em></li>
</ol>
<p style="text-align:center !important;text-indent:0 !important">This sentence is false.</p>
<ol start="2" type="1">
<li><em>Quine’s paradox.</em></li>
</ol>
<p style="text-align:center !important;text-indent:0 !important">“Yields falsehood when preceded by its quotation”<br />
yields falsehood when preceded by its quotation.</p>
<p>If we take the truth (or falsity) of a statement to be bivalent, then the truth-values of the above two sentences could not be consistently determined. Such antinomies motivated <a href="https://plato.stanford.edu/entries/tarski/">Alfred Tarski</a>’s proposal of the semantic theory of truth, which eliminates such potentially paradoxical use in a formal language.</p>
<p><strong>Semantic theory of truth. (Tarski, 1933)</strong> Given an object language <span class="math inline">\(\mathcal{L}_0\)</span> and a metalanguage <span class="math inline">\(\mathcal{L}_1\)</span>,</p>
<p style="text-align:center !important;text-indent:0 !important">⌜<span class="math inline">\(\psi_0\)</span>⌝ is <em>true</em> if and only if <span class="math inline">\(\psi_1\)</span>. (Convention T)</p>
<p>where “<span class="math inline">\(\psi_0\)</span>” is a sentence in <span class="math inline">\(\mathcal{L}_0\)</span>, “<span class="math inline">\(\psi_1\)</span>” is a sentence in <span class="math inline">\(\mathcal{L}_1\)</span>. It is demanded that <span class="math inline">\(\mathcal{L}_0\)</span> must be contained in <span class="math inline">\(\mathcal{L}_1\)</span>. Moreover, the word “true” (or “false”) does not occur in <span class="math inline">\(\psi_0\)</span>. Thus, no sentence in the object language <span class="math inline">\(\mathcal{L}_0\)</span> can assert the truth/falsity of itself; rather, its truth-value must be asserted by a higher-level metalanguage <span class="math inline">\(\mathcal{L}_1\)</span>. Virtually, it would be intuitive to imagine a semantic hierarchy of formal languages that rules out the use of self-reference: <span class="math display">\[\mathcal{L}_0 \sqsubset \mathcal{L}_1 \sqsubset \mathcal{L}_2 \sqsubset \cdots\]</span> that is, the truth of a sentence in an object language <span class="math inline">\(\mathcal{L}_i\)</span> can only be asserted in <span class="math inline">\(\mathcal{L}_{i+1}\)</span> as the metalanguage. Since there is no final stage in this hierarchy, the “most-meta” truths may never be asserted.</p>
<p><strong>T-schema.</strong> For a formal language containing a given set of logical connective symbols, we give an inductive definition of truth in the following form:</p>
<ol type="1">
<li>⌜<span class="math inline">\(\psi\)</span>⌝ is true if and only if <span class="math inline">\(\psi\)</span>.</li>
<li>(<em>Negation</em>) ⌜<span class="math inline">\(\lnot\psi\)</span>⌝ is true if and only if <span class="math inline">\(\psi\)</span> is not true.</li>
<li>(<em>Conditional</em>) ⌜<span class="math inline">\(\psi \to \theta\)</span>⌝ is true if and only if <span class="math inline">\(\psi\)</span> is not true or <span class="math inline">\(\theta\)</span>, or both.</li>
<li>(<em>Conjunction</em>) ⌜<span class="math inline">\(\psi \land \theta\)</span>⌝ is true if and only if <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\theta\)</span>.</li>
<li>(<em>Disjunction</em>) ⌜<span class="math inline">\(\psi \lor \theta\)</span>⌝ is true if and only if <span class="math inline">\(\psi\)</span> or <span class="math inline">\(\theta\)</span>, or both.</li>
<li>(<em>Universality</em>) ⌜<span class="math inline">\(\forall x \psi(x)\)</span>⌝ is true if and only if every object <span class="math inline">\(x\)</span> satisfies <span class="math inline">\(\psi(x)\)</span>.</li>
<li>(<em>Existence</em>) ⌜<span class="math inline">\(\exists x \psi(x)\)</span>⌝ is true if and only if there is an object <span class="math inline">\(x\)</span> that satisfies <span class="math inline">\(\psi(x)\)</span>.</li>
</ol>
<p>(Note that the subscripts distinguishing the object language / metalanguage are implicit.)</p>
<p>Tarski’s semantic theory of truth is not only limited to the application in philosophical logic though: A similar definition is also used in model theory, where we <a href="https://wiki.soimort.org/math/logic/fol/structures/">define the satisfaction</a> of a formula <span class="math inline">\(\varphi\)</span> with respect to a structure <span class="math inline">\(\mathfrak{A}\)</span> and an assignment <span class="math inline">\(s\)</span> inductively.</p>
<p><strong>Remark 9.1. (Semantic theory of truth and mathematical logic)</strong> The mathematical counterpart of Tarski’s semantic theory of truth yields the <strong>undefinability theorem (Tarski, 1936)</strong>, which briefly states that arithmetical truth cannot be defined in arithmetic itself (it is worth noting that the proof is a non-trivial one which requires Gödel numbering on formulas in a formal language). The semantic undefinability also has a strong correspondence with the <strong>incompleteness theorems (Gödel, 1931)</strong>, which will be covered in future notes hopefully.</p>
<blockquote style="background:gainsboro; border-radius:1em; padding:.25em 1em; font-size:.8em">
<p style="text-align:center !important;text-indent:0 !important"><strong><em>Reading list</em></strong></p>
<p>On Tarski’s semantic theory of truth:</p>
<ul>
<li>Wilfrid Hodges, “Tarski’s Truth Definitions,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/tarski-truth/" class="uri">https://plato.stanford.edu/entries/tarski-truth/</a>.</li>
<li>Alfred Tarski, “The Semantic Conception of Truth: And the Foundations of Semantics.” <a href="http://www.ditext.com/tarski/tarski.html">[HTML]</a></li>
<li>Alfred Tarski, “Truth and Proof.” <a href="https://cs.nyu.edu/mishra/COURSES/13.LOGIC/Tarski.pdf">[PDF]</a></li>
</ul>
</blockquote>
<p><strong>Truth and objectivity.</strong> Is truth objective? Yes. Given the object language <span class="math inline">\(\mathcal{L}_0\)</span>, consider a sentence <span class="math inline">\(\varphi[s]\)</span> which is true under the model <span class="math inline">\(\mathfrak{A}\)</span>: <span class="math display">\[\models_\mathfrak{A} \varphi[s]\]</span> It is true because we can argue in our metalanguage <span class="math inline">\(\mathcal{L}_1\)</span>, that it is satisfied decidedly; thus it must be a truth in the object language. For example, we can argue extensionally that <span class="math display">\[\models_\mathfrak{A} \text{Morning Star} = \text{Evening Star}\]</span> Opinionatedly, there could be no such thing as a “subjective truth”, since the truth of a sentence must be justified in a higher-level metalanguage (due to the semantic theory of truth). Per the object-metalanguage distinction, truth is only objective by nature. Once we claim that “something holds” without appealing to a metalanguage for justification, then it is merely a <em>belief</em> rather than a <em>truth</em>. Beliefs can be, of course, illogical and ungrounded.</p>
<p><strong>Logical validity of arguments.</strong> A true sentence may not always be true in every possible structure. Consider: <span class="math display">\[\models_\mathfrak{B} \text{Morning Star} = \text{Evening Star}\]</span> In an alternative universe <span class="math inline">\(\mathfrak{B}\)</span> where Venus fails to be both the morning star and the evening star, such a statement would be trivially false. However, a logical argument must not depend on a specific structure or interpretation of its parameters; that is, when we argue logically that something is true, the argument itself must hold <em>valid</em>. Validity is a purely logical notion here. For example, consider adding the following axiom to our deductive system: <span class="math display">\[t = t&#39; \to t \cong t&#39;\]</span></p>
<p>One can easily verify that this axiom is valid (i.e., always true): If <span class="math inline">\(t = t&#39;\)</span> is false, then <span class="math inline">\(t = t&#39; \to t \cong t&#39;\)</span> is vacuously true; if <span class="math inline">\(t = t&#39;\)</span> is true, then we have <span class="math inline">\(t \cong t&#39;\)</span> so <span class="math inline">\(t = t&#39; \to t \cong t&#39;\)</span> is also true.</p>
<p>Specifically, we have</p>
<p style="text-align:center !important;text-indent:0 !important"><span class="math inline">\(\text{Morning Star} = \text{Evening Star} \to \text{Morning Star} \cong \text{Evening Star}\)</span><br />
(If the morning star <em>is</em> the evening star, then it <em>is isomorphic to</em> the evening star.)</p>
<p>By modus ponens, the following deduction applies:</p>
<p style="text-align:center !important;text-indent:0 !important"><span class="math inline">\(\Gamma; \text{Morning Star} = \text{Evening Star} \quad\vdash\quad \text{Morning Star} \cong \text{Evening Star}\)</span><br />
(Assume that the morning star <em>is</em> the evening star, then it <em>is isomorphic to</em> the evening star.)</p>
<p>This is a valid argument, since it follows solely from our set of axioms and rules of inference, but relies on no particular structures or interpretations. Such a valid argument justifies the logical fact that the truth of its premises guarantees the truth of its conclusion; this, however, does not imply anything about the truth of premises (that would depend on the actual interpretation).</p>
<p><strong>Soundness of arguments.</strong> To put it shortly, a <em>sound</em> argument is just a valid argument with true premises.</p>
<p style="text-align:center !important;text-indent:0 !important"><span class="math inline">\(\frac{\text{Morning Star} = \text{Evening Star} \qquad \text{Morning Star} = \text{Evening Star} \to \text{Morning Star} \cong \text{Evening Star}}{\text{Morning Star} \cong \text{Evening Star}}\)</span><br />
(Given that the morning star <em>is</em> the evening star, it <em>is isomorphic to</em> the evening star.)</p>
<p>Given true premises, the valid argument proves its conclusion: <span class="math inline">\(\text{Morning Star} \cong \text{Evening Star}\)</span>, and we know that it is guaranteed to be true. Such a truth is also called a <em>logical truth</em>, since it follows from a logical consequence of a sound argument in our deductive system. Notice that the above is just an instance of the <strong>soundness theorem</strong>, loosely stated as: Every provable sentence is true.</p>
<p><strong>Two approaches to logical consequence.</strong></p>
<ol type="1">
<li><em>Model-theoretic</em> (or “realist”) approach. The validity of an argument is justified by the <em>absence of counterexample</em>. That is, if we can ever find a structure <span class="math inline">\(\frak{A}\)</span> and an assignment function <span class="math inline">\(s : V \to |\mathfrak{A}|\)</span> such that <span class="math inline">\(\frak{A}\)</span> satisfies every formula in hypotheses <span class="math inline">\(\Gamma\)</span> with <span class="math inline">\(s\)</span>, but <span class="math inline">\(\frak{A}\)</span> fails to satisfy <span class="math inline">\(\varphi\)</span> with <span class="math inline">\(s\)</span>, we will conclude that <span class="math inline">\(\Gamma \not\models \varphi\)</span>. Otherwise, we will have <span class="math inline">\(\Gamma \models \varphi\)</span>, where <span class="math inline">\(\varphi\)</span> is a logical truth.</li>
<li><em>Proof-theoretic</em> (or “formalist”) approach. The validity of an argument is justified by a <em>deduction</em>. That is, from our set of hypotheses and axioms <span class="math inline">\(\Gamma \cup \Lambda\)</span> we obtain <span class="math inline">\(\varphi\)</span> by applying various rules of inference as a finite sequence, then <span class="math inline">\(\Gamma \vdash \varphi\)</span>, where <span class="math inline">\(\varphi\)</span> is a logical truth.</li>
</ol>
<p>By the soundness and completeness theorems <span class="math inline">\(\Gamma \models \varphi \Leftrightarrow \Gamma \vdash \varphi\)</span>, the two approaches agree with each other extensionally.</p>
<blockquote style="background:gainsboro; border-radius:1em; padding:.25em 1em; font-size:.8em">
<p style="text-align:center !important;text-indent:0 !important"><strong><em>Reading list</em></strong></p>
<p>Philosophical logic-related issues:</p>
<ul>
<li>Jc Beall and Greg Restall, “Logical Consequence,” Stanford Encyclopedia of Philosophy, <a href="http://plato.stanford.edu/entries/logical-consequence/" class="uri">http://plato.stanford.edu/entries/logical-consequence/</a>.</li>
<li>Dorothy Edgington, “Indicative Conditionals,” Stanford Encyclopedia of Philosophy, <a href="http://plato.stanford.edu/entries/conditionals/" class="uri">http://plato.stanford.edu/entries/conditionals/</a>.</li>
<li>Michael Nelson, “Existence,” Stanford Encyclopedia of Philosophy, <a href="http://plato.stanford.edu/entries/existence/" class="uri">http://plato.stanford.edu/entries/existence/</a>.</li>
</ul>
</blockquote>
<p><strong>Motivations of alternative logics.</strong> In classical first-order logic, a deductive system preserves the semantic property of truth, as justified by the dual theorems of soundness/completeness. We will see that it does not always sufficiently capture the need for logical reasoning, thus numerous alternative (non-classical) logics has emerged for this reason.</p>
<ul>
<li><strong>Modality.</strong> Instead of saying that something is the case, a modal statement says about what <em>could be</em> or <em>must be</em> the case, e.g., “Venus must be the morning star”, “Venus could be both the morning star and the evening star”. <strong>Modal logic</strong> supplements classical logic with two modal operators representing the notions of <em>necessity</em> (□) and <em>possibility</em> (◇).</li>
<li><strong>Temporality and tense.</strong> Instead of saying that something is the case, a temporal statement says something to be the case as qualified in terms of time, e.g., “Venus is always the morning star”, “The evening star will eventually be visible”. <strong>Temporal logic</strong> (or <strong>tense logic</strong>) supplements classical logic with a set of temporal modal operators.</li>
<li><strong>Vagueness.</strong> While we could simply say that something is true or false, sometimes questions like “how true is that?” arise, e.g., “Venus is a distant planet”. The vagueness of predicates in natural languages leads to the approach of <strong>many-valued logic</strong>, where we are able to speak of truth-values like “fully true”, “fully false” or “neutral” (<strong>three-valued logic</strong>), or even any real truth-value in between the interval <span class="math inline">\([0,1]\)</span> (infinite valued <strong>fuzzy logic</strong>).</li>
<li><strong>Heterogeneity.</strong> In the semantics of classical logic, the universe (domain) is assumed to be a homogeneous collection of objects. <strong>Many-sorted logic</strong> allows us to divide the universe into disjoint subsets (sorts) so as to treat objects of different sorts differently.</li>
<li><strong>Quantification into predicates.</strong> In first-order logic, one can only quantify over a variable in the universe. Using the notation of set theory: <span class="math display">\[v \in |\mathfrak{A}|\]</span> where <span class="math inline">\(v\)</span> is an <em>urelement</em> (i.e., not a set itself).<br />
In <strong>second-order logic</strong>, any predicate or function may be quantified over as a “second-order variable” (which essentially represents a set), in the power set of the universe: <span class="math display">\[P \subseteq |\mathfrak{A}|^n \in \mathcal{P}(|\mathfrak{A}|)\]</span> <span class="math display">\[f \subseteq |\mathfrak{A}|^{n+1} \in \mathcal{P}(|\mathfrak{A}|)\]</span> In an <strong><span class="math inline">\(n\)</span>-th-order logic</strong>, one can quantify over an <span class="math inline">\(n\)</span>-th-order variable <span class="math inline">\(\chi\)</span> in the <span class="math inline">\((n-1)\)</span>-th-order power set of the universe: (When <span class="math inline">\(n=1\)</span>, the domain is just the original universe.) <span class="math display">\[\chi \in \mathcal{P}^{n-1}(|\mathfrak{A}|)\]</span> Therefore, we can say that higher-order logics are just “set theory in disguise” (Quine), since the set theory in a first-order language is capable of representing notions like “set”, “set of sets”, etc.<br />
It is worth mentioning here that since the power set operation is definable in second-order logic, second-order logic is expressive enough to simulate any finite-order logic. <span class="citation" data-cites="hintikka1955two">[1]</span></li>
<li><strong>Empty terms or domains.</strong> In classical logic, a structure <span class="math inline">\(\mathfrak{A}\)</span> (with a non-empty domain <span class="math inline">\(|\mathfrak{A}|\)</span>) assigns each term an interpretation. <strong>Free logic</strong> allows for uninterpreted terms (that do not denote any object); furthermore, <strong>inclusive logic</strong> allows structures to have an empty domain <span class="math inline">\(|\mathfrak{A}| = \emptyset\)</span>.</li>
</ul>
<p>It is not uncommon to revise one or more axioms in classical first-order logic. The resulting non-classical logics can be useful in a constructive setting. In the following text, we will use syntactical forms like <span class="math inline">\(\alpha \to \beta\)</span> to denote both a tautological conditional sentence and a valid deduction rule (<span class="math inline">\(\{\alpha\} \vdash \beta\)</span>), as justified by the dual theorems of deduction/resolution.</p>
<ul>
<li><strong>Justification.</strong> In classical logic we accept the principle of bivalence (i.e., anything is either true or false), thus we have the following tautologies: <span class="math display">\[\psi \lor \lnot\psi \qquad\text{(Law of Excluded Middle)}\]</span> <span class="math display">\[\lnot\lnot\psi \to \psi \qquad\text{(Double Negation Elimination)}\]</span> <span class="math display">\[(\lnot \psi \to \bot) \to \psi \qquad\text{(Proof by Contradiction)}\]</span> which are denied by <strong>intuitionistic logic</strong>. From a model-theoretic view, there could be logical truths that are not flawed, but not necessarily well justified. In a word, intuitionistic logic attempts to capture the semantic property of <em>justification</em> instead of truth. Particularly, in classical logic, assume that we can prove <span class="math inline">\(\forall x (\lnot\varphi) \to \bot\)</span>, then <span class="math inline">\(\lnot\lnot \forall x (\lnot\varphi) \to \bot\)</span>, by PbC we get <span class="math inline">\(\lnot \forall x (\lnot\varphi)\)</span>, which is just <span class="math inline">\(\exists x \varphi\)</span> – but we have no evidence that such a “witness” <span class="math inline">\(t\)</span> (which truthifies <span class="math inline">\(\varphi^x_t\)</span>) really exists yet. This intuitionistic thinking is extremely helpful in the so-called <em>constructive mathematics</em>, where the proof of the existence of a mathematical object requires a justification by providing an example.</li>
<li><strong>Inconsistency.</strong> In classical and intuitionistic logics we accept the principle of explosion (<em>ex falso quodlibet</em>), that is, a contradiction <span class="math inline">\(\psi \land \lnot\psi\)</span> (often abbreviated as <span class="math inline">\(\bot\)</span>) would make a theory inconsistent, thus any sentence <span class="math inline">\(\theta\)</span> is trivially true: <span class="math display">\[(\psi \land \lnot\psi) \to \theta \qquad\text{(Ex Falso Quodlibet)}\]</span> Sometimes, however, we are interested in formalizing inconsistent theories in a non-trivial way, in which case trivial truths by explosion must be excluded from our system. <strong>Paraconsistent logic</strong> is any such logic that rejects EFQ (or its equivalents). Specifically, <strong>minimal logic</strong> is the paraconsistent revision of intuitionistic logic.</li>
</ul>
<p>In proof theory, <em>substructural logics</em> are a family of non-classical logics where one (or more) <a href="https://en.wikipedia.org/wiki/Structural_rule">structural rule</a> (e.g., weakening, contraction, commutativity, associativity) is absent.</p>
<ul>
<li><strong>Relevance of implication.</strong> In classical logic we have the following tautologies (involving material/strict implication): <span class="math display">\[\psi \to (\theta \to \psi) \qquad\text{(1)}\]</span> <span class="math display">\[\lnot\psi \to (\psi \to \theta) \qquad\text{(2)}\]</span> <span class="math display">\[(\psi \to \theta) \lor (\theta \to \phi) \qquad\text{(3)}\]</span> <span class="math display">\[\psi \to (\theta \to \theta) \qquad\text{(4)}\]</span> <span class="math display">\[\psi \to (\theta \lor \lnot\theta) \qquad\text{(5)}\]</span> It is easy to see that they all turn out to be counterintuitive in some way. For example, by (1) we are allowed to argue things like “If Mars is a planet, then Venus is a star implies that Mars is a planet”, but this is clearly absurd not only because Venus isn’t astronomically a star, but also because the fact whether Venus is a star or not, has no direct relevance with Mars being a planet. But the whole sentence is valid in classical logic! <strong>Relevance logic</strong> denies these tautologies, since its implication <span class="math inline">\(\to\)</span> requires that the antecedent must be somehow relevant to the consequent. Relevance logic is both a substructural logic (as it lacks of the structural rule of weakening) and a paraconsistent logic (as it clearly denies EFQ from which any irrelevant truth is trivially deducible).</li>
<li><strong>Resource consciousness.</strong> With an emphasis on resource boundedness, <strong>linear logic</strong> leaves out both structural rules of weakening and contraction, as a reflection of the limitation that resources cannot always be duplicated or thrown away arbitrarily.</li>
<li>Other substructural logics include <strong>affine logic</strong>, which allows the structural rule of weakening but disallows contraction; <strong>ordered logic</strong> (non-commutative logic), which disallows the rule of exchange (commutativity) in addition to rejecting weakening and contraction.</li>
</ul>
<blockquote style="background:gainsboro; border-radius:1em; padding:.25em 1em; font-size:.8em">
<p style="text-align:center !important;text-indent:0 !important"><strong><em>Reading list</em></strong></p>
<p>On classical logic:</p>
<ul>
<li>Stewart Shapiro, “Classical Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-classical/" class="uri">https://plato.stanford.edu/entries/logic-classical/</a>.</li>
</ul>
<p>On modality and modal logic:</p>
<ul>
<li>Boris Kment, “Varieties of Modality,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/modality-varieties/" class="uri">https://plato.stanford.edu/entries/modality-varieties/</a>.</li>
<li>James Garson, “Modal Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-modal/" class="uri">https://plato.stanford.edu/entries/logic-modal/</a>.</li>
</ul>
<p>On temporality, tense and temporal logic:</p>
<ul>
<li>Friedrich Hamm and Oliver Bott, “Tense and Aspect,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/tense-aspect/" class="uri">https://plato.stanford.edu/entries/tense-aspect/</a>.</li>
<li>Valentin Goranko and Antony Galton, “Temporal Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-temporal/" class="uri">https://plato.stanford.edu/entries/logic-temporal/</a>.</li>
</ul>
<p>On vagueness, many-valued logic and fuzzy logic:</p>
<ul>
<li>Roy Sorensen, “Vagueness,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/vagueness/" class="uri">https://plato.stanford.edu/entries/vagueness/</a>.</li>
<li>Siegfried Gottwald, “Many-Valued Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-manyvalued/" class="uri">https://plato.stanford.edu/entries/logic-manyvalued/</a>.</li>
<li>Petr Cintula, Christian G. Fermüller and Carles Noguera, “Fuzzy Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-fuzzy/" class="uri">https://plato.stanford.edu/entries/logic-fuzzy/</a>.</li>
</ul>
<p>On quantification, second-order and higher-order logic, and their correspondence with set theory:</p>
<ul>
<li>Gabriel Uzquiano, “Quantifiers and Quantification,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/quantification/" class="uri">https://plato.stanford.edu/entries/quantification/</a>.</li>
<li>Herbert B. Enderton, “Second-order and Higher-order Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-higher-order/" class="uri">https://plato.stanford.edu/entries/logic-higher-order/</a>.</li>
<li>Jouko Väänänen, “Second-Order Logic and Set Theory.” <a href="http://www.math.helsinki.fi/logic/people/jouko.vaananen/Vaananen_Compass.pdf">[PDF]</a></li>
</ul>
<p>On free logic:</p>
<ul>
<li>John Nolt, “Free Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-free/" class="uri">https://plato.stanford.edu/entries/logic-free/</a>.</li>
</ul>
<p>On intuitionistic logic and constructive mathematics:</p>
<ul>
<li>Joan Moschovakis, “Intuitionistic Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-intuitionistic/" class="uri">https://plato.stanford.edu/entries/logic-intuitionistic/</a>.</li>
<li>Douglas Bridges and Erik Palmgren, “Constructive Mathematics,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/mathematics-constructive/" class="uri">https://plato.stanford.edu/entries/mathematics-constructive/</a>.</li>
<li>David Meredith, “Separating Minimal, Intuitionist, and Classical Logic.” <a href="https://projecteuclid.org/download/pdf_1/euclid.ndjfl/1093870451">[PDF]</a></li>
<li>Michael Dummett, “The Philosophical Basis of Intuitionistic Logic.”</li>
</ul>
<p>On paraconsistent logic:</p>
<ul>
<li>Graham Priest, Koji Tanaka and Zach Weber, “Paraconsistent Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-paraconsistent/" class="uri">https://plato.stanford.edu/entries/logic-paraconsistent/</a>.</li>
</ul>
<p>On substructural logics, relevance logic and linear logic:</p>
<ul>
<li>Greg Restall, “Substructural Logics,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-substructural/" class="uri">https://plato.stanford.edu/entries/logic-substructural/</a>.</li>
<li>Edwin Mares, “Relevance Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-relevance/" class="uri">https://plato.stanford.edu/entries/logic-relevance/</a>.</li>
<li>Roberto Di Cosmo and Dale Miller, “Linear Logic,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/logic-linear/" class="uri">https://plato.stanford.edu/entries/logic-linear/</a>.</li>
</ul>
</blockquote>
<section id="towards-higher-order-logic-hol-and-type-theory" class="level2">
<h2>Towards higher-order logic (HOL) and type theory</h2>
<p>As discussed by the subsection of “The limitation of first-order logic” in <a href="/mst/7/">Mst. #7</a>, a practical issue about first-order theory is that it provides no way to express the so-called “well-ordering property” (or its equivalents like the second-order induction axiom) of sets: Every non-empty set of numbers has a least element. <span class="math display">\[\forall P (\exists x Px \to \exists x (Px \land \forall y (Py \to (x = y \lor x &lt; y))))\]</span></p>
<p>Recall that in first-order logic, we have notably the following model-theoretic results:</p>
<ol type="1">
<li>(Completeness) <span class="math inline">\(\Gamma \models \varphi \implies \Gamma \vdash \varphi\)</span>.</li>
<li>(Compactness) If every finite subset <span class="math inline">\(\Gamma_0\)</span> of <span class="math inline">\(\Gamma\)</span> is satisfiable, then <span class="math inline">\(\Gamma\)</span> is satisfiable.</li>
<li>(Enumerability) For a countable language, the set of valid formulas is computably enumerable.</li>
<li>(Löwenheim-Skolem theorem) For a countable language, if a set of sentences has any model, then it has a countable model.</li>
</ol>
<p>however, they do not generally hold in the standard semantics of second-order (or any higher-order) logic. As a naïve counterexample <span class="citation" data-cites="sep-logic-higher-order">[2]</span> disproving the compactness, consider a second-order sentence that defines a strict ordering <span class="math inline">\(R\)</span>: <span class="math display">\[\exists R (\forall x \forall y \forall z (Rxy \land Ryz \to Rxz) \land \forall x (\lnot Rxx \land \exists y Rxy))\]</span> let <span class="math inline">\(\lambda_\infty\)</span> be the above sentence, which is true if and only if the universe is an infinite set. Let <span class="math inline">\(\lambda_i\)</span> be the first-order sentence saying that “there are at least <span class="math inline">\(i\)</span> urelements in the universe”. Then the infinite set <span class="math display">\[\Gamma = \{ \lnot \lambda_\infty, \lambda_1, \lambda_2, \lambda_3, \dots \}\]</span> has no model, since <span class="math inline">\(\lnot \lambda_\infty \land \lambda_\omega\)</span> would be a contradiction. However, every finite subset of <span class="math inline">\(\Gamma\)</span> is clearly satisfiable.</p>
<p>Another intuitive way of thinking this is that since second-order logic is capable of <em>categorically</em> axiomatizing Peano arithmetic (i.e., there is at most one model of arithmetic up to isomorphism), but we would be able to build a non-standard model of arithmetic via ultraproducts given that the compactness theorem holds (as shown in <a href="/mst/7/">Mst. #7</a>), therefore the compactness theorem must be false then.</p>
<p>In second-order logic, we have two separate “stages” of ranges that one may quantify over: variables and predicates/functions, denoted as set membership: <span class="math display">\[v_0 \in v_1 \in \mathcal{P}(|\mathfrak{A}|)\]</span></p>
<p>Naturally, in an <span class="math inline">\(n\)</span>-th-order logic we have <span class="math display">\[v_0 \in v_1 \in \cdots \in v_{n-1} \in \mathcal{P}^{n-1}(|\mathfrak{A}|)\]</span></p>
<p>Once the order of logic is raised to the limit ordinal <span class="math inline">\(\omega\)</span>, we reach the level of <em>type theory</em>. <span class="citation" data-cites="sep-logic-higher-order">[2]</span> Sloppily, in type theory, a type <span class="math inline">\(\text{T}_i\)</span> is just a set <span class="math inline">\(V_i\)</span>, a function <span class="math inline">\(\text{f} : \text{T}_x \to \text{T}_y\)</span> is just a relation set <span class="math inline">\(f\)</span> such that <span class="math inline">\(\forall x \forall y_1 \forall y_2 ((\langle x, y_1 \rangle \in f \land \langle x, y_2 \rangle \in f) \to y_1 = y_2)\)</span>, where <span class="math inline">\(x \in V_x\)</span> and <span class="math inline">\(y_1, y_2 \in V_y\)</span>. Hence to accommodate the function type <span class="math inline">\(\text{f}\)</span>, one needs a power set operation correspondingly in set theory. The <span class="math inline">\(\omega\)</span>-infinite hierarchy of sets of sets: <span class="math display">\[V_0 \in V_1 \in \cdots \in V_n \in \cdots\]</span> where <span class="math inline">\(V_0\)</span> is the empty set <span class="math inline">\(\varnothing\)</span> (a set that has no members), corresponds to the <span class="math inline">\(\omega\)</span>-infinite hierarchy of types of types: <span class="math display">\[\text{Type}_0 : \text{Type}_1 : \cdots : \text{Type}_n : \cdots\]</span> where <span class="math inline">\(\text{Type}_0\)</span> is the bottom type (a type that has no values).</p>
<p>Both set theory and type theory can serve as logical foundations of mathematics <span class="citation" data-cites="sep-type-theory">[3]</span>, owing to their equivalent expressive power <span class="citation" data-cites="Werner97setsin">[4]</span>. Some differences are worth noting:</p>
<ol>
<li>Set theory is built on top of classical first-order logic (although there are recent proposals of intuitionistic set theories), with its semantics denoting a concrete model (e.g., ZFC); type theory is essentially an extension of higher-order logic, which is syntactically strong enough on its own.</li>
<li>Set theory tends to use a Hilbert-like deductive system, with its specific non-logical axioms (e.g., ZF axioms, Axiom of Choice); type theory tends to use a natural or Gentzen-like deductive system, with its specific logical rules of inference.</li>
<li>Set theory needs the Axiom of Regularity to establish its well-foundedness, and numbers must be encoded as well-founded sets; in type theory one may define inductive types directly, this makes handling models like Peano arithmetic more easily and naturally.</li>
<li>So as for a set theory to be constructive, one must reject both LEM (or its equivalents) and AC; a type theory is constructive as long as its underlying logic is constructive (i.e., rejecting LEM).</li>
<li>Type theory allows many sorts and has a straightforward correspondence with statically typed functional programming, i.e., “propositions as types” and “proofs as programs”. In set theory, functions must be uniquely encoded as power sets in a quite verbose way, which makes it unsuitable for computer-based proof mechanization.</li>
<li>Both naïve set theory and type theory were under the paradoxical crisis of inconsistency, as revealed by Russell’s paradox and Girard’s paradox respectively. The resolution of these paradoxes has led to multiple axiomatic set theories (e.g., ZF, NBG, NFU) and modern type theories (e.g., Martin-Löf Type Theory, Calculus of Inductive Constructions).</li>
<li>As an offshoot of set theory we can talk about large cardinals, whose existence is independent of (unprovable in) ZFC; there is no analog to these in proof-centric, constructive type theory yet.</li>
<li>Set theory was not well tailored for modeling today’s category theory, since not all categories are truly sets; in type theory, it is natural to think of types as objects and function types as morphisms, and a mutual interpretation between the semantics of type theory and category theory would be intuitively possible. <span class="citation" data-cites="sep-type-theory">[3]</span></li>
</ol>
</section>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Books:</strong></p>
<p>Aristotle, <em>Metaphysics</em>. (English translation by W. D. Ross, 1925)<br />
Available: <a href="https://ebooks.adelaide.edu.au/a/aristotle/metaphysics/" class="uri">https://ebooks.adelaide.edu.au/a/aristotle/metaphysics/</a></p>
<p>W. V. O. Quine, <em>Philosophy of Logic</em>, 2nd ed.</p>
<p>Michael Dummett, <em>Truth and Other Enigmas</em>.</p>
<p>Stewart Shapiro, <em>Foundations Without Foundationalism: A Case for Second-Order Logic</em>.</p>
<p><strong>Articles:</strong></p>
<div id="refs" class="references">
<div id="ref-hintikka1955two">
<p>[1] J. Hintikka, “Two papers on symbolic logic form and content in quantification theory and reductions in the theory of types,” 1955. </p>
</div>
<div id="ref-sep-logic-higher-order">
<p>[2] H. B. Enderton, “Second-order and higher-order logic,” in <em>The stanford encyclopedia of philosophy</em>, Fall 2015., E. N. Zalta, Ed. <a href="https://plato.stanford.edu/archives/fall2015/entries/logic-higher-order/" class="uri">https://plato.stanford.edu/archives/fall2015/entries/logic-higher-order/</a>; Metaphysics Research Lab, Stanford University, 2015. </p>
</div>
<div id="ref-sep-type-theory">
<p>[3] T. Coquand, “Type theory,” in <em>The stanford encyclopedia of philosophy</em>, Summer 2015., E. N. Zalta, Ed. <a href="https://plato.stanford.edu/archives/sum2015/entries/type-theory/" class="uri">https://plato.stanford.edu/archives/sum2015/entries/type-theory/</a>; Metaphysics Research Lab, Stanford University, 2015. </p>
</div>
<div id="ref-Werner97setsin">
<p>[4] B. Werner, “Sets in types, types in sets,” in <em>Proceedings of tacs’97</em>, 1997, pp. 530–546. </p>
</div>
</div>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>The Referential Use</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/8" />
    <id>tag:www.soimort.org,2017:/mst/8</id>
    <published>2017-09-26T00:00:00+02:00</published>
    <updated>2017-10-01T00:00:00+02:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>So I was trying to make some sense of the relevance between language and logic these days, and here comes my rendering of some fundamental issues in the philosophy of language. This should not be taken as a comprehensive summary on the topic, but hopefully it makes clearer how the translation from a natural language to a formal (e.g., first-order) logic can go unambiguously (or ambiguously). Moreover, each subtopic of notes is followed by a reading list introducing main concepts and debates, which are the “real fruits” here.</p>
<p>Inspired by Wittgenstein’s theory of “Meaning as Use”, it is fair to say that when we use a word, we just mean it. The meaningful use of a word and a mention of it must be distinguished. Quotation, as the canonical device of making the distinction, is central to debates on this topic. Davidson argued that even quoted words can be both a use and a mention. Quine’s invention of quasi-quotations (as an alternative quotational device) is of technical importance, and this notion has found its use in inductive definitions of formal logic, denotational semantics, metaprogramming in Lisp and string interpolations in many practical programming languages.</p>
<p>As firstly noticed by Frege, the semantics of words embraces two differently supporting aspects: sense and reference. Referring expressions such as descriptions and proper names, may both be denoted using a descriptivist theory (proposed by Russell); the Russellian view of names, however, was criticized by Kripke in favor of a many-worlds causal view.</p>
<hr />
<p><strong>On quotation.</strong> In the following text, we will utilize English quotation marks intensively. Quoted text is used as a literal mention of things whatever they enclose. By contrast, unquoted text is used as whatever its meaning is. For example,</p>
<p style="text-align:center !important;text-indent:0 !important"><em>I</em> am who I am.</p>
<p>but</p>
<p style="text-align:center !important;text-indent:0 !important"><em>“I”</em> is a word. <em>“I”</em> is not who I am.</p>
<p>(This issue will be discussed further in “Use-mention distinction”.)</p>
<p><strong>Statement.</strong> A <em>statement</em> is defined as a <em>sentence</em> that says something is <em>true</em>. The notions of English words “sentence” and “true” are clear in this informal context, and they all have their naïve meanings. “Something” is just an object (or objects) that the statement talks about.</p>
<p><strong>Metalanguage and object language.</strong> To make a statement about something, we may use either a formal or a natural language. When that “something” is some language (or languages), we refer to it as the <em>object language</em>(s) that we are talking about and the language we are using to make the very statement as the <em>metalanguage</em>. For example,</p>
<p style="text-align:center !important;text-indent:0 !important">The German phrase “ein weißes Pferd ist weiß” means “a white horse is white” in English.</p>
<p>Here we are talking about the isomorphic meanings of two sentences in German and in English (as our object languages), and English is the metalanguage that we use to make the statement. It doesn’t matter which metalanguage we use and whether the metalanguage is identical to one of the object languages, as long as it is capable of representing the semantics we want; The following statement (using Esperanto as the metalanguage) has the same meaning as above:</p>
<p style="text-align:center !important;text-indent:0 !important">La germana frazo “ein weißes Pferd ist weiß” signifas “a white horse is white” en la angla.</p>
<blockquote style="background:gainsboro; border-radius:1em; padding:.25em 1em; font-size:.8em">
<p style="text-align:center !important;text-indent:0 !important"><strong><em>Reading list</em></strong></p>
<p>On the application of metalanguages and object languages in mathematical logic, see:</p>
<ul>
<li>G. J. Mattey, “Object Language and Metalanguage.” <a href="http://hume.ucdavis.edu/mattey/phi112/objectmeta_ho.pdf">[PDF]</a></li>
<li>“Metalanguage,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Metalanguage" class="uri">https://en.wikipedia.org/wiki/Metalanguage</a>.</li>
<li>“Object Language,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Object_language" class="uri">https://en.wikipedia.org/wiki/Object_language</a>.</li>
</ul>
</blockquote>
<p><strong>Ambiguity in interpretations.</strong> It might be tempting to just use a natural language as our metalanguage, but ambiguity could arise due to the sloppy nature of any human languages. Consider the following statement in Chinese:</p>
<p style="text-align:center !important;text-indent:0 !important">白馬非馬。<br />
(A white horse is not a horse.)</p>
<p>The argument is that the intension of “white horse” consists of the properties of being white and being horse-like, while the intension of “horse” consists merely of the property of being horse-like, hence conceptually “white horse” and “horse” are not identical to each other. However, if we interpret “是 (be)” as “be a kind of” thus “非 (be not)” as “be not a kind of”, as in the usual context, we could immediately tell that it is absurd to say:</p>
<p style="text-align:center !important;text-indent:0 !important">白馬非馬。<br />
(A white horse is not a kind of horses.)</p>
<p>Natural languages can be vague, however we are almost always in need of a definite interpretation when studying the semantics (of whatever language we like). It is most advisable to employ a formal language so that we don’t fall into dialectic traps like this. Consider the formal language of set theory, where we define a relation <span class="math inline">\(P_H\)</span> (“being horse-like”) such that <span class="math inline">\(P_H x\)</span> if and only if <span class="math inline">\(x \in P_H\)</span>, and a relation <span class="math inline">\(P_W\)</span> (“being white”) such that <span class="math inline">\(P_W x\)</span> if and only if <span class="math inline">\(x \in P_W\)</span>. Then we can write the first meaning of the informal statement sloppily, but unambiguously as: <span class="math display">\[\{ x \in P_H : x \in P_W \} \neq P_H\]</span> which is true if and only if there is at least one <span class="math inline">\(x&#39;\)</span> such that <span class="math inline">\(x&#39; \in P_H \land x&#39; \not\in P_W\)</span>, so that <span class="math inline">\(x&#39; \not\in \{ x \in P_H : x \in P_W \}\)</span> but <span class="math inline">\(x&#39; \in P_H\)</span>. There is a plausible argument by nature, since we are easily convinced that “There is a horse that is not white”.</p>
<p><strong><em>Exercise 8.1.</em></strong> Rewrite <span class="math inline">\(\{ x \in P_H : x \in P_W \} \neq P_H\)</span> formally (i.e., without using set comprehension), in the first-order language of set theory containing only <span class="math inline">\(\in\)</span> as a predicate symbol.</p>
<p>The second meaning of the statement can be unambiguously written as: <span class="math display">\[\forall x ((x \in P_H \land x \in P_W) \to x \not\in P_H)\]</span> which is true if and only if <span class="math inline">\(\forall x (\lnot (x \in P_H \land x \in P_W))\)</span>, i.e., “There is no such thing as a white horse”; or even <span class="math inline">\(\forall x (x \not\in P_H)\)</span>, i.e., “Nothing is a horse”. Both are ungrounded beliefs contradicting our empirical evidence.</p>
<p><strong>Ambiguity in referring expressions.</strong> Difficulties in translating from a natural language to a formal language often arise in unintended ways. Consider again, the statement:</p>
<p style="text-align:center !important;text-indent:0 !important">A white horse is not a kind of horses.</p>
<p>Doe the mentioned “white horse” refer to any arbitrary object that simply holds the properties of being a white horse, or a very specific white horse that one happens to find to be not a kind of horses? This distinction is significant because in the latter case, we have to formulate the statement using an existential quantification instead of the universal quantification: <span class="math display">\[\exists x ((x \in P_H \land x \in P_W) \land x \not\in P_H)\]</span> that gives us a third meaning of the statement “白馬非馬”.</p>
<p><strong><em>Exercise 8.2.</em></strong> (1) Can it be the case that the sentence <span class="math inline">\(\exists x ((x \in P_H \land x \in P_W) \land x \not\in P_H)\)</span> is true? (2) Why don’t we formulate its meaning as <span class="math inline">\(\exists x ((x \in P_H \land x \in P_W) \to x \not\in P_H)\)</span>?</p>
<p><strong>Descriptions and names.</strong> To eliminate any potential ambiguity when referring to something in a natural language, we firstly review three kinds of referring expressions:</p>
<ol>
<li><strong>Demonstratives</strong>. Examples: “this”; “this horse”; “that white horse”.</li>
<li><strong>Descriptions</strong>.
<ul>
<li><em>Indefinite</em> descriptions. Examples: “a white horse”.</li>
<li><em>Definite</em> descriptions. Examples: “the white horse”.</li>
</ul></li>
<li><strong>Proper names</strong> (<em>names</em> that uniquely identify their referents). Examples: “Princess Celestia”; “Rainbow Dash”.</li>
</ol>
<p>A referring expression (singular term) picks out a particular object (or some particular objects) that the sentence talks about. The demonstratives are clearly context sensitive, so for now we will consider only descriptions and proper names for the sake of unambiguity.</p>
<p>For indefinite descriptions, consider the following sentence:</p>
<p style="text-align:center !important;text-indent:0 !important">A white horse is white.</p>
<p>Based on our preceding discussion, there are two different ways of formulating it:</p>
<p style="text-align:center !important;text-indent:0 !important">(If there is a white horse, then it is white.) <span class="math display">\[\forall x ((x \in P_H \land x \in P_W) \to x \in P_W)\]</span></p>
<p>and</p>
<p style="text-align:center !important;text-indent:0 !important">(There is a white horse which is white.) <span class="math display">\[\exists x ((x \in P_H \land x \in P_W) \land x \in P_W)\]</span></p>
<p>N.B. The latter (and weaker) formulation is proposed by <a href="https://plato.stanford.edu/entries/russell/">Bertrand Russell</a>. <span class="citation" data-cites="sep-descriptions">[1]</span></p>
<p>For definite descriptions,</p>
<p style="text-align:center !important;text-indent:0 !important">The white horse is white.</p>
<p>How do we understand its semantics? Russell proposed the following formulation:</p>
<p style="text-align:center !important;text-indent:0 !important">(There is exactly one white horse which is white.) <span class="math display">\[\exists! x ((x \in P_H \land x \in P_W) \land x \in P_W)\]</span></p>
<p><strong><em>Exercise 8.3.</em></strong> Rewrite the above sentence using the existential quantifier.</p>
<p>Russell’s theory of descriptions can lead to justified denials of some obviously ridiculous statements. For example, the sentence</p>
<p style="text-align:center !important;text-indent:0 !important">The invisible pink unicorn is white.</p>
<p>is puzzling, since we have no idea what an “invisible pink unicorn” would be like. Most of us would agree that there is no such thing as an invisible pink unicorn. Can the above statement be true then? By Russell’s formulation of definite descriptions, we say that this is equivalent to</p>
<p style="text-align:center !important;text-indent:0 !important">There is exactly one invisible pink unicorn which is white.</p>
<p>Then we can immediately assert that it is not true, given our belief that no invisible pink unicorn really exists.</p>
<p>As an alternative view of the theory of descriptions, <a href="https://plato.stanford.edu/entries/frege/">Gottlob Frege</a> suggested that a sentence like above makes use of a definite description that fails to refer (as “invisible pink unicorn” does not refer to anything), thus it does not express a proposition and has no truth value. <span class="citation" data-cites="strawson1950referring">[2]</span></p>
<p>For proper names, we could say things like</p>
<p style="text-align:center !important;text-indent:0 !important">Princess Celestia is white.</p>
<p>It has been disputed, however, whether a descriptivist theory can be applied on proper names. Russell argued that most names have some descriptive meanings, e.g., “Princess Celestia” can be descriptively understood as “The benevolent ruler of Equestria”. Modern causalists such as <a href="https://scholar.google.com/citations?user=MRCc_ugAAAAJ">Saul Kripke</a>, criticized the descriptivist theory <span class="citation" data-cites="kripke1972naming">[3]</span>, for that a name need not be a uniquely identifying description so as to refer to an object; rather, there must be a <em>causal chain</em> that makes the name a reference. For example, we know that the name “Princess Celestia” and the description “The benevolent ruler of Equestria” refer to the same referent because she defeated King Sombra and took over Equestria, or more realistically, the story-writer let her be so. However, in an alternate world where King Sombra still rules Equestria, the name “Princess Celestia” would not have the same referent as “The benevolent ruler of Equestria” (in fact the latter term would actually refer to nothing), but “Princess Celestia” will always be Princess Celestia; that is, a proper name is a <a href="https://en.wikipedia.org/wiki/Rigid_designator"><em>rigid designator</em></a> of objects, while a definite description may refer to very different things due to varying causality in different worlds. Kripke’s causal theory of reference suggested that, a name refers uniquely to the same object in every possible world, regardless of any particular facts about it. By contrast, the descriptivist view of a name may fail to capture its referent consistently in all worlds.</p>
<p><strong>Sense and reference.</strong> As per discussion on descriptions and names, it is essential to make a distinction between a <em>sense</em> and a <em>reference</em> (this distinction is attributed to Frege’s 1892 work “Über Sinn und Bedeutung” (<em>On Sense and Reference</em>)). A reference denotes a particular object in the world that the term applies, while a sense denotes the way in which the term presents, regardless of what the actual referent is (or whether there is a referent). For example, the description “The invisible pink unicorn” clearly makes sense, but it has no reference in any possible physical world. As a second example, the proper name “Princess Celestia” and the description “The benevolent ruler of Equestria” have different senses (i.e., “Princess Celestia” means Princess Celestia, “The benevolent ruler of Equestria” means the benevolent ruler of Equestria, and these two meanings are never the same), however, they do refer to the same thing in this world.</p>
<p>It is worth noting that senses may not be injectively mapped to words or phrases, as a word can present different senses in different contexts. For example, the word “structure” has a meaning in mathematical logic, and it also has a meaning in abstract algebra. Despite that their senses are similar in essence, they apply to different domains of mathematics and should not be taken as identical. They can, nonetheless, have one common reference when we talk about a formal language <span class="math inline">\(\mathcal{A}\)</span> of algebra, studying the theory of a particular algebraic structure (e.g., abelian group <span class="math inline">\((A, \cdot)\)</span>) thus it is also a structure <span class="math inline">\(\mathfrak{A}\)</span> that we assign to our language <span class="math inline">\(\mathcal{A}\)</span>.</p>
<blockquote style="background:gainsboro; border-radius:1em; padding:.25em 1em; font-size:.8em">
<p style="text-align:center !important;text-indent:0 !important"><strong><em>Reading list</em></strong></p>
<p>On a first introduction to descriptions, proper names, Fregean-Russellian descriptivist theory and Kripke’s causal theory, see Wikipedia:</p>
<ul>
<li>“Sense and Reference,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Sense_and_reference" class="uri">https://en.wikipedia.org/wiki/Sense_and_reference</a>.</li>
<li>“Definite Description,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Definite_description" class="uri">https://en.wikipedia.org/wiki/Definite_description</a>.</li>
<li>“Proper Name,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Proper_name_(philosophy)" class="uri">https://en.wikipedia.org/wiki/Proper_name_(philosophy)</a>.</li>
<li>“Descriptivist Theory of Names,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Descriptivist_theory_of_names" class="uri">https://en.wikipedia.org/wiki/Descriptivist_theory_of_names</a>.</li>
<li>“Causal Theory of Reference,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Causal_theory_of_reference" class="uri">https://en.wikipedia.org/wiki/Causal_theory_of_reference</a>.</li>
</ul>
<p>Further reading on descriptions, names and the issue of reference:</p>
<ul>
<li>Peter Ludlow, “Descriptions,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/descriptions/" class="uri">https://plato.stanford.edu/entries/descriptions/</a>.</li>
<li>Marga Reimer and Eliot Michaelson, “Reference,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/reference/" class="uri">https://plato.stanford.edu/entries/reference/</a>.</li>
<li>P. F. Strawson, “On Referring.” <a href="http://semantics.uchicago.edu/kennedy/classes/f09/semprag1/strawson50.pdf">[PDF]</a></li>
</ul>
<p>On Frege’s innovative distinction between sense and reference:</p>
<ul>
<li>Gottlob Frege, “On Sense and Reference.” <a href="http://www.scu.edu.tw/philos/98class/Peng/05.pdf">[PDF]</a> (English translation of “Über Sinn und Bedeutung”)</li>
<li>Michael Dummett, “Frege’s Distinction Between Sense and Reference.”</li>
</ul>
<p>Russell’s original essay on his theory of descriptions:</p>
<ul>
<li>Bertrand Russell, “On Denoting.” <a href="https://www.uvm.edu/~lderosse/courses/lang/Russell(1905).pdf">[PDF]</a>
<ul>
<li>“On Denoting,” Wikipedia, <a href="https://en.wikipedia.org/wiki/On_Denoting" class="uri">https://en.wikipedia.org/wiki/On_Denoting</a>.</li>
</ul></li>
</ul>
<p>Kripke’s objections to descriptivist theories of proper names:</p>
<ul>
<li>Saul Kripke, “Naming and Necessity.” <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.472.9081&amp;rep=rep1&amp;type=pdf">[PDF]</a></li>
</ul>
</blockquote>
<p><strong>Use-mention distinction.</strong> When trying to understand the semantics of a natural language, it is also critical to make the distinction between <em>using</em> a word (or phrase) and <em>mentioning</em> it. Consider again, the following sentence:</p>
<p style="text-align:center !important;text-indent:0 !important">The white horse is white.</p>
<p>This sentence is true because a white horse is indeed, white (in the conventional interpretation). Furthermore,</p>
<p style="text-align:center !important;text-indent:0 !important">“The white horse” is a definite description.</p>
<p>This sentence is also true because the phrase “the white horse” is well defined with a meaning that refers to the collection of horses that are white. However, it does not make much sense to say:</p>
<p style="text-align:center !important;text-indent:0 !important">The white horse is a definite description.</p>
<p>Since a white horse is just a white horse, not a word or phrase with meanings. And we would also agree that</p>
<p style="text-align:center !important;text-indent:0 !important">“The white horse” is white.</p>
<p>is confusing in some way, because the quotation marks suggest that “the white horse” may not have the intended meaning, i.e., it may appear as a literal mention of the English phrase “white horse” instead.</p>
<p>It is worth noting that though quotation marks are often an indication of mention while unquoted text is simply used, as in the above example, it is not always the case. <a href="https://plato.stanford.edu/entries/davidson/">Donald Davidson</a> argued that “mixed quotation” exists so that quoted words can be simultaneously used and mentioned. <span class="citation" data-cites="Davidson1979">[4]</span></p>
<p><strong>Quasi-quotation.</strong> While quotation marks can be used for denoting a mention, a practical issue is that when mentioning something (e.g., a definition), we sometimes need to <em>use</em> the reference of a part of the mention inside it. Recall that when defining a first-order language in <a href="/mst/7/">Mst. #7</a>, we made claims like</p>
<p style="text-align:center !important;text-indent:0 !important">If <span class="math inline">\(\psi\)</span> is a well-formed formula, <span class="math inline">\((\lnot\psi)\)</span> is a well-formed formula.</p>
<p>Note that the mention is implicit here, without any extra meta-symbols. Can we just make the definition using quotation marks?</p>
<p style="text-align:center !important;text-indent:0 !important">If <span class="math inline">\(\psi\)</span> is a well-formed formula, “<span class="math inline">\((\lnot\psi)\)</span>” is a well-formed formula.</p>
<p>It is immediately clear that the above definition does not fit our intended purpose: Let the open term (in our metalanguage) <span class="math inline">\(\psi\)</span> be the formula (in the object language) “<span class="math inline">\((\alpha \to \beta)\)</span>”, then what we are saying basically is</p>
<p style="text-align:center !important;text-indent:0 !important">If “<span class="math inline">\((\alpha \to \beta)\)</span>” is a well-formed formula, “<span class="math inline">\((\lnot\psi)\)</span>” is a well-formed formula.</p>
<p>This sentence does not really make an inductive definition as intended. The problem is that the symbol “<span class="math inline">\(\psi\)</span>” in “‘<span class="math inline">\((\lnot\psi)\)</span>’” is not interpolated as what its use is (i.e., “<span class="math inline">\((\alpha \to \beta)\)</span>”), but only appears as a literal Greek letter in the mention. To resolve this issue, <a href="https://plato.stanford.edu/entries/quine/">W. V. O. Quine</a> introduced the meta-symbols called <em>quasi-quotation</em> <span class="citation" data-cites="Quine1940-QUIML">[5]</span>, so that it is technically feasible to make inductive definitions like this:</p>
<p style="text-align:center !important;text-indent:0 !important">If <span class="math inline">\(\psi\)</span> is a well-formed formula, ⌜<span class="math inline">\((\lnot\psi)\)</span>⌝ is a well-formed formula.</p>
<p>Then let <span class="math inline">\(\psi\)</span> be “<span class="math inline">\((\alpha \to \beta)\)</span>”, ⌜<span class="math inline">\((\lnot\psi)\)</span>⌝ becomes “<span class="math inline">\((\lnot (\alpha \to \beta))\)</span>”. We have that</p>
<p style="text-align:center !important;text-indent:0 !important">If “<span class="math inline">\((\alpha \to \beta)\)</span>” is a well-formed formula, “<span class="math inline">\((\lnot (\alpha \to \beta))\)</span>” is a well-formed formula.</p>
<p>just as intended.</p>
<blockquote style="background:gainsboro; border-radius:1em; padding:.25em 1em; font-size:.8em">
<p style="text-align:center !important;text-indent:0 !important"><strong><em>Reading list</em></strong></p>
<p>On the importance of use-mention distinction:</p>
<ul>
<li>“Use-mention Distinction,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Use–mention_distinction" class="uri">https://en.wikipedia.org/wiki/Use–mention_distinction</a>.</li>
<li>W. V. O. Quine, <em>Mathematical Logic</em>, chap. 1.4. (Use versus Mention)</li>
<li>A. W. Moore, “How significant is the use/mention distinction?”</li>
</ul>
<p>On quotation:</p>
<ul>
<li>W. V. O. Quine, <em>Mathematical Logic</em>, chap. 1.5. (Statements about Statements)</li>
<li>Herman Cappelen and Ernest Lepore, “Quotation,” Stanford Encyclopedia of Philosophy, <a href="https://plato.stanford.edu/entries/quotation/" class="uri">https://plato.stanford.edu/entries/quotation/</a>.</li>
<li>Donald Davidson, “Quotation.”</li>
</ul>
<p>On the practical device of quasi-quotation:</p>
<ul>
<li>“Quasi-quotation,” Wikipedia, <a href="https://en.wikipedia.org/wiki/Quasi-quotation" class="uri">https://en.wikipedia.org/wiki/Quasi-quotation</a>.</li>
<li>W. V. O. Quine, <em>Mathematical Logic</em>, chap. 1.6. (Quasi-Quotation)</li>
</ul>
</blockquote>
<section id="whats-coming-next" class="level2">
<h2>What’s coming next</h2>
<p>The use-mention distinction and the auxiliary device of quasi-quotation are philosophically essential not only for the definition of formulas/sentences in formal languages, but also for Alfred Tarski’s semantic theory of <em>truth</em> and <em>models</em>. Deeper debates on the essence of truth often involve the correspondence theory, the deflationary theory and the coherence theory. It is also informing to know how the <em>liar’s paradox</em> and <em>Quine’s paradox</em> can arise from direct or indirect self-referential statements, and what Tarski’s undefinability theorem implies. Michael Dummett’s characterization of the principle of bivalence (in classical logic) as <em>realism</em>, together with the defence of <em>intuitionism</em> (an anti-realism), are also highly relevant for understanding the philosophical basis of modern logic.</p>
</section>
<section id="solutions-to-exercises" class="level2">
<h2>Solutions to exercises</h2>
<p><strong><em>Exercise 8.1.</em></strong> <span class="math inline">\(\{ x \in P_H : x \in P_W \} \neq P_H\)</span> is just <span class="math display">\[\lnot ((x \in P_H \land x \in P_W) \leftrightarrow x \in P_H)\]</span> which is just <span class="math display">\[((\lnot (x \in P_H \to (\lnot x \in P_W))) \to x \in P_H) \to (\lnot (x \in P_H \to (\lnot (x \in P_H \to (\lnot x \in P_W)))))\]</span></p>
<p><strong><em>Exercise 8.2.</em></strong> (1) Notice that <span class="math inline">\(\lnot ((x \in P_H \land x \in P_W) \land x \not\in P_H)\)</span> is a tautology, by generalization we get that <span class="math inline">\(\forall x (\lnot ((x \in P_H \land x \in P_W) \land x \not\in P_H))\)</span>. By double negation <span class="math display">\[\lnot (\lnot \forall x (\lnot ((x \in P_H \land x \in P_W) \land x \not\in P_H)))\]</span> which is just <span class="math display">\[\lnot \exists x ((x \in P_H \land x \in P_W) \land x \not\in P_H)\]</span> so <span class="math inline">\(\exists x ((x \in P_H \land x \in P_W) \land x \not\in P_H)\)</span> cannot be satisfied by any interpretation. (or: it is never true.)</p>
<p>(2) <span class="math inline">\(\exists x ((x \in P_H \land x \in P_W) \to x \not\in P_H)\)</span> is equivalent to <span class="math display">\[\lnot \forall x (\lnot ((x \in P_H \land x \in P_W) \to x \not\in P_H))\]</span> Assume that there is an <span class="math inline">\(x_c\)</span> such that <span class="math inline">\(x_c \not\in P_H\)</span>, then <span class="math inline">\((x_c \in P_H \land x_c \in P_W) \to x_c \not\in P_H\)</span> would be true.</p>
<p>Notice that given <span class="math inline">\(\Gamma; \forall x (\lnot ((x \in P_H \land x \in P_W) \to x \not\in P_H))\)</span>, by substituting <span class="math inline">\(x\)</span> with <span class="math inline">\(x_c\)</span> we get <span class="math inline">\(\lnot ((x_c \in P_H \land x_c \in P_W) \to x_c \not\in P_H)\)</span>, but that would be inconsistent with our previous assumption. Thus by Reductio ad Absurdum, <span class="math inline">\(\Gamma \vdash \lnot \forall x (\lnot ((x \in P_H \land x \in P_W) \to x \not\in P_H))\)</span>.</p>
<p>Now that we can be easily convinced that our assumption holds, that is, there exists something that is not a horse. So this formulation would be trivially true!</p>
<p><strong><em>Exercise 8.3.</em></strong> <span class="math display">\[\exists x (((x \in P_H \land x \in P_W) \land \forall y ((y \in P_H \land y \in P_W) \to y = x)) \land x \in P_W)\]</span></p>
</section>
<section id="references" class="level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-sep-descriptions">
<p>[1] P. Ludlow, “Descriptions,” in <em>The stanford encyclopedia of philosophy</em>, Fall 2013., E. N. Zalta, Ed. <a href="https://plato.stanford.edu/archives/fall2013/entries/descriptions/" class="uri">https://plato.stanford.edu/archives/fall2013/entries/descriptions/</a>; Metaphysics Research Lab, Stanford University, 2013. </p>
</div>
<div id="ref-strawson1950referring">
<p>[2] P. F. Strawson, “On referring,” <em>Mind</em>, vol. 59, no. 235, pp. 320–344, 1950. </p>
</div>
<div id="ref-kripke1972naming">
<p>[3] S. A. Kripke, “Naming and necessity,” in <em>Semantics of natural language</em>, Springer, 1972, pp. 253–355. </p>
</div>
<div id="ref-Davidson1979">
<p>[4] D. Davidson, “Quotation,” <em>Theory and Decision</em>, vol. 11, no. 1, pp. 27–40, Mar. 1979 [Online]. Available: <a href="https://doi.org/10.1007/BF00126690" class="uri">https://doi.org/10.1007/BF00126690</a></p>
</div>
<div id="ref-Quine1940-QUIML">
<p>[5] W. V. Quine, <em>Mathematical logic</em>. Cambridge: Harvard University Press, 1940. </p>
</div>
</div>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>The Logical Implication</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/7" />
    <id>tag:www.soimort.org,2017:/mst/7</id>
    <published>2017-09-06T00:00:00+02:00</published>
    <updated>2017-09-09T00:00:00+02:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p><strong>Prologue.</strong> This is the summary of a set of notes I took in KU’s <a href="http://kurser.ku.dk/course/nmaa13036u/2016-2017">Introduction to Mathematical Logic</a> course, that I should have finished months ago but somehow procrastinated until recently. I basically followed Enderton’s <em>A Mathematical Introduction to Logic</em> book (with all its conventions of mathematical notations). A few issues to address in advance:</p>
<ul>
<li>These notes are mainly about first-order logic. <em>Propositional logic</em> is assumed to be a prerequisite. The important thing to know is that its semantics (as a Boolean algebra) can be fully described by a truth table, which is obviously finite and mechanizable.</li>
<li><em>Naïve set theory</em> is also a prerequisite, that is, one must accept the notion of <em>sets</em> unconditionally: A set is just a collection of objects, and it must exist as we describe it. We are convinced that no paradoxical use of sets will ever emerge, and that no “naïve set” could be a proper class. The formal notion of sets is built upon first-order logic, but without some informal reference to sets we can’t even say what a first-order logic is.
<ul>
<li>A set <span class="math inline">\(S\)</span> is said to be “countable” iff there exists a bijection <span class="math inline">\(f : \mathbb{N} \to S\)</span>; otherwise it is “uncountable”.</li>
</ul></li>
<li>One must be willing to accept the validity of mathematical induction on natural numbers (or the well-ordering principle for natural numbers). Again, to formalize the induction principle we need the set theory, but we don’t have it in the first place (until we introduce a first-order logic).</li>
<li>On the <em>model-theoretic</em> part: Notes on <a href="https://wiki.soimort.org/math/logic/fol/definability/">definability and homomorphisms</a> are omitted from this summary, not to mention ultraproducts and the Löwenheim-Skolem theorem. Model theory is a big topic in its own right and deserves an individual treatment, better with some algebraic and topological contexts. (Note that the homomorphism theorem is used in proving the completeness theorem; also the notion of definability is mentioned in the supplementary sections.)</li>
<li>On the <em>proof-theoretic</em> part: Notes on <a href="https://wiki.soimort.org/math/logic/fol/metatheorems/">some metatheorems</a> are also omitted, as they are a purely technical aspect of a Hilbert-style deductive system. (One may find it convenient to prove an actual theorem with more metatheorems, but they are really not adding any extra power to our system.)</li>
<li>The relation between logic and <em>computability</em> (i.e., <em>Gödel’s incompleteness theorems</em>) is not discussed.
<ul>
<li>But the meanings of “decidable” and “undecidable” are clear from the previous notes <a href="/mst/6/">Mst. #6</a> (from a computer scientist’s perspective).</li>
</ul></li>
<li><em>Axiomatic set theory</em>, which is another big part of the course, is not included in these notes. (Maybe I’m still too unintelligent to grasp the topic.) But it is good to know:
<ul>
<li>First-order logic has its limitation in definability (i.e., it’s not capable of ruling out non-standard models of arithmetic), until we assign to it a set-theoretic context. So set theory is often considered a foundation of all mathematics (for its expressive power).</li>
<li>Axiom of Choice (AC) causes some counter-intuitive consequences, but it was shown to be consistent with ZF axioms (Gödel 1938). And there are models of ZF<span class="math inline">\(\cup\lnot\)</span>AC so well as ZF<span class="math inline">\(\cup\)</span>AC.</li>
<li>Constructivists tend to avoid AC in mathematics. However, <a href="https://wiki.soimort.org/math/logic/fol/completeness/">Henkin’s proof of the completeness theorem</a> in first-order logic assumes AC (Step II in finding a maximal consistent set). (thus it is a non-constructive proof!<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>)</li>
</ul></li>
<li><em>Intuitionistic logic</em> is not in the scope of these course notes. (And most logic books, including the Enderton one, are not written by constructive mathematicians.) Basically, in a Hilbert-style system, a classical logic would admit all tautologies in propositional logic as Axiom Group 1. Intuitionistic logic, in contrast, rejects those tautologies that are non-constructive in a first-logic setting.</li>
</ul>
<hr />
<ul>
<li><a href="https://wiki.soimort.org/math/logic/fol/languages/"><strong>First-order language</strong></a>: A <a href="https://wiki.soimort.org/comp/language/">formal language</a> consisting of the following symbols:
<ol type="1">
<li>Logical symbols
<ul>
<li>Parentheses: <span class="math inline">\((\)</span>, <span class="math inline">\()\)</span>.</li>
<li>Connective symbols: <span class="math inline">\(\to\)</span>, <span class="math inline">\(\lnot\)</span>.</li>
<li>Variables: <span class="math inline">\(v_1\)</span>, <span class="math inline">\(v_2\)</span>, …</li>
<li>Equality symbol (optional 2-place predicate symbols): <span class="math inline">\(=\)</span>.</li>
</ul></li>
<li>Parameters (non-logical symbols; open to interpretation)
<ul>
<li>Universal quantifier symbol: <span class="math inline">\(\forall\)</span>.</li>
<li>Predicate symbols (relation symbols): <span class="math inline">\(P_1\)</span>, <span class="math inline">\(P_2\)</span>, …</li>
<li>Constant symbols (0-place function symbols): <span class="math inline">\(c_1\)</span>, <span class="math inline">\(c_2\)</span>, …</li>
<li>Function symbols: <span class="math inline">\(f_1\)</span>, <span class="math inline">\(f_2\)</span>, …</li>
</ul></li>
</ol>
<ul>
<li>When specifying a concrete first-order language <span class="math inline">\(\mathcal{L}\)</span>, we must say (i) whether the quality symbol is present; (ii) what the parameters are.</li>
</ul></li>
</ul>
<p><strong>Remark 7.1. (Language of propositional logic)</strong> The language of propositional logic may be seen as a stripped form of first-order languages, in which parentheses, connective symbols and sentential symbols (the only parameters; may be treated as 0-place predicate symbols) are present. Intuitively, that language might seem too weak to encode our formal reasoning in all kinds of mathematics and many practical areas, so to speak.</p>
<ul>
<li><strong>Terms</strong> and <strong>formulas</strong>
<ul>
<li>An <em>expression</em> is a finite sequence of symbols (i.e., a finite string). Among all expressions, we are interested in two kinds of them which we refer to as terms and formulas.</li>
<li>A <em>term</em> is either:
<ul>
<li>a single variable or constant symbol; or</li>
<li><span class="math inline">\(f t_1 \cdots t_n\)</span>, where <span class="math inline">\(f\)</span> is a <span class="math inline">\(n\)</span>-place function symbol, and every <span class="math inline">\(t_i\)</span> <span class="math inline">\((1 \leq i \leq n)\)</span> is also a term.</li>
</ul></li>
<li>A <em>formula</em> (or <em>wff</em>, well-formed formula) is either:
<ul>
<li><span class="math inline">\(P t_1 \cdots t_n\)</span>, where <span class="math inline">\(P\)</span> is a <span class="math inline">\(n\)</span>-place predicate symbol (or the equality symbol <span class="math inline">\(=\)</span>), and every <span class="math inline">\(t_i\)</span> <span class="math inline">\((1 \leq i \leq n)\)</span> is a term; or</li>
<li>one of the following forms:
<ul>
<li><span class="math inline">\((\lnot \psi)\)</span>, where <span class="math inline">\(\psi\)</span> is also a formula;</li>
<li><span class="math inline">\((\psi \to \theta)\)</span>, where <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\theta\)</span> are also formulas;</li>
<li><span class="math inline">\(\forall v_i \psi\)</span>, where <span class="math inline">\(v_i\)</span> is a variable and <span class="math inline">\(\psi\)</span> is also a formula.</li>
</ul></li>
</ul></li>
<li>A variable may occur <em>free</em> in a formula. A formula without any free variable is called a <em>sentence</em>.</li>
</ul></li>
</ul>
<p><strong>Remark 7.2. (Metatheory and philosophical concerns)</strong> A first-order expression, as a finite sequence (also called a <em>tuple</em>), may be defined in terms of ordered pairs in axiomatic set theory. But we will not appeal to set theory in our first definitions of expressions in logic. (So far we have no notion about what a “set” formally is!)</p>
<p>A further concern is whether our definitions of terms and formulas are well established, that is, since we are defining the notions of terms and formulas <em>inductively</em>, would it be possible that there is a certain term or formula that is covered by our recursive definition, but can never be actually built using these operations? To show that first-order terms/formulas are well-defined, a beginner might try to prove these induction principles by mathematical induction on the complexities of terms/formulas, but that would rely on the fact that the set of natural numbers <span class="math inline">\(\omega\)</span> is well-ordered so that we can apply induction on numbers; to justify things like this, it is essential to use set theory or second-order logic, which we don’t even have until we define a first-order logic. Thus, unavoidable circularity emerges if we try to look really fundamentally.</p>
<p>For now, we must appeal to a metatheory that we can easily convince ourselves by intuition, so that we will accept these induction principles and the notion of “naïve sets” (or <em>collections</em>, if we don’t want to abuse the formal term of sets too much). Notwithstanding, I <em>believe</em> that a prudent person can bootstrap theories like this without drawing in any inconsistency.</p>
<p><strong>Remark 7.3. (Context freedom, unique readability and parentheses)</strong> Since the formations of first-order terms and formulas make use of context-free rules, one familiar with <a href="/mst/6/">formal languages and automata theory</a> might ask, “Are the set of terms/formulas <a href="https://wiki.soimort.org/comp/language/context-free/">context-free languages</a>?” Generally they are not, since our set <span class="math inline">\(V\)</span> of variables (samely for predicate and function symbols) could be infinitely (or even uncountably) large, but a context-free grammar requires that every set must be finite. However, in our first-order language <span class="math inline">\(\mathcal{L}\)</span>, if these symbols can be effectively decidable, then there is an algorithm that accepts terms or formulas (or parses them). Furthermore, such parses are guaranteed to be unique, as shown by the Unique Readability Theorems in Enderton p. 105ff. Indeed, the inclusion of parentheses in our first-order language enables us to write any formula unambiguously. If we leave out all the parentheses, does a formula like <span class="math inline">\(\forall x P x \to \lnot Q x\)</span> mean <span class="math inline">\((\forall x P x \to (\lnot Q x))\)</span> or <span class="math inline">\(\forall x (P x \to (\lnot Q x))\)</span>? An alternative syntax would be to use logical connectives in a prefix manner, e.g., <span class="math inline">\(\to \forall x P x \lnot Q x\)</span> and <span class="math inline">\(\forall x \to P x \lnot Q x\)</span>, but that is hardly as comprehensible as our chosen syntax.</p>
<p><strong>Remark 7.4. (Abbreviations on notation)</strong> Why don’t we have the existential quantifier <span class="math inline">\(\exists\)</span> and some other connectives such like <span class="math inline">\(\land\)</span>, <span class="math inline">\(\lor\)</span> and <span class="math inline">\(\leftrightarrow\)</span>, in our language? Because any first-order formula that makes use of these symbols can be seen as syntactical abbreviations and should be rewritten using <span class="math inline">\(\forall\)</span>, <span class="math inline">\(\to\)</span> and <span class="math inline">\(\lnot\)</span>, as will be shown. A deeper reason is that <span class="math inline">\(\{ \to, \lnot \}\)</span> is a functionally complete set of Boolean algebraic operators that is sufficient to express all possible truth tables in propositional logic. On the other hand, a formula like <span class="math inline">\(\exists x \varphi\)</span> is just <span class="math inline">\((\lnot \forall x (\lnot \varphi))\)</span>, following from our understanding of what an existential quantification is.</p>
<p><strong>Remark 7.5. (Sentences and truth values)</strong> In propositional logic, we don’t <em>generally</em> know whether a formula evaluates to true until every sentential symbol is assigned a truth value. (Sometimes we can tell the truth value with a little less information than what is required, if we apply a so-called short-circuit evaluation strategy, e.g., if <span class="math inline">\(A_1\)</span> is false then we immediately know <span class="math inline">\((A_1 \to A_2)\)</span> is true, or if <span class="math inline">\(A_2\)</span> is true then <span class="math inline">\((A_1 \to A_2)\)</span> is also true. But it is not the general case, and one should expect to evaluate both <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> before getting the answer.) Similarly, in a first-order logic, every free variable needs to have a definite assignment so as to give rise to the truth value of a formula. This is done by specifying a function <span class="math inline">\(s\)</span> (where <span class="math inline">\(\operatorname{dom} s = V\)</span> is the set of all variables) as the assignment of variables, and when applying <span class="math inline">\(s\)</span> to a formula <span class="math inline">\(\varphi\)</span> we get <span class="math inline">\(\varphi[s]\)</span>, which is a sentence that has a definite meaning (i.e., no variable occurs free). Note that the assignment of variables alone is not sufficient to determine the truth of a sentence – For example, <span class="math inline">\((P x y \to P x f y)\ [s(x \,|\, 0)(y \,|\, 0)]\)</span> is a sentence since no variable occurs free in it, but we can’t decide whether it is true because we don’t know what the predicate <span class="math inline">\(P\)</span> and the function <span class="math inline">\(f\)</span> are. If we say, <span class="math inline">\(P\)</span> is the arithmetic “less-than” relation and <span class="math inline">\(f\)</span> is the successor function <span class="math inline">\(f : x \mapsto x + 1\)</span>, then we can tell that this is a true sentence (in fact <span class="math inline">\(P x y\)</span> is false as <span class="math inline">\(0 &lt; 0\)</span> is false, but <span class="math inline">\(P x f y\)</span> is true as <span class="math inline">\(0 &lt; 1\)</span> is true, so the overall sentence as a conditional is true). We could write <span class="math inline">\(P\)</span> and <span class="math inline">\(f\)</span> as <span class="math inline">\(&lt;\)</span> and <span class="math inline">\(S\)</span>, but the conventional interpretation of these symbols should not be taken for granted as if every symbol comes with an inherited meaning – They don’t, until we give them meanings.</p>
<ul>
<li><a href="https://wiki.soimort.org/math/logic/fol/structures/"><strong>Structures</strong></a>: A <em>structure</em> (or an <em>interpretation</em>) <span class="math inline">\(\mathfrak{A}\)</span> assigns a domain <span class="math inline">\(|\mathfrak{A}|\)</span> to the language <span class="math inline">\(\mathcal{L}\)</span>, and:
<ul>
<li>Every predicate symbol is assigned a relation <span class="math inline">\(P^\mathfrak{A} \subseteq |\mathfrak{A}|^n\)</span>.</li>
<li>Every function symbol is assigned a function <span class="math inline">\(f^\mathfrak{A} : |\mathfrak{A}|^n \to |\mathfrak{A}|\)</span>.</li>
<li>Every constant symbol is assigned a member <span class="math inline">\(c^\mathfrak{A}\)</span> of the domain <span class="math inline">\(\mathfrak{A}\)</span>.</li>
<li>The universal quantifier symbol <span class="math inline">\(\forall\)</span> is assigned the domain <span class="math inline">\(|\mathfrak{A}|\)</span>. (So it makes sense to say: “for all <span class="math inline">\(x\)</span> in <span class="math inline">\(|\mathfrak{A}|\)</span>…”)</li>
</ul></li>
<li><strong>Satisfaction</strong> and <strong>truth</strong>
<ul>
<li>Given a structure <span class="math inline">\(\mathfrak{A}\)</span> and an assignment of variables <span class="math inline">\(s : V \to |\mathfrak{A}|\)</span>, we define an extension function <span class="math inline">\(\bar{s} : T \to |\mathfrak{A}|\)</span> (where <span class="math inline">\(T\)</span> is the set of all terms) that maps any term into the domain <span class="math inline">\(|\mathfrak{A}|\)</span>.</li>
<li>With the term valuation <span class="math inline">\(\bar{s}\)</span>, we define recursively that a structure <span class="math inline">\(\mathfrak{A}\)</span> <em>satisfies</em> a formula <span class="math inline">\(\varphi\)</span> with an assignment <span class="math inline">\(s\)</span> of variables, written as <span class="math display">\[\models_\mathfrak{A} \varphi[s]\]</span> If this is not the case, then <span class="math inline">\(\not\models_\mathfrak{A} \varphi[s]\)</span> and we say that <span class="math inline">\(\mathfrak{A}\)</span> does not satisfy <span class="math inline">\(\varphi\)</span> with <span class="math inline">\(s\)</span>.</li>
<li>For a sentence <span class="math inline">\(\sigma\)</span> (which is just a formula with no free variables), the assignment of variables <span class="math inline">\(s : V \to |\mathfrak{A}|\)</span> does not make a difference whether <span class="math inline">\(\varphi\)</span> is satisfied by <span class="math inline">\(\mathfrak{A}\)</span>. So if <span class="math inline">\(\models_\mathfrak{A} \sigma\)</span>, we say that <span class="math inline">\(\sigma\)</span> is <em>true</em> in <span class="math inline">\(\mathfrak{A}\)</span> or that <span class="math inline">\(\mathfrak{A}\)</span> is a <em>model</em> of <span class="math inline">\(\sigma\)</span>.</li>
<li><strong>Satisfiability</strong> of formulas: A set <span class="math inline">\(\Gamma\)</span> of formulas is said to be <em>satisfiable</em> iff there is a structure <span class="math inline">\(\mathfrak{A}\)</span> and an assignment <span class="math inline">\(s\)</span> of variables such that <span class="math inline">\(\models_\mathfrak{A} \Gamma[s]\)</span>.</li>
</ul></li>
<li><strong>Logical implication</strong> and <strong>validity</strong>
<ul>
<li>In a language <span class="math inline">\(\mathcal{L}\)</span>, we say that a set <span class="math inline">\(\Gamma\)</span> of formulas <em>logically implies</em> a formula <span class="math inline">\(\varphi\)</span>, iff for every structure <span class="math inline">\(\mathfrak{A}\)</span> of <span class="math inline">\(\mathcal{L}\)</span> and every assignment <span class="math inline">\(s : V \to |\mathfrak{A}|\)</span> such that <span class="math inline">\(\models_\mathfrak{A} \gamma [s]\)</span> (for all <span class="math inline">\(\gamma \in \Gamma\)</span>), it also holds that <span class="math inline">\(\models_\mathfrak{A} \varphi [s]\)</span> (note that <span class="math inline">\(\varphi\)</span> is not required to be a sentence): <span class="math display">\[\Gamma \models \varphi\]</span> This is the analogue of tautological implication in propositional logic: <span class="math inline">\(A \Rightarrow B\)</span>, iff every truth assignment that satisfies <span class="math inline">\(A\)</span> also satisfies <span class="math inline">\(B\)</span>.</li>
<li>If the empty set logically implies a formula, i.e., <span class="math inline">\(\emptyset \models \varphi\)</span>, we write this fact simply as <span class="math inline">\(\models \varphi\)</span> and say that <span class="math inline">\(\varphi\)</span> is <em>valid</em>. A formula is valid iff given any assignment of variables, it is true in every structure; this is the analogue of tautologies in propositional logic: something that is considered “always true”.</li>
</ul></li>
</ul>
<p><strong>Remark 7.6. (Dichotomy of semantic truthness and the liar’s paradox)</strong> It should be made clear from the definition that given a structure and an assignment, either <span class="math inline">\(\models_\mathfrak{A} \varphi[s]\)</span> (exclusive) or <span class="math inline">\(\not\models_\mathfrak{A} \varphi[s]\)</span>, but not both! It follows from our intuition that a statement is either semantically true or false; and there is no third possibility.</p>
<p>A problem arises with self-referential terms, woefully: Assume that we have a first-order language <span class="math inline">\(\mathcal{L}\)</span> with a 1-place predicate symbol <span class="math inline">\(P\)</span>, and the structure <span class="math inline">\(\mathfrak{A}\)</span> assigns it the domain <span class="math inline">\(|\mathfrak{A}| = \text{Formula}(\mathcal{L})\)</span>, <span class="math inline">\(P\)</span> is interpreted as <span class="math inline">\(P^\mathfrak{A} = \{ \langle \sigma \rangle \,:\, \models \sigma \}\)</span>, that is, <span class="math inline">\(\sigma \in P^\mathfrak{A}\)</span> iff <span class="math inline">\(\models \sigma\)</span>. Let the sentence <span class="math inline">\(\tau\)</span> be <span class="math inline">\((\lnot P x)\)</span> and the assignment <span class="math inline">\(s : V \to |\mathfrak{A}|\)</span> maps the variable <span class="math inline">\(x\)</span> to the sentence <span class="math inline">\(\tau\)</span>, then is <span class="math inline">\(\tau[s]\)</span> true or false in <span class="math inline">\(\mathfrak{A}\)</span>? If we take <span class="math inline">\(\tau[s]\)</span> as true, that is, <span class="math inline">\((\lnot P \tau)\)</span> is true, then <span class="math inline">\(P \tau\)</span> must be false, so <span class="math inline">\(\tau \not\in P^\mathfrak{A}\)</span> thus <span class="math inline">\(\not\models \tau\)</span>. If we take <span class="math inline">\(\tau[s]\)</span> as false, that is, <span class="math inline">\((\lnot P \tau)\)</span> is false, then <span class="math inline">\(P \tau\)</span> must be true, so <span class="math inline">\(\tau \in P^\mathfrak{A}\)</span> thus <span class="math inline">\(\models \tau\)</span>. This is known as the classical <em>liar’s paradox</em>. One possible way to resolve this (given by Alfred Tarski) is by disabling impredicativity in our structures; more precisely, one can define a semantic hierarchy of structures that allows us to predicate truth only of a formula at a lower level, but never at the same or a higher level. This matter is far beyond the scope of this summary, but the important lesson to learn here is that it is generally a bad idea to allow something both true <em>and</em> false in our semantics; it would put our enduring effort to cumulate all “mathematical truths” into void.</p>
<p><strong>Remark 7.7. (Decidability of truth/validity)</strong> In propositional logic, it is easy to see that given a truth assignment of sentential symbols, every formula can be decided for its truth or falsehood. Moreover, even without any truth assignment, one can enumerate a truth table to find out whether a given formula is a tautology. Truth and validity are decidable in propositional logic. However, this is often not the case in first-order logic: In order to decide whether a sentence is true, one needs to find the truth values of all prime formulas (i.e., formulas like <span class="math inline">\(P t_1 \cdots t_n\)</span> and <span class="math inline">\(\forall v_i \psi\)</span>) first, but the domain <span class="math inline">\(|\mathfrak{A}|\)</span> may be an (uncountably) infinite set, thus makes it impossible to mechanically check the universal quantification for all members; moreover, the functions used in building terms may not be Turing-computable at all. To decide the validity of a sentence, we have to check its truth in all structures of the language (whose set may also be uncountably large), and that is an even more impossible task.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>If semantic truth/validity is generally undecidable, how do we say that some formula is true in a predefined structure? Well, we can’t, in most general cases, since an infinite argument of truth is a useless argument (you can’t present it to someone / some Turing machine, as no physical device is capable of handling such an infinite object). Fortunately, there is a feasible way to say something is true, without appealing to any specific structures (that may give rise to unwanted undecidability), and that is called a formal deduction (also called a proof, expectedly).</p>
<ul>
<li><a href="https://wiki.soimort.org/math/logic/fol/deductions/"><strong>Formal deduction</strong></a>: Given a set <span class="math inline">\(\Lambda\)</span> of formulas (<em>axioms</em>), a set of <em>rules of inference</em>, we say that a set <span class="math inline">\(\Gamma\)</span> of formulas (<em>hypotheses</em>) <em>proves</em> another formula <span class="math inline">\(\varphi\)</span>, or <span class="math inline">\(\varphi\)</span> is a <em>theorem</em> of <span class="math inline">\(\Gamma\)</span>, iff there is a finite sequence (called a <em>deduction</em> of <span class="math inline">\(\varphi\)</span> from <span class="math inline">\(\Gamma\)</span>) <span class="math inline">\(\langle \alpha_0, \dots, \alpha_n \rangle\)</span> such that
<ol>
<li><span class="math inline">\(\alpha_n\)</span> is just <span class="math inline">\(\varphi\)</span>.</li>
<li>For each <span class="math inline">\(0 \leq k \leq n\)</span>, either
<ul>
<li><span class="math inline">\(\alpha_k \in \Gamma \cup \Lambda\)</span>; or</li>
<li><span class="math inline">\(\alpha_k\)</span> is obtained by a rule of inference from a subset of previous formulas <span class="math inline">\(A \subseteq \bigcup_{0 \leq i &lt; k} \alpha_i\)</span>. <span class="math display">\[\Gamma \vdash \varphi\]</span></li>
</ul></li>
</ol>
<ul>
<li><strong>Formal systems</strong> and <strong>proof calculi</strong>: Different deductive systems made different choices on the set of axioms and rules of inferences. A <em>natural deduction</em> system may consist of no axiom but many rules of inference; on the contrary, a Hilbert-style system (named obviously after David Hilbert) uses many axioms but only two rules of inference. A <em>proof calculus</em> is the approach to formal deduction in a specified system, and as it is called a “calculus”, any derivation in it must contain only a finite number of steps so as to be calculable (by a person or by a machine).</li>
<li>We will use a Hilbert-style deductive system here:
<ul>
<li><strong>Rules of inference</strong>
<ol>
<li><em>Modus ponens</em> <span class="math display">\[\frac{\Gamma \vdash \psi \quad \Gamma \vdash (\psi \to \varphi)}{\Gamma \vdash \varphi}\]</span></li>
<li><em>Generalization</em> <span class="math display">\[\frac{\vdash \theta}{\vdash \forall x_1 \cdots \forall x_n \theta}\]</span> (where <span class="math inline">\(\theta \in \Lambda\)</span>.)</li>
</ol></li>
<li><strong>Logical axioms</strong>: In a deductive system, axioms are better called <em>logical axioms</em>, to stress the fact that they are logically valid formulas in every structure, i.e., that their validity is not open to interpretation.
<ol>
<li>(Tautology) <span class="math inline">\(\alpha\)</span>, where <span class="math inline">\(\models_t \alpha\)</span>. (take sentential symbols to be prime formulas in first-order logic)</li>
<li>(Substitution) <span class="math inline">\(\forall x \alpha \to \alpha^x_t\)</span>, where <span class="math inline">\(t\)</span> is substitutable for <span class="math inline">\(x\)</span> in <span class="math inline">\(\alpha\)</span>.</li>
<li><span class="math inline">\(\forall x (\alpha \to \beta) \to (\forall x \alpha \to \forall x \beta)\)</span>.</li>
<li><span class="math inline">\(\alpha \to \forall x \alpha\)</span>, where <span class="math inline">\(x\)</span> does not occur free in <span class="math inline">\(\alpha\)</span>.</li>
<li><span class="math inline">\(x = x\)</span>.</li>
<li><span class="math inline">\(x = y \to (\alpha \to \alpha&#39;)\)</span>, where <span class="math inline">\(\alpha\)</span> is atomic and <span class="math inline">\(\alpha&#39;\)</span> is obtained from <span class="math inline">\(\alpha\)</span> by replacing <span class="math inline">\(x\)</span> in zero or more places by <span class="math inline">\(y\)</span>.</li>
</ol></li>
</ul></li>
</ul></li>
</ul>
<p><strong>Remark 7.8. (Validity of logical axioms)</strong> It should be intuitively clear that all logical axioms are convincing, and that their validity can be argued without appealing to any specific model. In particular, for an axiom <span class="math inline">\(\theta \in \Lambda\)</span>, there is <span class="math inline">\(\vdash \theta\)</span>; we must be able to argue (in our meta-language) that <span class="math inline">\(\models \theta\)</span>, so that we can be convinced that our deductive system is a <em>sound</em> one. Remember that for any formula <span class="math inline">\(\varphi\)</span>, either <span class="math inline">\(\models \varphi\)</span> or <span class="math inline">\(\not\models \varphi\)</span> (which is just <span class="math inline">\(\models (\lnot \varphi)\)</span>). If a proof of <span class="math inline">\(\theta\)</span> (not as a formal deduction, but as an argument in our meta-language) does not even imply <span class="math inline">\(\models \theta\)</span>, that would be very frustrating.</p>
<p><strong>Remark 7.9. (Tautological implication, logical implication and deduction)</strong> If <span class="math inline">\(\Gamma \models_t \varphi\)</span> (i.e., <span class="math inline">\(\varphi\)</span> is tautologically implied by <span class="math inline">\(\Gamma\)</span> in propositional logic), we can argue that <span class="math inline">\(\Gamma \models \varphi\)</span> when replacing sentential symbols by prime formulas in first-order logic. In the special case that <span class="math inline">\(\Gamma = \emptyset\)</span>, we are proving the validity of Axiom Group 1: <span class="math inline">\(\models_t \alpha \implies \models \alpha\)</span> (every tautology is valid). The converse does not hold though, since we have <span class="math inline">\(\models (\alpha \to \forall x \alpha)\)</span> (by Axiom Group 4), but <span class="math inline">\(\not\models_t (\alpha \to \forall x \alpha)\)</span> as <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\forall x \alpha\)</span> are two different sentential symbols (surely <span class="math inline">\((A_1 \to A_2)\)</span> is not a tautology in propositional logic!).</p>
<p>It is worth noticing that even though <span class="math inline">\(\Gamma \models \varphi \not\implies \Gamma \models_t \varphi\)</span>, we do have <span class="math inline">\(\Gamma \models \varphi \iff \Gamma \cup \Lambda \models_t \varphi\)</span>. Intuitively, the set <span class="math inline">\(\Lambda\)</span> of logical axioms gives us a chance to establish truths about quantifiers and equalities (other than treating these prime formulas as sentential symbols that are too unrefined for our first-order logic). I haven’t done a proof of this, but I believe it should be <em>non-trivial</em> on both directions. Combining with Theorem 24B in Enderton p. 115, we get the nice result concluding that <span class="math display">\[\Gamma \vdash \varphi \iff \Gamma \cup \Lambda \models_t \varphi \iff \Gamma \models \varphi\]</span> which entails both the soundness and the completeness theorems. It is basically saying that these three things are equivalent:</p>
<ol type="1">
<li><span class="math inline">\(\Gamma\)</span> proves <span class="math inline">\(\varphi\)</span>. (There is a formal deduction that derives <span class="math inline">\(\varphi\)</span> from <span class="math inline">\(\Gamma\)</span> and axioms <span class="math inline">\(\Lambda\)</span> in our deductive system; it is finite and purely syntactical, in the sense that it does not depend on any structure or assignment of variables.)</li>
<li><span class="math inline">\(\Gamma\)</span> logically implies <span class="math inline">\(\varphi\)</span>. (For every structure and every assignment of variables, <span class="math inline">\(\varphi\)</span> holds true given <span class="math inline">\(\Gamma\)</span>. Of course, this is a semantical notion in the sense that it does involve structures and assignments of variables, which are infinite in numbers so it would be impossible for one to check this mechanically.)</li>
<li><span class="math inline">\(\Gamma \cup \Lambda\)</span> tautologically implies <span class="math inline">\(\varphi\)</span>. (We can reduce a first-order logic to propositional logic by adding logical axioms to the set of hypotheses, preserving all truths. For each prime formula, this is still a semantical notion for its truth value depends on structures / assignments of variables.)</li>
</ol>
<ul>
<li><a href="https://wiki.soimort.org/math/logic/fol/soundness/"><strong>Soundness</strong></a>
<ol type="a">
<li><span class="math inline">\(\Gamma \vdash \varphi \implies \Gamma \models \varphi\)</span>.</li>
</ol>
<ul>
<li>Proof idea:
<ol>
<li>For <span class="math inline">\(\varphi \in \Lambda\)</span>, show that every logical axiom is valid (Lemma 25A in Enderton p. 132ff.), that is, <span class="math inline">\(\models \varphi\)</span>. Then trivially <span class="math inline">\(\Gamma \models \varphi\)</span>;</li>
<li>For <span class="math inline">\(\varphi \in \Gamma\)</span>, we have trivially <span class="math inline">\(\Gamma \models \varphi\)</span>;</li>
<li><span class="math inline">\(\varphi\)</span> is obtained by generalization on variable <span class="math inline">\(x\)</span> from a valid formula <span class="math inline">\(\theta\)</span>. Since <span class="math inline">\(\models \theta\)</span> (if <span class="math inline">\(\theta\)</span> is an axiom, then this is already shown in Step 1; if <span class="math inline">\(\theta\)</span> is another generalization, then this can be shown by IH), for every structure <span class="math inline">\(\mathfrak{A}\)</span> and <span class="math inline">\(a \in |\mathfrak{A}|\)</span>, <span class="math inline">\(\models_\mathfrak{A} \theta[s(x|a)]\)</span>, then by definition we have <span class="math inline">\(\models_\mathfrak{A} \forall x \theta[s]\)</span>. Therefore <span class="math inline">\(\models \forall x \theta\)</span>;</li>
<li><span class="math inline">\(\varphi\)</span> is obtained by modus ponens from <span class="math inline">\(\psi\)</span> and <span class="math inline">\((\psi \to \varphi)\)</span>. By IH we have <span class="math inline">\(\Gamma \models \psi\)</span> and <span class="math inline">\(\Gamma \models (\psi \to \varphi)\)</span>. Show that <span class="math inline">\(\Gamma \models \varphi\)</span> using Exercise 1 in Enderton p. 99. (NB. the wording in the last line of Enderton p. 131, i.e., “follows at once”, seems too sloppy to me: we have not proved modus ponens semantically yet.)</li>
</ol></li>
<li><strong>Consistency</strong> of formulas: A set <span class="math inline">\(\Gamma\)</span> of formulas is said to be <em>consistent</em> iff for no formula <span class="math inline">\(\varphi\)</span> it is the case that both <span class="math inline">\(\Gamma \vdash \varphi\)</span> and <span class="math inline">\(\Gamma \vdash (\lnot\varphi)\)</span>.
<ul>
<li>By the soundness theorem, an inconsistent set <span class="math inline">\(\Gamma\)</span> of formulas gives rise to both <span class="math inline">\(\Gamma \models \varphi\)</span> and <span class="math inline">\(\Gamma \models (\lnot\varphi)\)</span>. As discussed before, it would throw our trust on mathematical truths into fire – we will have proved that some statement is both true and false!</li>
</ul></li>
</ul>
<ol start="2" type="a">
<li>If <span class="math inline">\(\Gamma\)</span> is satisfiable, then <span class="math inline">\(\Gamma\)</span> is consistent.<br />
(a. and b. are equivalent representations of the soundness theorem.)</li>
</ol></li>
<li><a href="https://wiki.soimort.org/math/logic/fol/completeness/"><strong>Completeness</strong></a>
<ol type="a">
<li><span class="math inline">\(\Gamma \models \varphi \implies \Gamma \vdash \varphi\)</span>.</li>
<li>If <span class="math inline">\(\Gamma\)</span> is consistent, then <span class="math inline">\(\Gamma\)</span> is satisfiable.<br />
(a. and b. are equivalent representations of the completeness theorem.)</li>
</ol>
<ul>
<li>Proof idea: We will prove first a weaker form of b., i.e., the completeness for a countable language <span class="math inline">\(\mathcal{L}\)</span>. Let <span class="math inline">\(\Gamma\)</span> be a consistent set of formulas. We show that it is satisfiable.
<ol>
<li>Extend the language <span class="math inline">\(\mathcal{L}\)</span> with a countable set <span class="math inline">\(\bar{C}\)</span> of new constant symbols and get a new language <span class="math inline">\(\mathcal{L}_\bar{C}\)</span>;</li>
<li>Given the set <span class="math inline">\(\Gamma\)</span> of <span class="math inline">\(\mathcal{L}\)</span>-formulas, show that there is a set <span class="math inline">\(\bar\Gamma\)</span> of <span class="math inline">\(\mathcal{L}_\bar{C}\)</span>-formulas that is consistent, complete, deductively closed and Henkinized, i.e., for every formula <span class="math inline">\(\exists x \varphi \in \Gamma\)</span> there is a “witness” constant <span class="math inline">\(\bar{c} \in \bar{C}\)</span> such that <span class="math inline">\(\varphi^x_\bar{c} \in \bar{\Gamma}\)</span>;</li>
<li>Build a structure <span class="math inline">\(\mathfrak{A}_0\)</span> from <span class="math inline">\(\bar\Gamma\)</span> where <span class="math inline">\(|\mathfrak{A}_0|\)</span> is the set of terms of <span class="math inline">\(\mathcal{L}_\bar{C}\)</span>. The assignment <span class="math inline">\(s\)</span> maps every variable to itself;</li>
<li>Define an equivalence relation <span class="math inline">\(E\)</span> on <span class="math inline">\(|\mathfrak{A}_0|\)</span>: <span class="math inline">\(t E t&#39; \iff t = t&#39; \in \bar\Gamma\)</span>. Show by induction that for any <span class="math inline">\(\mathcal{L}_\bar{C}\)</span>-formula <span class="math inline">\(\varphi\)</span>, <span class="math inline">\(\models_{\mathfrak{A}_0} \varphi^* [s] \iff \varphi \in \bar\Gamma\)</span> (where <span class="math inline">\(\varphi^*\)</span> is <span class="math inline">\(\varphi\)</span> with <span class="math inline">\(=\)</span> replaced by <span class="math inline">\(E\)</span> everywhere);</li>
<li>Show by the homomorphism theorem that <span class="math inline">\(\models_\mathfrak{A} \varphi[s] \iff \varphi \in \bar\Gamma\)</span> (where <span class="math inline">\(\mathfrak{A} = \mathfrak{A}_0 / E\)</span>);</li>
<li>Restrict the structure <span class="math inline">\(\mathfrak{A}\)</span> (a model of <span class="math inline">\(\mathcal{L}_\bar{C}\)</span>) to <span class="math inline">\(\mathcal{L}\)</span> by dropping all new constants <span class="math inline">\(\bar{C}\)</span>. Then <span class="math inline">\(\Gamma\)</span> is satisfiable with <span class="math inline">\(\mathfrak{A}\)</span> and <span class="math inline">\(s\)</span> in <span class="math inline">\(\mathcal{L}\)</span>.</li>
</ol></li>
</ul></li>
<li><a href="https://wiki.soimort.org/math/logic/fol/compactness/"><strong>Compactness</strong></a>
<ol type="a">
<li><span class="math inline">\(\Gamma \models \varphi \implies\)</span> There is a finite <span class="math inline">\(\Gamma_0 \subseteq \Gamma\)</span> such that <span class="math inline">\(\Gamma_0 \models \varphi\)</span>.</li>
<li>If every finite subset <span class="math inline">\(\Gamma_0\)</span> of <span class="math inline">\(\Gamma\)</span> is satisfiable, then <span class="math inline">\(\Gamma\)</span> is satisfiable.<br />
(a. and b. are equivalent representations of the compactness theorem.)</li>
</ol>
<ul>
<li>Proof idea: A simple corollary of soundness and completeness theorems.</li>
</ul></li>
</ul>
<p><strong>Remark 7.10. (Soundness and completeness)</strong> The soundness and the completeness theorems together evidence the equivalence of consistency and satisfiability of a set of formulas, or that of validity and provability of a formula. The completeness theorem is by no means an obvious result; the first proof was given by Kurt Gödel in 1930<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>, but the proof that we use today is given by Leon Henkin in 1949 <span class="citation" data-cites="henkin1949completeness">[1]</span>, which easily generalizes to uncountable languages.</p>
<p><strong>Remark 7.11. (Completeness and incompleteness)</strong> Note that the completeness theorem should not be confused with Gödel’s <em>incompleteness theorems</em>. Specifically, the completeness theorem claims that (unconditionally) every formula that is logically implied by <span class="math inline">\(\Gamma\)</span> is also deducible from <span class="math inline">\(\Gamma\)</span> (i.e., <span class="math inline">\(\Gamma \models \varphi \implies \Gamma \vdash \varphi\)</span>), while the first incompleteness theorem claims that (under some conditions) some consistent deductive systems are incomplete (i.e., there is some formula <span class="math inline">\(\varphi\)</span> such that neither <span class="math inline">\(\Gamma \vdash \varphi\)</span> nor <span class="math inline">\(\Gamma \vdash (\lnot\varphi)\)</span>). As is clearly seen, the incompleteness theorem is purely syntactical and matters for provability (or decidability, one might say). The aforementioned liar’s paradox, where our semantics raises a contradiction that neither <span class="math inline">\(\Gamma \models \varphi\)</span> nor <span class="math inline">\(\Gamma \models (\lnot\varphi)\)</span> reasonably holds, may be seen as a semantical analogue of the first incompleteness theorem.</p>
<section id="equality-is-logical" class="level2">
<h2>Equality is logical</h2>
<p>The equality symbol <span class="math inline">\(=\)</span> is a logical symbol, in the sense that the equivalence relation it represents is <em>not</em> open to interpretation and always means what it’s intended to mean (i.e., “the LHS is <em>equal</em> to the RHS”). But if so, how do we say <span class="math display">\[1+1=2\]</span> is a true sentence then? Can’t we just interpret the equality symbol as something else in a structure <span class="math inline">\(\mathfrak{A}\)</span> such that <span class="math inline">\(\models_\mathfrak{A} (\lnot 1+1=2)\)</span>?</p>
<p>One reason is that in many first-order systems, functions (operations) are defined as axioms using equalities; we need a general way to say “something is defined as…” or just “something is…” There would be no better way of saying this rather than representing it as an equality, so we won’t have the hassle of interpreting a made-up relation in every model. Consider the language of elementary number theory, in the intended model <span class="math inline">\(\mathfrak{N} = (\mathbb{N}; \mathbf{0}, \mathbf{S}, +, \cdot)\)</span> of Peano arithmetic, the addition function is defined as a set of domain-specific axioms: <span class="math display">\[\begin{align*}
a + \mathbf{0} &amp;= a &amp;\qquad(1) \\
a + \mathbf{S} b &amp;= \mathbf{S} (a + b) &amp;\qquad(2)
\end{align*}\]</span> By Axiom Group 6 we have <span class="math inline">\(\mathbf{S0} + \mathbf{0} = \mathbf{S0} \to (\mathbf{S0} + \mathbf{S0} = \mathbf{S}(\mathbf{S0} + \mathbf{0}) \to \mathbf{S0} + \mathbf{S0} = \mathbf{S}\mathbf{S0})\)</span>. By (1) <span class="math inline">\(\mathbf{S0} + \mathbf{0} = \mathbf{S0}\)</span>. By (2) <span class="math inline">\(\mathbf{S0} + \mathbf{S0} = \mathbf{S}(\mathbf{S0} + \mathbf{0})\)</span>. Applying modus ponens twice, we get <span class="math inline">\(\mathbf{S0} + \mathbf{S0} = \mathbf{S}\mathbf{S0}\)</span>, which is the result we want (sloppily written as <span class="math inline">\(1+1=2\)</span>).</p>
<p>The equality is a little special, since it is the most common relation with <em>reflexivity</em>, as justified by Axiom Group 5, i.e., <span class="math inline">\(x = x\)</span> for any variable <span class="math inline">\(x\)</span>. We could exclude these from our logical axioms, but in many cases we would still need a reflexive relation symbol to denote equivalence (justified by some domain-specific axioms) otherwise. Technically, it would be convenient to just treat it as a logical symbol (together with the useful Axiom Groups 5 and 6). Note that though our logical axioms did not say anything about other properties like <em>symmetry</em>, <em>antisymmetry</em> and <em>transitivity</em>, they follow easily from Axiom Groups 5 and 6, in our deductive system:</p>
<p><strong>Lemma 7.12. (Symmetry)</strong> If <span class="math inline">\(x = y\)</span>, then <span class="math inline">\(y = x\)</span>.</p>
<p><em>Proof.</em> Given <span class="math inline">\(x = y\)</span>. By Axiom Group 5 we have <span class="math inline">\(x = x\)</span>. By Axiom Group 6 we have <span class="math inline">\(x = y \to (x = x \to y = x)\)</span>. Applying modus ponens twice, we get <span class="math inline">\(y = x\)</span>. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p><strong>Lemma 7.13. (Transitivity)</strong> If <span class="math inline">\(x = y\)</span> and <span class="math inline">\(y = z\)</span>, then <span class="math inline">\(x = z\)</span>.</p>
<p><em>Proof.</em> Given <span class="math inline">\(x = y\)</span>, by symmetry it holds <span class="math inline">\(y = x\)</span>. Also <span class="math inline">\(y = z\)</span>. By Axiom Group 6 we have <span class="math inline">\(y = x \to (y = z \to x = z)\)</span>. Applying modus ponens twice, we get <span class="math inline">\(x = z\)</span>. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p><strong>Lemma 7.14. (Antisymmetry)</strong> If <span class="math inline">\(x = y\)</span> and <span class="math inline">\(y = x\)</span>, then <span class="math inline">\(x = y\)</span>.</p>
<p><em>Proof.</em> Trivial.</p>
<p>Note that if any partial order is <a href="https://wiki.soimort.org/math/logic/fol/definability/">definable</a> on a structure, the equality symbol may not be indispensable in our language, e.g., consider the language of set theory, where the equivalence of two sets <span class="math inline">\(x = y\)</span> may be defined as <span class="math display">\[(\forall v (v \in x \to v \in y) \land \forall v (v \in y \to v \in x))\]</span></p>
</section>
<section id="the-limitation-of-first-order-logic" class="level2">
<h2>The limitation of first-order logic</h2>
<p>Consider again the language of elementary number theory, in the intended model <span class="math inline">\(\mathfrak{N} = (\mathbb{N}; \mathbf{0}, \mathbf{S}, +, \cdot)\)</span> of Peano arithmetic, we have the set of all true sentences <span class="math inline">\(\text{Th}\mathfrak{N}\)</span>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Now we add a new constant symbol <span class="math inline">\(c&#39;\)</span> to our language, and extend our theory with a countable set of sentences <span class="math inline">\(\text{Th}\mathfrak{N} \cup \{ \underbrace{\mathbf{S} \cdots \mathbf{S}}_{n\ \text{times}} \mathbf{0} &lt; c&#39; \,:\, n \in \mathbb{N} \}\)</span> (Here we define <span class="math inline">\(x &lt; y\)</span> as <span class="math inline">\(\lnot\forall z ((\lnot z = \mathbf{0}) \to (\lnot x + z = y))\)</span>). Is there still a model for this extended theory?</p>
<p>For any given <span class="math inline">\(n \in \mathbb{N}\)</span>, the sentence <span class="math inline">\(\underbrace{\mathbf{S} \cdots \mathbf{S}}_{n\ \text{times}} \mathbf{0} &lt; c&#39;\)</span> is clearly satisfiable (by simply taking <span class="math inline">\(c&#39; = \mathbf{S} n\)</span>). Then it is easily shown that every finite subset <span class="math inline">\(\Gamma_0 \subseteq \text{Th}\mathfrak{N} \cup \{ \underbrace{\mathbf{S} \cdots \mathbf{S}}_{n\ \text{times}} \mathbf{0} &lt; c&#39; \,:\, n \in \mathbb{N} \}\)</span> is satisfiable. By the compactness theorem (b.), <span class="math inline">\(\text{Th}\mathfrak{N} \cup \{ \underbrace{\mathbf{S} \cdots \mathbf{S}}_{n\ \text{times}} \mathbf{0} &lt; c&#39; \,:\, n \in \mathbb{N} \}\)</span> is also satisfiable. This means that we can construct a <em>non-standard model of arithmetic</em> <span class="math inline">\(\mathfrak{N}&#39;\)</span> with an additional bizarre element (specifically <span class="math inline">\(c&#39;_0\)</span>) that turns out to be greater than any other number (thus the model of this theory is not isomorphic to our standard model <span class="math inline">\(\mathfrak{N}\)</span>).</p>
<p>Recall that in a Peano arithmetic modeled by <span class="math inline">\(\mathfrak{N}&#39;\)</span>, numbers are closed under the successor function <span class="math inline">\(\mathbf{S}\)</span>. More precisely, if <span class="math inline">\(k \in |\mathfrak{N}&#39;|\)</span> , then <span class="math inline">\(\mathbf{S}k \in |\mathfrak{N}&#39;|\)</span> and <span class="math inline">\(\mathbf{S}k \neq \mathbf{0}\)</span>. This implies that not only <span class="math inline">\(c&#39;_0 \in |\mathfrak{N}&#39;|\)</span>, but also <span class="math inline">\(\mathbf{S} c&#39;_0, \mathbf{S}\mathbf{S} c&#39;_0, \mathbf{S}\mathbf{S}\mathbf{S} c&#39;_0, \dots\)</span> are all non-standard numbers in <span class="math inline">\(|\mathfrak{N}&#39;|\)</span>. As none of these numbers is equal to <span class="math inline">\(\mathbf{0}\)</span> (by one of Peano axioms), they form an infinite “chain” separately besides our familiar standard ones. One can write down all the (standard and non-standard) numbers sloppily as the following sequence: <span class="math display">\[\langle 0, 1, 2, \dots, \quad c&#39;_0, c&#39;_1, c&#39;_2, \dots \rangle\]</span> where <span class="math inline">\(0\)</span> is just <span class="math inline">\(\mathbf{0}\)</span>, <span class="math inline">\(1\)</span> is <span class="math inline">\(\mathbf{S0}\)</span>, <span class="math inline">\(2\)</span> is <span class="math inline">\(\mathbf{SS0}\)</span>, <span class="math inline">\(c&#39;_1\)</span> is <span class="math inline">\(\mathbf{S} c&#39;_0\)</span>, <span class="math inline">\(c&#39;_2\)</span> is <span class="math inline">\(\mathbf{SS} c&#39;_0\)</span>, etc.</p>
<p>Clearly, every number but <span class="math inline">\(0\)</span> and <span class="math inline">\(c&#39;_0\)</span> in the sequence has a unique predecessor. There is certainly no such a predecessor <span class="math inline">\(j\)</span> of <span class="math inline">\(0\)</span>, because otherwise we would have <span class="math inline">\(\mathbf{S}j = \mathbf{0}\)</span>, contradicting our axioms. But can we have a predecessor of <span class="math inline">\(c&#39;_0\)</span>? There is no axiom preventing us from constructing that thing! So here we go, enlarge our collection of numbers to: <span class="math display">\[\langle 0, 1, 2, \dots, \quad \dots, c&#39;_{-2}, c&#39;_{-1}, c&#39;_0, c&#39;_1, c&#39;_2, \dots \rangle\]</span> where for each <span class="math inline">\(c&#39;_{i}\)</span>, <span class="math inline">\(c&#39;_{i+1} = \mathbf{S} c&#39;_{i}\)</span>. Again, we know that every such non-standard numbers <span class="math inline">\(c&#39;_i\)</span> is greater than any standard number <span class="math inline">\(n\)</span> (otherwise we can find a standard number <span class="math inline">\(n-i\)</span> such that <span class="math inline">\((\lnot n-i &lt; c&#39;_0)\)</span>, contradicting our initial construction of <span class="math inline">\(c&#39;_0\)</span> by compactness. So the non-standard part is still a separate chain, thus as written above.</p>
<p>We can go even further. Let <span class="math inline">\(|\mathfrak{N}&#39;|\)</span> be this set of standard and non-standard numbers, and <span class="math inline">\(\mathfrak{N}&#39; = (|\mathfrak{N}&#39;|; \mathbf{0}, \mathbf{S}, +, \cdot)\)</span> is still the intended model of Peano arithmetic on <span class="math inline">\(|\mathfrak{N}&#39;|\)</span>. Consider adding yet another constant symbol <span class="math inline">\(c&#39;&#39;\)</span>. Is <span class="math inline">\(\text{Th}\mathfrak{N}&#39; \cup \{ \underbrace{\mathbf{S} \cdots \mathbf{S}}_{n&#39;\ \text{times}} \mathbf{0} &lt; c&#39;&#39; \,:\, n&#39; \in |\mathfrak{N}&#39;| \}\)</span> satisfiable? By the same reasoning as before, we get a model <span class="math inline">\(\mathfrak{N}&#39;&#39;\)</span>, with its domain of numbers <span class="math display">\[\langle 0, 1, 2, \dots, \quad \dots, c&#39;_{-2}, c&#39;_{-1}, c&#39;_0, c&#39;_1, c&#39;_2, \dots, \quad \dots, c&#39;&#39;_{-2}, c&#39;&#39;_{-1}, c&#39;&#39;_0, c&#39;&#39;_1, c&#39;&#39;_2, \dots \rangle\]</span></p>
<p>Counter-intuitively, this is not the kind of “arithmetic” we are used to. What we are trying to do is to formulate the arithmetic for <em>standard</em> natural numbers that we use everyday (i.e., <span class="math inline">\(0, 1, 2, \dots\)</span>) in first-order logic, but these non-standard numbers come out of nowhere and there is an infinite “stage” of them, such that each number in a latter stage is greater than every number in a former stage (How is that even possible?). And what is worse, in a subset of the non-standard model <span class="math inline">\(\mathfrak{N}&#39;\)</span>: <span class="math display">\[\{ \dots, c&#39;_{-2}, c&#39;_{-1}, c&#39;_0, c&#39;_1, c&#39;_2, \dots \}\]</span> There is no minimal element with respect to the intended ordering relation <span class="math inline">\(&lt;\)</span>, thus it is not <em>well-ordered</em> by <span class="math inline">\(&lt;\)</span>, so our good old mathematical induction will no longer work with non-standard numbers.</p>
<p>Well, the lucky part is that we can at least have the induction axiom as a first-order sentence: <span class="math display">\[\varphi(\mathbf{0}, y_1, \dots, y_k) \land \forall x (\varphi (x, y_1, \dots, y_k) \to \varphi (\mathbf{S}(x), y_1, \dots, y_k))
\to \forall x \varphi(x, y_1, \dots, y_k))\]</span> Since the standard model <span class="math inline">\(\mathfrak{N}\)</span> and the non-standard model <span class="math inline">\(\mathfrak{N}&#39;\)</span> are <em>elementarily equivalent</em> (i.e., they satisfy the same sentences in a language excluding non-standard numbers), we still enjoy the nice induction principle for all of standard natural numbers. But for the non-standard part, we’re out of luck.</p>
<p>Ideally, we would like to have a bunch of axioms that perfectly defines <em>the model</em> of arithmetic, where no non-standard part is allowed to exist, i.e., the set of numbers is well-ordered by a definable ordering relation <span class="math inline">\(&lt;\)</span> so that we can apply the induction principle on all of them.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> Unfortunately, this is infeasible in first-order logic (without formulating the notion of sets). We will demonstrate two potential resolutions.</p>
<p>The intuition here is that in order to rule out any non-standard chains of numbers, we must find a 1-place predicate <span class="math inline">\(P \subseteq |\mathfrak{N}|\)</span> such that for every standard number <span class="math inline">\(n\)</span> we have <span class="math inline">\(P n\)</span>, distinguishing it from <span class="math inline">\((\lnot P n&#39;)\)</span> where <span class="math inline">\(n&#39;\)</span> is non-standard. Certainly <span class="math inline">\(\mathbf{0}\)</span> is a standard number; consequently every standard number <span class="math inline">\(x\)</span> is followed by <span class="math inline">\(\mathbf{S}x\)</span>, which is also a standard one. <span class="math display">\[P \mathbf{0} \land \forall x (P x \to P \mathbf{S} x)\]</span> Once we have this <span class="math inline">\(P\)</span> in mind, we say that every number <span class="math inline">\(n \in P\)</span>, so it is also standard. There would be no other numbers in our model, thus define our set of all numbers: <span class="math display">\[\forall n P n\]</span> Notice that <span class="math inline">\(P\)</span> is not in our language; it is something we have yet to construct for our standard model. How to deal with this issue?</p>
<ol type="1">
<li>The first approach is by allowing quantification over relations. So we can say “for all such relations <span class="math inline">\(P\)</span>, it holds that <span class="math inline">\(\forall n P n\)</span>”. Formally: <span class="math display">\[\forall P (P \mathbf{0} \land \forall x (P x \to P \mathbf{S} x)) \to \forall n P n\]</span> Of course, in our previous definition of first-order languages, for <span class="math inline">\(\forall v_i \psi\)</span> to be a well-formed formula, <span class="math inline">\(v_i\)</span> is a variable such that <span class="math inline">\(v_i \in |\mathfrak{N}|\)</span> given a model <span class="math inline">\(\mathfrak{N}\)</span>; here we have <span class="math inline">\(P \subseteq |\mathfrak{N}|\)</span> hence <span class="math inline">\(P \in \mathcal{P}(|\mathfrak{N}|)\)</span>. So in a first-order logic we would not be able to do this (we can only quantify a variable in the domain <span class="math inline">\(|\mathfrak{N}|\)</span>). This approach leads to a <strong>second-order logic</strong> (where we can not only quantify over a plain variable in <span class="math inline">\(|\mathfrak{N}|\)</span>, but also quantify over a relation variable in the power set of the domain, i.e., <span class="math inline">\(\mathcal{P}(|\mathfrak{N}|)\)</span>; that gives our logic more expressive power!).</li>
<li>As we see, a relation is essentially a subset of <span class="math inline">\(|\mathfrak{N}|\)</span> (thus its range is also a set); it is tempting to formulate Peano arithmetic using the notion of sets. Indeed, we can rewrite the formula in Approach 1 into the language of set theory as: <span class="math display">\[\forall y (\emptyset \in y \land \forall x (x \in y \to S(x) \in y)) \to \forall n\ n \in y\]</span> where we encode the standard number <span class="math inline">\(\mathbf{0}\)</span> as <span class="math inline">\(\emptyset\)</span>, <span class="math inline">\(\mathbf{S}x\)</span> as <span class="math inline">\(S(x) = x \cup \{x\}\)</span>. Clearly there is no non-standard number in this set-theoretic model. This is exactly how we define natural numbers <span class="math inline">\(\mathbb{N}\)</span> (as a minimal <em>inductive set</em> <span class="math inline">\(\omega\)</span>) in <strong>set theory</strong>, and its existence is justified by the so-called <em>axiom of infinity</em>. Note that once we introduce the set theory (using a first-order language), we have the equivalently expressive (sometimes paradoxical) power of any arbitrary higher-order logic. Fundamentally.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></li>
</ol>
</section>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Books:</strong></p>
<p>Herbert B. Enderton, <em>A Mathematical Introduction to Logic</em>, 2nd ed.</p>
<p>Kenneth Kunen, <em>The Foundations of Mathematics</em>.</p>
<p><strong>Articles:</strong></p>
<p>Terence Tao, “The completeness and compactness theorems of first-order logic,” <a href="https://terrytao.wordpress.com/2009/04/10/the-completeness-and-compactness-theorems-of-first-order-logic/" class="uri">https://terrytao.wordpress.com/2009/04/10/the-completeness-and-compactness-theorems-of-first-order-logic/</a>.</p>
<p>Asger Törnquist, “The completeness theorem: a guided tour,” <a href="http://www.math.ku.dk/~asgert/teachingnotes/iml-completenessguide.pdf" class="uri">http://www.math.ku.dk/~asgert/teachingnotes/iml-completenessguide.pdf</a>.</p>
<p>Eliezer Yudkowsky, “Godel’s completeness and incompleteness theorems,” <a href="http://lesswrong.com/lw/g1y/godels_completeness_and_incompleteness_theorems/" class="uri">http://lesswrong.com/lw/g1y/godels_completeness_and_incompleteness_theorems/</a>.</p>
<p>Eliezer Yudkowsky, “Standard and nonstandard numbers,” <a href="http://lesswrong.com/lw/g0i/standard_and_nonstandard_numbers/" class="uri">http://lesswrong.com/lw/g0i/standard_and_nonstandard_numbers/</a>.</p>
<p>David A. Ross, “Fun with nonstandard models,” <a href="http://www.math.hawaii.edu/~ross/fun_with_nsmodels.pdf" class="uri">http://www.math.hawaii.edu/~ross/fun_with_nsmodels.pdf</a>.</p>
<p><strong>Papers:</strong></p>
<div id="refs" class="references">
<div id="ref-henkin1949completeness">
<p>[1] L. Henkin, “The completeness of the first-order functional calculus,” <em>The journal of symbolic logic</em>, vol. 14, no. 3, pp. 159–166, 1949. </p>
</div>
</div>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Is there a constructive approach as a replacement of Henkin’s construction? <a href="https://math.stackexchange.com/questions/1462408/is-there-a-constructive-approach-as-a-replacement-of-henkins-construction" class="uri">https://math.stackexchange.com/questions/1462408/is-there-a-constructive-approach-as-a-replacement-of-henkins-construction</a><a href="#fnref1" class="footnoteBack">↩</a></p></li>
<li id="fn2"><p>We are using the notion of decidability/undecidability here even before we get to Gödel’s incompleteness theorem, but hopefully it’s no stranger to us as computability theory has <a href="/mst/6/">all the model-specific issues</a> (though non-logical) covered.<a href="#fnref2" class="footnoteBack">↩</a></p></li>
<li id="fn3"><p>Gödel’s original proof of the completeness theorem: <a href="https://en.wikipedia.org/wiki/Original_proof_of_G%C3%B6del%27s_completeness_theorem">https://en.wikipedia.org/wiki/Original_proof_of_G%C3%B6del%27s_completeness_theorem</a><a href="#fnref3" class="footnoteBack">↩</a></p></li>
<li id="fn4"><p>It might be interesting to know that <span class="math inline">\(\text{Th}(\mathbb{N}; \mathbf{0}, \mathbf{S}, +, \cdot)\)</span> is an undecidable theory, as shown by the aforementioned incompleteness theorem.<a href="#fnref4" class="footnoteBack">↩</a></p></li>
<li id="fn5"><p>If we accept the Axiom of Choice, then every set can have a well-ordering; so this is actually a reasonable request.<a href="#fnref5" class="footnoteBack">↩</a></p></li>
<li id="fn6"><p>Many logicians (Kurt Gödel, Thoralf Skolem, Willard Van Quine) seem to adhere to first-order logic. And that’s why.<a href="#fnref6" class="footnoteBack">↩</a></p></li>
</ol>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>When GNOME Shell Freezes (But You Have Unsaved Stuff There)</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/notes/170902" />
    <id>tag:www.soimort.org,2017:/notes/170902</id>
    <published>2017-09-02T00:00:00+02:00</published>
    <updated>2017-09-02T00:00:00+02:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>In the past few months, I was running into the mysterious bug that GNOME Shell crashes with a segfault in <code>libgjs.so.0.0.0</code>, leaving me an unresponsive desktop:</p>
<pre><code>  kernel: gnome-shell[963]: segfault at 7f06645fffe8 ip 00007f06d94898dd
  sp 00007fff562ee7e0 error 4 in libgjs.so.0.0.0[7f06d9</code></pre>
<p>On crashing GNOME Shell does not restart itself correctly, nor <code>gnome-shell</code> processes are actually terminated, for some unknown reason. When this happens Mutter is completely frozen, <kbd>Alt</kbd>+<kbd>F2</kbd> is unavailable; what’s even worse, <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>F3</kbd> can’t get you a lifesaving TTY.</p>
<p>Clearly, the only way one can access the box then is via SSH. But what to do next? While it is possible to simply restart <code>gdm</code> like: (for systemd users)</p>
<pre><code>$ systemctl restart gdm</code></pre>
<p>This would, however, destroy the whole X session and all your open GUI processes will be lost! A desirable, more elegant approach is to restart <code>gnome-shell</code> alone, but this requires a few tricks in case you’re working through <code>ssh</code> and <code>killall -9 gnome-shell</code> doesn’t restart GNOME Shell properly.</p>
<ol type="1">
<li>Login to the (borked) remote machine, and enable X11 Forwarding in its SSH daemon by modifying <code>/etc/ssh/sshd_config</code>, if you’ve never done it before:<br />
(Note that this option is not the default on most distros; you’ll have to restart <code>sshd</code> on the remote machine after then.)</li>
</ol>
<pre><code>        X11Forwarding yes</code></pre>
<ol start="2" type="1">
<li>Login to the remote machine again, with X11 forwarding enabled this time:</li>
</ol>
<pre><code>        $ ssh yourUser@yourHost -X</code></pre>
<ol start="3" type="1">
<li>On the borked remote machine, kill all the FUBARed <code>gnome-shell</code> processes, if any:</li>
</ol>
<pre><code>        $ pkill gnome-shell</code></pre>
<ol start="4" type="1">
<li>Set the correct <code>DISPLAY</code> environment otherwise you won’t be able to run a WM from a remote shell:</li>
</ol>
<pre><code>        $ export DISPLAY=:0</code></pre>
<ol start="5" type="1">
<li>Bring back the GNOME Shell WM, detached (so it can keep running after closing the remote shell):</li>
</ol>
<pre><code>        $ setsid gnome-shell</code></pre>
<p><strong>Related Gjs bug(s).</strong> The bug was there since gjs 1.48 and has been reported multiple times: <a href="https://bugzilla.gnome.org/show_bug.cgi?id=781799">#781799</a>, <a href="https://bugzilla.gnome.org/show_bug.cgi?id=783935">#783935</a>, <a href="https://bugzilla.gnome.org/show_bug.cgi?id=785657">#785657</a>. Still, a recent gjs 1.49 build crashes my desktop: ( It seems rather hard to find out what would trigger the issue, but no more worries when it bumps in the night – Just get back the WM and no running window is lost.</p>

]]>
    </content>
  </entry>
  <entry>
    <title>Understanding Colors – A Walkthrough of Modern Color Theory and Practice</title>
    <link rel="alternate" type="text/html" href="https://blog.soimort.org/cv/colors/" />
    <id>tag:www.soimort.org,2017:/posts/170803</id>
    <published>2017-08-03T00:00:00+02:00</published>
    <updated>2017-08-03T00:00:00+02:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="blog" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[


]]>
    </content>
  </entry>
  <entry>
    <title>The Stateful Automata</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/6" />
    <id>tag:www.soimort.org,2017:/mst/6</id>
    <published>2017-04-12T00:00:00+02:00</published>
    <updated>2017-04-18T00:00:00+02:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<ul>
<li>What is a <a href="https://wiki.soimort.org/comp/language/">formal language</a>? (review of <a href="/mst/1/">Mst. #1</a>)
<ul>
<li>An <em>alphabet</em> <span class="math inline">\(\Sigma\)</span> is a well-defined set of symbols. A <em>string</em> is a sequence of symbols.</li>
<li>A <em>language</em> <span class="math inline">\(L\)</span> over alphabet <span class="math inline">\(\Sigma\)</span> is a set of strings, where each string <span class="math inline">\(w \in \Sigma^*\)</span>.</li>
<li>We can define a <em>class of languages</em> (which is a set of languages with some common properties).</li>
<li>Operations on languages (as sets): <em>union</em>, <em>intersection</em>, <em>complement</em>, <em>concatenation</em>, <em>Kleene star</em>.</li>
<li>A class of languages is said to be <em>closed under some operation</em>, if the operation yields a language that is in the same class. (closure properties)</li>
</ul></li>
<li><a href="https://wiki.soimort.org/comp/language/finite/"><strong>Finite language</strong></a>: Any finite set of strings.
<ul>
<li>Closure properties: FL is closed under union, intersection and concatenation. (but not under complementation or Kleene star)</li>
</ul></li>
<li><a href="https://wiki.soimort.org/comp/language/regular/"><strong>Regular language</strong></a>: Any language that is recognized by a DFA.
<ul>
<li>Closure properties: RL is closed under union, concatenation and Kleene star. (<em>regular operations</em>)</li>
<li>Equivalent models and conversions:
<ul>
<li><strong>DFA</strong> (Deterministic finite automaton)</li>
<li><strong>NFA</strong> (Nondeterministic finite automaton)
<ul>
<li>DFA to NFA: trivial (every DFA is an NFA).</li>
<li>NFA to DFA: powerset (Rabin-Scott construction).</li>
</ul></li>
<li><strong>Regular expression</strong>
<ul>
<li>RegExp to NFA: structural induction (Thompson’s construction).</li>
<li>RegExp to DFA: via NFA (structural induction and powerset).</li>
<li>NFA to RegExp: via GNFA (node removal).</li>
<li>DFA to RegExp: Kleene’s algorithm.</li>
</ul></li>
</ul></li>
<li>Show that a language is regular:
<ol type="1">
<li>By construction: find a DFA/NFA that recognizes it or a RegExp that describes it.</li>
<li>By the Myhill-Nerode theorem.</li>
<li>By hierarchy: every finite language is regular.</li>
</ol></li>
<li>Show that a language is <em>not</em> regular:
<ol type="1">
<li>By the pumping lemma.</li>
<li>By the Myhill-Nerode theorem.</li>
</ol></li>
</ul></li>
</ul>
<figure>
<img src="https://i0.wp.com/dl.dropboxusercontent.com/s/i05152kpetd8xyc/fsm.png" alt="Conversion among different models of RL" style="width:75.0%" /><figcaption>Conversion among different models of RL</figcaption>
</figure>
<ul>
<li>(Nondeterministic) <a href="https://wiki.soimort.org/comp/language/context-free/"><strong>Context-free language</strong></a>: Any language that is generated by a context-free grammar.
<ul>
<li>Closure properties: CFL is closed under union, concatenation and Kleene star. (but not under complementation or intersection)</li>
<li>Equivalent models:
<ul>
<li><strong>Context-free grammar</strong>
<ul>
<li>Every CFG can be converted into <em>Chomsky normal form</em>.</li>
</ul></li>
<li><strong>PDA</strong> (Nondeterministic pushdown automaton)
<ul>
<li>Finite automaton + 1 infinite stack</li>
</ul></li>
</ul></li>
<li>Show that a language is context-free:
<ol type="1">
<li>By construction: find a CFG that generates it or a PDA that recognizes it.</li>
<li>By hierarchy: every regular language is context-free.</li>
</ol></li>
<li>Show that a language is <em>not</em> context-free:
<ol type="1">
<li>By the pumping lemma.</li>
</ol></li>
</ul></li>
<li><a href="https://wiki.soimort.org/comp/church-turing/">Church-Turing thesis</a>
<ul>
<li>It is (informally) a “thesis” because it cannot be formalized as a provable proposition (since we use it to give a notion for what a <em>computation</em> is).</li>
<li>Computation may be performed on <em>unrestricted languages</em>:
<ul>
<li><em>Recognizing</em> a language <span class="math inline">\(L\)</span>:
<ul>
<li>For every string <span class="math inline">\(w \in L\)</span>, the machine <span class="math inline">\(M\)</span> must <em>accept</em> it.</li>
<li>We call such a machine <span class="math inline">\(M\)</span> an <em>acceptor</em> or a <em>recognizer</em> of the language <span class="math inline">\(L\)</span>.</li>
</ul></li>
<li><em>Deciding</em> a language <span class="math inline">\(L\)</span>:
<ul>
<li>For every string <span class="math inline">\(w \in L\)</span>, the machine <span class="math inline">\(M\)</span> must <em>accept</em> it; moreover, for every <span class="math inline">\(w \notin L\)</span>, the machine <span class="math inline">\(M\)</span> must <em>reject</em> it. That is, the machine <span class="math inline">\(M\)</span> must halt on every input <span class="math inline">\(w\)</span>.</li>
<li>We call such a machine <span class="math inline">\(M\)</span> a <em>decider</em> of the language <span class="math inline">\(L\)</span>.</li>
</ul></li>
</ul></li>
<li>Equivalent <em>Turing-complete models of computation</em>:
<ul>
<li>(Single-tape, deterministic, read-write) <strong>Turing machine</strong>
<ul>
<li>Finite automaton + 1 infinite linear table (or 2 infinite stacks)</li>
</ul></li>
<li>Multi-tape Turing machine</li>
<li>Nondeterministic Turing machine</li>
<li>Enumerator</li>
<li>Abstract rewriting system</li>
<li>Lambda calculus (combinatory logic)</li>
<li>…</li>
</ul></li>
<li>The Church-Turing thesis claims that all Turing-complete models are equivalent in their abilities to compute (recognize / decide a language).</li>
<li>There are some languages that cannot be recognized by any Turing machine, that is, <strong>unrecognizable languages</strong> exist. (shown by the diagonal method)</li>
<li><strong>Turing-recognizable language</strong>: Any language that is recognized by a Turing machine.
<ul>
<li>Closure properties: closed under union, intersection, concatenation and Kleene star. (but not under complementation)</li>
</ul></li>
<li><strong>Co-Turing-recognizable language</strong>: Any language whose complement is recognized by a Turing machine.</li>
<li><strong>Turing-decidable language</strong> (or simply <strong>decidable language</strong>): Any language that is decided by a Turing machine.
<ul>
<li>A language is decidable if and only if it is both Turing-recognizable and co-Turing-recognizable.</li>
<li>Closure properties: closed under union, intersection, complementation, concatenation and Kleene star.</li>
</ul></li>
<li>Show that a language is Turing-recognizable:
<ol type="1">
<li>By construction: find a Turing machine that recognizes it.</li>
<li>By mapping reducibility: <span class="math inline">\(L \leq_\text{m} L&#39;\)</span>, where <span class="math inline">\(L&#39;\)</span> is Turing-recognizable.</li>
<li>By hierarchy: any context-sensitive language (thus also context-free language or regular language) is Turing-recognizable.</li>
</ol></li>
<li>Show that a language is <em>not</em> Turing-recognizable:
<ol type="1">
<li>By contraposition: we already know that <span class="math inline">\(\overline{L}\)</span> is Turing-recognizable; if <span class="math inline">\(L\)</span> is Turing-recognizable, then <span class="math inline">\(L\)</span> would be decidable. But we know that <span class="math inline">\(L\)</span> is undecidable, thus <span class="math inline">\(L\)</span> cannot be Turing-recognizable.</li>
<li>By mapping reducibility: <span class="math inline">\(L&#39; \leq_\text{m} L\)</span>, where <span class="math inline">\(L&#39;\)</span> is not Turing-recognizable.</li>
</ol></li>
<li>Show that a language is decidable:
<ol type="1">
<li>By construction: find a Turing machine that decides it.</li>
<li>Show that it is both Turing-recognizable and co-Turing-recognizable.</li>
<li>By mapping reducibility: <span class="math inline">\(L \leq_\text{m} L&#39;\)</span>, where <span class="math inline">\(L&#39;\)</span> is decidable.</li>
<li>By Turing reducibility: <span class="math inline">\(L \leq_\text{T} L&#39;\)</span>, where <span class="math inline">\(L&#39;\)</span> is decidable.</li>
<li>By hierarchy: any context-sensitive language (thus also context-free language or regular language) is decidable.</li>
</ol></li>
<li>Show that a language is <em>undecidable</em>:
<ol type="1">
<li>By mapping reducibility: <span class="math inline">\(L&#39; \leq_\text{m} L\)</span>, where <span class="math inline">\(L&#39;\)</span> is undecidable.</li>
<li>By Rice’s theorem.</li>
</ol></li>
</ul></li>
</ul>
<section id="chomsky-hierarchy-of-languages-grammars-and-automata" class="level2">
<h2>Chomsky hierarchy of languages, grammars and automata</h2>
<p><span class="math display">\[\text{FL} \subsetneq \text{RL} \subsetneq \text{DCFL} \subsetneq \text{CFL} \subsetneq \text{CSL} \subsetneq \text{R} \subsetneq \text{U} \subsetneq \mathcal{P}(\mathcal{P}(\Sigma^*))
\]</span></p>
<ul>
<li>FL (<strong>Finite languages</strong>), described by finite enumeration, recognized by finite Boolean circuits.</li>
<li>RL (<strong>Regular languages</strong>, Type-3 grammars), described by regular expressions, recognized by DFAs/NFAs.</li>
<li>DCFL (<strong>Deterministic context-free languages</strong>), described by deterministic context-free grammars, recognized by deterministic pushdown automata.</li>
<li>CFL (<strong>Context-free languages</strong>, Type-2 grammars), described by nondeterministic context–free grammars, recognized by nondeterministic pushdown automata.</li>
<li>CSL (<strong>Context-sensitive languages</strong>, Type-1 grammars), described by context-sensitive grammars, recognized by linear bounded automata (restricted form of Turing machines).</li>
<li>R (<strong>Turing-decidable languages</strong> or <strong>recursive languages</strong>), described by recursive grammars, recognized by Turing machines that always halt (i.e., deciders).</li>
<li>U (<strong>Turing-recognizable languages</strong> or <strong>recursively enumerable languages</strong>, Type-0 grammars), described by unrestricted grammars, recognized by Turing machines (but not guaranteed to halt thus not necessarily decidable).</li>
<li><span class="math inline">\(\mathcal{P}(\mathcal{P}(\Sigma^*))\)</span>, may have unrestricted grammars or no grammar at all, not necessarily Turing-recognizable.</li>
</ul>
<p><em>Grammars</em> provide a mathematical, recursive perspective for describing languages, while <em>automata</em> provide a physically implementable, iterative approach of recognizing languages (via finite states and possibly infinite stacks). A language is Turing-recognizable if and only if it has a well-defined grammar (so that it’s possible to construct some Turing machine that recognizes it), although it can sometimes be hard to formulate. Quite naturally, if we cannot specify a grammar for a language <span class="math inline">\(L \subset \mathcal{P}(\Sigma^*)\)</span> theoretically, it would be impossible to construct a Turing machine that recognizes it.</p>
</section>
<section id="decidability-and-undecidability" class="level2">
<h2>Decidability and undecidability</h2>
<p>We consider mainly three types of decision problems concerning different computational models: (generative grammars may be viewed as a form of machines like automata, in the following setting)</p>
<ol type="1">
<li>Acceptance problem: <span class="math inline">\(A_\mathsf{X} = \{ \langle M, w\rangle\ |\ M \text{ is a machine of type } X \text{, and } M \text{ accepts } w \}\)</span>.</li>
<li>Emptiness problem: <span class="math inline">\(E_\mathsf{X} = \{ \langle M \rangle\ |\ M \text{ is a machine of type } X \text{, and } \mathcal{L}(M) = \emptyset \}\)</span>.</li>
<li>Equality problem: <span class="math inline">\(EQ_\mathsf{X} = \{ \langle M_1, M_2 \rangle\ |\ M_1 \text{ and } M_2 \text{ are machines of type } X \text{, and } \mathcal{L}(M_1) = \mathcal{L}(M_2) \}\)</span>.</li>
</ol>
<table style="width:96%;">
<colgroup>
<col style="width: 8%" />
<col style="width: 12%" />
<col style="width: 26%" />
<col style="width: 25%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Language</th>
<th>Acceptance problem</th>
<th>Emptiness problem</th>
<th>Equality problem</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DFA <br>(Deterministic finite automaton)</td>
<td>Regular language: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(A_\textsf{DFA}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(E_\textsf{DFA}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(EQ_\textsf{DFA}\)</span>: <strong><em>decidable</em></strong></td>
</tr>
<tr class="even">
<td>NFA <br>(Nondeterministic finite automaton)</td>
<td>Regular language: <strong>decidable</strong></td>
<td><span class="math inline">\(A_\textsf{NFA}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(E_\textsf{NFA}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(EQ_\textsf{NFA}\)</span>: <strong><em>decidable</em></strong></td>
</tr>
<tr class="odd">
<td>Regular expression</td>
<td>Regular language: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(A_\textsf{REX}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(E_\textsf{REX}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(EQ_\textsf{REX}\)</span>: <strong><em>decidable</em></strong></td>
</tr>
<tr class="even">
<td>CFG <br>(Context-free grammar)</td>
<td>Context-free language: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(A_\textsf{CFG}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(E_\textsf{CFG}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(EQ_\textsf{CFG}\)</span>: <em>undecidable</em></td>
</tr>
<tr class="odd">
<td>PDA <br>(Pushdown automaton)</td>
<td>Context-free language: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(A_\textsf{PDA}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(E_\textsf{PDA}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(EQ_\textsf{PDA}\)</span>: <em>undecidable</em></td>
</tr>
<tr class="even">
<td>CSG <br>(Context-sensitive grammar)</td>
<td>Context-sensitive language: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(A_\textsf{CSG}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(E_\textsf{CSG}\)</span>: <em>undecidable</em></td>
<td><span class="math inline">\(EQ_\textsf{CSG}\)</span>: <em>undecidable</em></td>
</tr>
<tr class="odd">
<td>LBA <br>(Linear bounded automaton)</td>
<td>Context-sensitive language: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(A_\textsf{LBA}\)</span>: <strong><em>decidable</em></strong></td>
<td><span class="math inline">\(E_\textsf{LBA}\)</span>: <em>undecidable</em></td>
<td><span class="math inline">\(EQ_\textsf{LBA}\)</span>: <em>undecidable</em></td>
</tr>
<tr class="even">
<td>Turing machine and equivalent Turing-complete models</td>
<td>Turing-recognizable language: may be <strong><em>decidable</em></strong> or <em>undecidable</em></td>
<td><span class="math inline">\(A_\textsf{TM}\)</span>: <em>undecidable</em></td>
<td><span class="math inline">\(E_\textsf{TM}\)</span>: <em>undecidable</em></td>
<td><span class="math inline">\(EQ_\textsf{TM}\)</span>: <em>undecidable</em> <br>(not Turing-recognizable)</td>
</tr>
</tbody>
</table>
<p>The fact that <span class="math inline">\(A_\mathsf{TM}\)</span> is undecidable implies that a Turing machine may not halt on some input <span class="math inline">\(w\)</span>. Consider the following problem: (<strong>the halting problem</strong> <span class="citation" data-cites="turing1937computable">[1]</span>) <span class="math display">\[\textit{HALT}_\textsf{TM} = \{ \langle M, w \rangle\ | \ M \text{ is a TM, and } M \text{ halts on input } w \}\]</span></p>
<p><strong>Corollary 6.1.</strong> <span class="math inline">\(\textit{HALT}_\textsf{TM}\)</span> is undecidable.</p>
<p><strong>Proof.</strong> (By contraposition) Assume that there exists a Turing machine <span class="math inline">\(R\)</span> that decides <span class="math inline">\(\textit{HALT}_\textsf{TM}\)</span>. We construct a Turing machine <span class="math inline">\(S\)</span> that decides <span class="math inline">\(A_\mathsf{TM}\)</span>, using <span class="math inline">\(R\)</span>:</p>
<blockquote>
<p><span class="math inline">\(S =\)</span> “On input <span class="math inline">\(\langle M, w \rangle\)</span>:</p>
<ol type="1">
<li>Run <span class="math inline">\(R\)</span> on input <span class="math inline">\(\langle M, w \rangle\)</span>.</li>
<li>If <span class="math inline">\(R\)</span> rejects, reject.</li>
<li>If <span class="math inline">\(R\)</span> accepts (so that we know <span class="math inline">\(M\)</span> always halt on <span class="math inline">\(w\)</span>), simulate <span class="math inline">\(M\)</span> on <span class="math inline">\(w\)</span> until it halts.</li>
<li>If <span class="math inline">\(M\)</span> accepts, accept; otherwise, reject.’’</li>
</ol>
</blockquote>
<p>If <span class="math inline">\(R\)</span> decides <span class="math inline">\(\textit{HALT}_\textsf{TM}\)</span>, then <span class="math inline">\(S\)</span> decides <span class="math inline">\(A_\mathsf{TM}\)</span> by construction. Since <span class="math inline">\(A_\mathsf{TM}\)</span> is undecidable, such an <span class="math inline">\(R\)</span> that decides <span class="math inline">\(\textit{HALT}_\textsf{TM}\)</span> does not exist. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>Apart from Turing’s original halting problem, other undecidable problems of historical importance include:</p>
<ul>
<li><strong>Post correspondence problem</strong> (Post 1946 <span class="citation" data-cites="post1946variant">[2]</span>)</li>
<li><strong>Word problem for semigroups</strong> (proposed by Axel Thue in 1914; its undecidability was shown by Emil Post and Andrey Markov Jr. independently in 1947 <span class="citation" data-cites="post1947recursive">[3]</span>)</li>
<li><strong>The busy beaver game</strong> (Rado 1962 <span class="citation" data-cites="rado1962non">[4]</span>)</li>
<li><strong>Hilbert’s tenth problem</strong> (proposed by D. Hilbert in 1900; its undecidability was shown by Matiyasevich’s theorem in 1970, and is in no way an obvious result)</li>
</ul>
<p>Some problems, such as <span class="math inline">\(EQ_\mathsf{TM}\)</span>, are not even Turing-recognizable. Another example is <span class="math inline">\(MIN_\mathsf{TM}\)</span>: <span class="math display">\[MIN_\mathsf{TM} = \{ \langle M \rangle\ | \ M \text{ is a minimal Turing machine} \}\]</span></p>
<p><strong>Theorem 6.2.</strong> <span class="math inline">\(MIN_\mathsf{TM}\)</span> is not Turing-recognizable.</p>
<p><strong>Proof.</strong> Assume that there exists an enumerator <span class="math inline">\(E\)</span> that enumerates <span class="math inline">\(MIN_\mathsf{TM}\)</span> (note that an enumerator is equivalent to a Turing machine). We construct a Turing machine <span class="math inline">\(C\)</span> using <span class="math inline">\(E\)</span>:</p>
<blockquote>
<p><span class="math inline">\(C =\)</span> “On input <span class="math inline">\(w\)</span>:</p>
<ol type="1">
<li>Obtain the own description <span class="math inline">\(\langle C \rangle\)</span>. (by the recursion theorem)</li>
<li>Run <span class="math inline">\(E\)</span> until a machine <span class="math inline">\(D\)</span> appears with a longer description than that of <span class="math inline">\(C\)</span>.</li>
<li>Simulate <span class="math inline">\(D\)</span> on <span class="math inline">\(w\)</span>.’’</li>
</ol>
</blockquote>
<p>Since <span class="math inline">\(MIN_\textsf{TM}\)</span> is infinitely large but the description of <span class="math inline">\(C\)</span> is of finite length, the enumerator <span class="math inline">\(E\)</span> must eventually terminate with some <span class="math inline">\(D\)</span> that has a longer description than that of <span class="math inline">\(C\)</span>. As <span class="math inline">\(C\)</span> simulates <span class="math inline">\(D\)</span> in the last step, <span class="math inline">\(C\)</span> is equivalent to <span class="math inline">\(D\)</span>. But then the description of <span class="math inline">\(C\)</span> is shorter than that of <span class="math inline">\(D\)</span>, thus <span class="math inline">\(D\)</span> could not be an output of <span class="math inline">\(E\)</span> (which enumerates only <span class="math inline">\(MIN_\mathsf{TM}\)</span>). That is a contradiction. Therefore, such an enumerator <span class="math inline">\(E\)</span> for <span class="math inline">\(MIN_\mathsf{TM}\)</span> does not exist, that is, <span class="math inline">\(MIN_\mathsf{TM}\)</span> is not Turing-recognizable. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>The Turing-unrecognizability of <span class="math inline">\(MIN_\textsf{TM}\)</span> implies that the Kolmogorov complexity (descriptive complexity) <span class="math inline">\(K(x)\)</span> is not a computable function.</p>
</section>
<section id="beyond-turing-machines" class="level2">
<h2>Beyond Turing machines</h2>
<p>Per the Church-Turing thesis, a Turing machine (one finite automaton + one infinite linear table) defines the “limitation of computation”. Several hypothetical models (sometimes referred to as <em>hypercomputation</em> <span class="citation" data-cites="davis2004myth">[5]</span> as they are assumed to have the abilities to solve non-Turing-computable problems) have been proposed for the sake of theoretical interest:</p>
<ul>
<li><strong>Oracle machine</strong>: a know-all machine that can solve a certain non-Turing-computable problem such as <span class="math inline">\(A_\textsf{TM}\)</span> or <span class="math inline">\(\textit{HALT}_\textsf{TM}\)</span> (in a black-boxed way).</li>
<li><strong>Real computer</strong> (e.g., Blum-Shub-Smale machine): an idealized analog computer that can compute infinite-precision real numbers. (Real numbers are uncountable as shown by Cantor’s diagonal argument, so Turing machines are incapable of handling arbitrary reals)</li>
<li><strong>Zeno machine</strong> (i.e., supertasking): a Turing machine that can complete infinitely many steps in finite time.</li>
<li><strong>Infinite-time Turing machine</strong>: a Turing machine that is simply allowed to halt in an infinite amount of time.</li>
</ul>
<p>Note that standard (qubit-based) quantum computers are PSPACE-reducible, thus they are still a Turing-complete model of computation (i.e., quantum computers cannot be real computers or Zeno machines, and they do not break the Church-Turing thesis). Moreover, it has been proposed that the simulation of every (quantum) physical process, where only computable reals present, is actually a Turing-computable problem (Church-Turing-Deutsch principle).</p>
</section>
<section id="notes-on-interesting-problems-in-computability-theory" class="level2">
<h2>Notes on interesting problems in computability theory</h2>
<p><strong>Finite languages.</strong> A finite language consists of a finite set of strings, thus it may be written as a regular expression of a finite union of those strings. It may be recognized by a time-independent combinational circuit, which can be viewed as a special form of acyclic finite automaton. (See also the blog post: <a href="https://blog.soimort.org/comp/c/boolean-circuit/">What I Wish I Knew When Learning Boolean Circuits</a>) There are several unsolved problems concerning the circuit complexity.</p>
<p><strong>Generalized regular expression.</strong> A generalized regular expression is defined as <span class="math display">\[R ::= a\ |\ \varepsilon\ |\ \emptyset\ |\ (R_1 \cup R_2)\ |\ (R_1 \cap R_2)\ |\ (R_1 \circ R_2)\ |\ (\lnot R_1)\ |\ (R_1^*)\]</span> where <span class="math inline">\(a \in \Sigma\)</span>, <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> are generalized regular expressions. Although it comes with two extra operators (intersection <span class="math inline">\(\cap\)</span> and complementation <span class="math inline">\(\lnot\)</span>), it actually has the same representational power as regular expressions, due to the fact that the class of regular languages is closed under intersection and complementation.</p>
<p><strong>Star-free languages.</strong> A star-free language is a regular language that can be described by a generalized regular expression but without the Kleene star operation. Clearly, this class includes all finite languages. One example of an infinite star-free language is <span class="math inline">\(a^*\)</span>, because <span class="math inline">\(a^* = \lnot((\lnot\emptyset) \circ (\lnot{a}) \circ (\lnot\emptyset))\)</span>. On the other hand, <span class="math inline">\((a \circ a)^*\)</span> is not star-free.</p>
<p>(Unsolved) <strong>Generalized star height problem.</strong> Can all regular languages be expressed using generalized regular expressions with a limited nesting depth of Kleene stars (star height)? Moreover, is the minimum required star height a computable function? For example, <span class="math inline">\(((((a \circ a)^*)^*)\cdots)^*\)</span> can be expressed as <span class="math inline">\((a \circ a)^*\)</span>, which has a minimum required star height of 1 (but it is not star-free). The general result and whether the minimum star height is decidable are not yet known.</p>
<p><strong>Linear bounded automata.</strong> LBAs provide an accurate model for real-world computers, which have only bounded memories. Given sufficient memory on an LBA, we can simulate another (smaller) LBA; a practical example would be a virtual machine running on VirtualBox or QEMU.</p>
<p>(Unsolved) <strong>LBA problem.</strong> Is the class of languages accepted by LBA equal to the class of languages accepted by deterministic LBA? Or, in terms of the complexity theory, is <span class="math inline">\(\text{SPACE}(n) = \text{NSPACE}(n)\)</span>? We know that DFA and NFA accept the same class of languages, so do TM and NTM, while PDA and DPDA are not completely equivalent. However, it is still an open question whether LBA and DLBA are equivalent models. Note that it is known that <span class="math inline">\(\text{PSPACE} = \bigcup_k \text{SPACE}(n^k)\)</span> is equivalent to <span class="math inline">\(\text{NPSPACE} = \bigcup_k \text{NSPACE}(n^k)\)</span>, that is, deterministic polynomially bounded automata and nondeterministic polynomially bounded automata are equivalent, by Savitch’s theorem.</p>
</section>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Books:</strong></p>
<p>M. Sipser, <em>Introduction to the Theory of Computation</em>, 3rd ed.</p>
<p>J. E. Hopcroft, R. Motwani, J. D. Ullman, <em>Introduction to Automata Theory, Languages, and Computation</em>, 3rd ed.</p>
<p><strong>Articles:</strong></p>
<p>M. Davis, “What is a computation?” <em>Mathematics Today: Twelve Informal Essays</em>, pp. 241–267, 1978.</p>
<p>D. Zeilberger, “A 2-minute proof of the 2nd-most important theorem of the 2nd millennium,” <a href="http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimTeX/halt" class="uri">http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimTeX/halt</a>.</p>
<p><strong>Papers:</strong></p>
<div id="refs" class="references">
<div id="ref-turing1937computable">
<p>[1] A. M. Turing, “On computable numbers, with an application to the Entscheidungsproblem,” <em>Proceedings of the London Mathematical Society</em>, vol. 2, no. 1, pp. 230–265, 1937. </p>
</div>
<div id="ref-post1946variant">
<p>[2] E. L. Post, “A variant of a recursively unsolvable problem,” <em>Bulletin of the American Mathematical Society</em>, vol. 52, no. 4, pp. 264–268, 1946. </p>
</div>
<div id="ref-post1947recursive">
<p>[3] E. L. Post, “Recursive unsolvability of a problem of Thue,” <em>The Journal of Symbolic Logic</em>, vol. 12, no. 01, pp. 1–11, 1947. </p>
</div>
<div id="ref-rado1962non">
<p>[4] T. Rado, “On non-computable functions,” <em>Bell System Technical Journal</em>, vol. 41, no. 3, pp. 877–884, 1962. </p>
</div>
<div id="ref-davis2004myth">
<p>[5] M. Davis, “The myth of hypercomputation,” in <em>Alan Turing: Life and legacy of a great thinker</em>, Springer, 2004, pp. 195–211. </p>
</div>
</div>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>Recovering from a Corrupted Arch Linux Upgrade</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/notes/170407" />
    <id>tag:www.soimort.org,2017:/notes/170407</id>
    <published>2017-04-07T00:00:00+02:00</published>
    <updated>2017-04-08T00:00:00+02:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="engineering" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p><strong>(Probably unimportant) Backstory:</strong> It’s been a while since the last full-system upgrade of my Arch Linux laptop – It sat well with me for a span of months’ uptime. When I finally got the time to run a <code>pacman -Syu</code> (so well as to dare any potential issues due to Arch’s infamous instability) earlier today, weird things did happen: Something other than <code>pacman</code> was eating up my CPU. I assumed it was Chromium in the first place, unless it wasn’t this time. It turned out to be <code>gnome-settings-daemon</code> together with some other GUI processes running on GNOME, that were consistently hogging my CPU in turn. And that made a percent of 100% (!). As I killed the seemingly problematic process, something else would insist to emerge on the top of <code>top</code> with similarly excessive CPU usage. I decided that this was an unusual situation and without much further thought, forced a <code>reboot</code> while <code>pacman</code> was still running. Then I ended up with a corrupted system: the kernel didn’t load during the early boot process, complaining about missing <code>modules.devname</code> and that <code>/dev/sda11</code> (which is the root partition) could not be found:</p>
<pre><code>  Warning: /lib/modules/4.10.8-1-ARCH/modules.devname not found - ignoring
  starting version 232
  Waiting 10 seconds for device /dev/sda11 ...
  ERROR: device &#39;/dev/sda11&#39; not found. Skipping fsck.
  mount: you must specify the filesystem type
  You are now being dropped into an emergency shell.</code></pre>
<p>Without proper modules loaded even the keyboard was not working, so I couldn’t do anything under the emergency shell. But I wasn’t completely out of luck – I have a dual-booting of Arch and Ubuntu, which is prepared just for recovery purposes like this. (otherwise I had to make room for another Live USB, which was a little bit inconvenient; I don’t use those rescue-ware for years)</p>
<p><strong>Recover an (unbootable) Arch system from another distro:</strong> This is mostly relevant if you can’t boot into the system (due to a corrupted kernel upgrade), otherwise it’s possible to just login from TTY (locally or via SSH) and perform the fix (e.g., in the case that it’s just X, display manager or WM that fails to load but the kernel boots well).</p>
<ol type="1">
<li>On the host system (e.g., Ubuntu), create a working chroot environment: (Assume that the root for Arch is mounted to <code>/dev/sda11</code>)</li>
</ol>
<pre><code>    # TARGETDEV=&quot;/dev/sda11&quot;
    # TARGETDIR=&quot;/mnt/arch&quot;

    # mount $TARGETDEV $TARGETDIR
    # mount -t proc /proc $TARGETDIR/proc
    # mount --rbind /sys $TARGETDIR/sys
    # mount --rbind /dev $TARGETDIR/dev

    # cp /etc/hosts $TARGETDIR/etc
    # cp /etc/resolv.conf $TARGETDIR/etc
    # chroot $TARGETDIR rm /etc/mtab 2&gt; /dev/null
    # chroot $TARGETDIR ln -s /proc/mounts /etc/mtab

    # chroot $TARGETDIR</code></pre>
<p>Alternatively, use the <code>arch-chroot</code> script from an <a href="https://mirrors.kernel.org/archlinux/iso/latest/">Arch bootstrap image</a>:</p>
<pre><code>    # arch-chroot $TARGETDIR</code></pre>
<ol start="2" type="1">
<li>Now that we have a chroot environment with access to the (currently broken) Arch system, continue with unfinished <code>pacman</code> upgrades and clean up any lock file if needed:</li>
</ol>
<pre><code>    [/mnt/arch]# pacman -Syu</code></pre>
<p>Or, if there’s any unwanted package upgrade that causes the system to fail, downgrade it.<br />
The important thing to remember is that, while continuing, a previously failed transaction will not run its hooks anymore. Therefore it might be wiser to find out which exact packages <code>pacman</code> was trying to upgrade in the last transaction (from <code>/var/log/pacman.log</code>) and <em>reinstall</em> all of them, rather than a complementing <code>pacman -Syu</code>. But if it’s known which package is causing the problem (and in this very common case, <code>linux</code>), simply run the following to regenerate initramfs for the new kernel:</p>
<pre><code>    [/mnt/arch]# mkinitcpio -p linux</code></pre>
<p>And we are done. A few commands and our Arch system is now back and booting.</p>
<p><strong>Why the mess?</strong> Initramfs images need to be regenerated (from a corresponding mkinitcpio preset file) after each kernel update. Starting from Arch’s package <code>linux 4.8.8-2</code>, the calling of <code>mkinitcpio</code> was specified in a PostTransaction alpm hook, in place of an explicit <code>post_install()</code> command to be invoked from the <code>.install</code> script <a href="https://git.archlinux.org/svntogit/packages.git/commit/trunk/linux.install?h=packages/linux&amp;id=617173dcde960f00112ebdfee4c80ede71e67375">as before</a>: (The move towards alpm hooks is said to be a fix for <a href="https://bugs.archlinux.org/task/51818">#51818</a>, as two or more packages in one transaction may need the regeneration of initramfs images)</p>
<div class="sourceCode"><pre class="sourceCode ini"><code class="sourceCode ini"><div class="sourceLine" id="1" href="#1" data-line-number="1"><span class="kw">[Trigger]</span></div>
<div class="sourceLine" id="2" href="#2" data-line-number="2"><span class="dt">Type </span><span class="ot">=</span><span class="st"> File</span></div>
<div class="sourceLine" id="3" href="#3" data-line-number="3"><span class="dt">Operation </span><span class="ot">=</span><span class="st"> Install</span></div>
<div class="sourceLine" id="4" href="#4" data-line-number="4"><span class="dt">Operation </span><span class="ot">=</span><span class="st"> Upgrade</span></div>
<div class="sourceLine" id="5" href="#5" data-line-number="5"><span class="dt">Target </span><span class="ot">=</span><span class="st"> boot/vmlinuz-%PKGBASE%</span></div>
<div class="sourceLine" id="6" href="#6" data-line-number="6"><span class="dt">Target </span><span class="ot">=</span><span class="st"> usr/lib/initcpio/*</span></div>
<div class="sourceLine" id="7" href="#7" data-line-number="7"></div>
<div class="sourceLine" id="8" href="#8" data-line-number="8"><span class="kw">[Action]</span></div>
<div class="sourceLine" id="9" href="#9" data-line-number="9"><span class="dt">Description </span><span class="ot">=</span><span class="st"> Updating %PKGBASE% initcpios</span></div>
<div class="sourceLine" id="10" href="#10" data-line-number="10"><span class="dt">When </span><span class="ot">=</span><span class="st"> PostTransaction</span></div>
<div class="sourceLine" id="11" href="#11" data-line-number="11"><span class="dt">Exec </span><span class="ot">=</span><span class="st"> /usr/bin/mkinitcpio -p %PKGBASE%</span></div></code></pre></div>
<p>The faulty part about PostTransaction alpm hooks is that they run in a <em>post-transaction</em> manner; in the context of Arch Linux, a “transaction” means a complete run of <code>pacman</code>, regardless of how many packages it installs or upgrades. Such PostTransaction hooks will not run until all preceding operations succeed. Quoted from the CAVEATS section in the <a href="https://www.archlinux.org/pacman/alpm-hooks.5.html">alpm-hooks(5) Manual Page</a>:</p>
<blockquote>
<p>“PostTransaction hooks will <strong>not</strong> run if the transaction fails to complete for any reason.”</p>
</blockquote>
<p>This (kind of) misbehavior could lead to unintended aftermath in a failed system upgrade, as it does in the case of the <code>linux</code> package. Confusingly, a <code>pacman</code> “transaction” can be an arbitrary set of unrelated package updates; it could possibly fail to complete at any point (e.g., <code>.install</code> script errors, process accidentally killed, or even power lost), and ideally, it should remain safe while encountering such failures before commit. However, since the generation of <code>initramfs</code> in the PostTransaction hook is actually an <strong>indispensable</strong> step for the newly upgraded <code>linux</code> package here (whether other packages complete their upgrades or not), it is not safe if a <code>pacman</code> “transaction” fails halfway; PostTransaction hook won’t run and all you get is a halfway system that doesn’t boot (so you’ll need to fix that through a fallback or alternative kernel). To see this problem more clearly, think of the following two scenarios:</p>
<p><strong><em>A successful system upgrade:</em></strong></p>
<pre><code>    pacman -Syu
        sanity check (for packages to upgrade)
            |
        start upgrading package L (requires hook M to run)
            |
        finish upgrading package L
        start upgrading package A
            |
        finish upgrading package A
        start upgrading package B
            |
        finish upgrading package B
        complete transaction
        run post-transaction hook M
            |
        done.
    [packages L, A, B are upgraded; hook M is run; working system]</code></pre>
<p><strong><em>An (ungracefully) failing system upgrade:</em></strong></p>
<pre><code>    pacman -Syu
        sanity check (for packages to upgrade)
            |
        start upgrading package L (requires hook M to run)
            |
        finish upgrading package L
        start upgrading package A
            |
        finish upgrading package A
        start upgrading package B
            |
            crash!
    [packages L, A are upgraded; hook M is not run; broken system]</code></pre>
<p><strong>How to perform a system upgrade safely?</strong> Although it ultimately depends on how we define a “safe system upgrade” and how much we want it, some robustness may be achieved at least. It’s clear to see that the upgrade of package <code>L</code> and the run of hook <code>M</code> should be dealt as one transaction, or treating <code>M</code> in a post-transaction way will put us on the risk that if some other package fails to upgrade (or maybe it’s just computer crashes), we end up with having only <code>L</code> upgraded, but not <code>M</code> run (while it is desirable to have both or none).</p>
<p><strong><em>A (gracefully) failing system upgrade:</em></strong></p>
<pre><code>        sanity check (for packages to upgrade)
            |
        start upgrading package L (requires hook M to run)
            |
        finish upgrading package L
        run hook M
            |
        complete transaction
        start upgrading package A
            |
        finish upgrading package A
        start upgrading package B
            |
            crash!
    [packages L, A are upgraded; hook M is run; working system]</code></pre>
<p>This is exactly the way things worked before (using the old-fashioned <code>post_install()</code>): It’s less fragile under possible failures, although in order to fix #51818 (another package <code>L2</code> also requires hook <code>M</code> to run), we’ll need to manage the ordering of package upgrades properly:</p>
<pre><code>        start upgrading package L (requires hook M to run)
            |
        finish upgrading package L
        start upgrading package L2 (requires hook M to run)
            |
        finish upgrading package L2
        run hook M
            |
        complete transaction
        start upgrading package A
            ...</code></pre>
<p>That is, <code>L</code>, <code>L2</code> and <code>M</code> are one transaction. It is not allowed to have only <code>L</code> and <code>L2</code>, or only <code>L</code>, without a run of hook <code>M</code>. Other unrelated packages (<code>A</code>, <code>B</code>, …) are prevented from causing such issues in the above scheme. There is still a possibility that the process crashes during the phase of <code>L</code>, <code>L2</code> or <code>M</code>, but the chance would be much smaller, because we are working with a transaction consisting of the strongly coupling upgrades of <code>L</code>, <code>L2</code> and <code>M</code>, not a whole <code>pacman</code> “transaction” consisting of easily hundreds of arbitrary system upgrades and post-transaction hooks. Even better, in case of such failures, we can ask to redo uncommitted transactions anyway so we don’t rerun hooks manually:</p>
<pre><code>        redo uncommitted transactions (upgrades &amp; hooks)
            |
        start upgrading package B
            ...</code></pre>
<p>I don’t know if there’s any software management tool implementing this fully recovering approach. Transactions need to be more refined (<code>pacman</code>’s “all is a transaction” is really a naïve semantics to have) and the ordering of upgrades is essential, in order to give best robustness in case of possible crashes during a system upgrade, so it might not be as implementable as most package managers are (although still far less sophisticated than a real-world database). Or – with reasonable efforts – if we cannot guarantee anything for a transaction – maybe a Windows-inspired “user-friendly” warning message like “Don’t turn off your computer” is good enough. (At least you’ll know when you’re breaking shit: a transaction is not completed yet, some necessary hooks are not run, and you’re on your own then.)</p>
<section id="references" class="level2">
<h2>References</h2>
<p>[1] ArchWiki, “Install from existing Linux”. <a href="https://wiki.archlinux.org/index.php/Install_from_existing_Linux" class="uri">https://wiki.archlinux.org/index.php/Install_from_existing_Linux</a></p>
<p>[2] ArchWiki, “Change root”. <a href="https://wiki.archlinux.org/index.php/change_root" class="uri">https://wiki.archlinux.org/index.php/change_root</a></p>
<p>[3] ArchWiki, “mkinitcpio”. <a href="https://wiki.archlinux.org/index.php/mkinitcpio" class="uri">https://wiki.archlinux.org/index.php/mkinitcpio</a></p>
<p>[4] alpm-hooks(5) Manual Page. <a href="https://www.archlinux.org/pacman/alpm-hooks.5.html" class="uri">https://www.archlinux.org/pacman/alpm-hooks.5.html</a></p>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>A Pandoc Filter for Typesetting Operational Semantics</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/notes/170323" />
    <id>tag:www.soimort.org,2017:/notes/170323</id>
    <published>2017-03-23T00:00:00+01:00</published>
    <updated>2017-03-24T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="tooling" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>Recently I decided to make my life a little easier by simplifying the laborious, tedious math typing that I had to do from time to time. For me this is most relevant for typesetting the formal semantics of programming languages, where proof trees are interwoven with a variety of programming constructs. Here’s my initial intention: To typeset something like (in big-step semantics) <span class="math display">\[
\frac{
  \stackrel{\mathcal{E}_0}
    {\langle b, \sigma \rangle \downarrow \textbf{true}} \qquad
  \stackrel{\mathcal{E}_1}
    {\langle c_0, \sigma \rangle \downarrow \sigma&#39;&#39;} \qquad
  \stackrel{\mathcal{E}_2}
    {\langle \textbf{while } b \textbf{ do } c_0, \sigma&#39;&#39; \rangle \downarrow \sigma&#39;}
  }{
    \langle \textbf{while } b \textbf{ do } c_0, \sigma \rangle \downarrow \sigma&#39;}
\]</span> Instead of the bloated TeX syntax:</p>
<div class="sourceCode"><pre class="sourceCode latex"><code class="sourceCode latex"><div class="sourceLine" id="1" href="#1" data-line-number="1">  <span class="fu">\frac</span>{</div>
<div class="sourceLine" id="2" href="#2" data-line-number="2">    <span class="fu">\stackrel</span>{<span class="fu">\mathcal</span>{E}_0}</div>
<div class="sourceLine" id="3" href="#3" data-line-number="3">      {<span class="fu">\langle</span> b, <span class="fu">\sigma</span> <span class="fu">\rangle</span> <span class="fu">\downarrow</span> <span class="fu">\textbf</span>{true}} <span class="fu">\qquad</span></div>
<div class="sourceLine" id="4" href="#4" data-line-number="4">    <span class="fu">\stackrel</span>{<span class="fu">\mathcal</span>{E}_1}</div>
<div class="sourceLine" id="5" href="#5" data-line-number="5">      {<span class="fu">\langle</span> c_0, <span class="fu">\sigma</span> <span class="fu">\rangle</span> <span class="fu">\downarrow</span> <span class="fu">\sigma</span>&#39;&#39;} <span class="fu">\qquad</span></div>
<div class="sourceLine" id="6" href="#6" data-line-number="6">    <span class="fu">\stackrel</span>{<span class="fu">\mathcal</span>{E}_2}</div>
<div class="sourceLine" id="7" href="#7" data-line-number="7">      {<span class="fu">\langle</span> <span class="fu">\textbf</span>{while } b <span class="fu">\textbf</span>{ do } c_0, <span class="fu">\sigma</span>&#39;&#39; <span class="fu">\rangle</span> <span class="fu">\downarrow</span> <span class="fu">\sigma</span>&#39;}</div>
<div class="sourceLine" id="8" href="#8" data-line-number="8">  }{</div>
<div class="sourceLine" id="9" href="#9" data-line-number="9">    <span class="fu">\langle</span> <span class="fu">\textbf</span>{while } b <span class="fu">\textbf</span>{ do } c_0, <span class="fu">\sigma</span> <span class="fu">\rangle</span> <span class="fu">\downarrow</span> <span class="fu">\sigma</span>&#39;}</div></code></pre></div>
<p>I could just write it more neatly, intuitively, without all those hexes in magic backslashes and braces:</p>
<pre><code>  &lt;b, sig&gt; ! true                 -- E_0
  &lt;c_0, sig&gt; ! sig&#39;&#39;              -- E_1
  &lt;while b do c_0, sig&#39;&#39;&gt; ! sig&#39;  -- E_2
  ----
  &lt;while b do c_0, sig&gt; ! sig&#39;</code></pre>
<p>But, of course, preserving the nice-looking outcome (from either TeX or MathJax).</p>
<p>Besides natural derivations, we also got a lot of evidently agreeable conventions for typesetting general mathematics: parentheses (brackets, braces) are almost always paired; “<code>\left</code>” and “<code>\right</code>” are very often desired since <span class="math inline">\(\left[\frac{1}{2}\right]\)</span> is a bit less awkward than <span class="math inline">\([\frac{1}{2}]\)</span>; when referring to a function name, “<code>\operatorname</code>” looks aesthetically better; etc. Furthermore, if we write down some math notations in plain text, clearly, “<code>=&gt;</code>” has to be a “<span class="math inline">\(\Rightarrow\)</span>” and “<code>|-&gt;</code>” has to be a “<span class="math inline">\(\mapsto\)</span>”; “<code>|-</code>” means “<span class="math inline">\(\vdash\)</span>” and “<code>|=</code>” means “<span class="math inline">\(\vDash\)</span>”; a standalone letter “<code>N</code>” is just <span class="math inline">\(\mathbb{N}\)</span>; etc. Taking such matters into consideration, there could be some handy alternative syntax (I call it “lazybones’ syntax”) for typesetting formulas, at least for a specific field someone specializes in, where these conventions are consistent:</p>
<table style="width:75%;">
<colgroup>
<col style="width: 23%" />
<col style="width: 27%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Typesetting outcome</th>
<th>Syntax</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Provability</td>
<td><span class="math inline">\(\Gamma \vdash \varphi\)</span></td>
<td><strong>TeX:</strong><br> <code>\Gamma \vdash \varphi</code> <br> <strong>Lazybones’:</strong><br> <code>Gam |- phi</code></td>
</tr>
<tr class="even">
<td>Validity</td>
<td><span class="math inline">\(\Gamma \vDash \varphi\)</span></td>
<td><strong>TeX:</strong><br> <code>\Gamma \vDash \varphi</code> <br> <strong>Lazybones’:</strong><br> <code>Gam |= phi</code></td>
</tr>
<tr class="odd">
<td>Big-step semantics</td>
<td><span class="math inline">\(\langle b_0 \land b_1, \sigma \rangle \downarrow t\)</span></td>
<td><strong>TeX:</strong><br> <code>\langle b_0 \land b_1, \sigma</code><br><code>\rangle \downarrow t</code> <br> <strong>Lazybones’:</strong><br> <code>&lt;b_0 &amp;&amp; b_1, sig&gt; ! t</code></td>
</tr>
<tr class="even">
<td>Small-step semantics</td>
<td><span class="math inline">\(\sigma \vdash b_0 \land b_1 \to^* t\)</span></td>
<td><strong>TeX:</strong><br> <code>\sigma \vdash b_0 \land b_1</code><br><code>\to^* t</code> <br> <strong>Lazybones’:</strong><br> <code>sig |- b_0 &amp;&amp; b_1 -&gt;* t</code></td>
</tr>
<tr class="odd">
<td>Hoare logic</td>
<td><span class="math inline">\(\vdash \{A\} \textbf{if } b \textbf{ then } c_0 \textbf{ else } c_1 \{B\}\)</span></td>
<td><strong>TeX:</strong><br> <code>\vdash \{A\} \textbf{if } b</code><br><code>\textbf{ then } c_0</code><br><code>\textbf{ else } c_1 \{B\}</code> <br> <strong>Lazybones’:</strong><br> <code>|- &lt;[A]&gt; if b then c_0</code><br><code>else c_1 &lt;[B]&gt;</code></td>
</tr>
<tr class="even">
<td>Denotational semantics</td>
<td><span class="math inline">\(\mathcal{C}[\![X:=a]\!] = \lambda \sigma . \eta (\sigma[X \mapsto \mathcal{A}[\![a]\!] \sigma])\)</span></td>
<td><strong>TeX:</strong><br> <code>\mathcal{C}[\![X:=a]\!] =</code><br><code>\lambda \sigma . \eta</code><br><code>(\sigma[X \mapsto</code><br><code>\mathcal{A}[\![a]\!] \sigma])</code> <br> <strong>Lazybones’:</strong><br> <code>C[[X:=a]] = lam sig . eta</code><br><code>(sig[X |-&gt; A[[a]] sig])</code></td>
</tr>
</tbody>
</table>
<p>For simplicity, we require that all such transformations are <em>regular</em>, i.e., “lazybones’ syntax” may be translated into plain TeX code using merely substitutions of regular expressions.</p>
<p>As a basic example, let’s consider angle brackets, for which we desire to type in simply “<code>&lt;</code>” and “<code>&gt;</code>” instead of “<code>\langle</code>” and “<code>\rangle</code>”. To avoid any ambiguity (with normal less-than or greater-than sign), it is required that such brackets have no internal spacing, that is, we write “<code>&lt;x, y&gt;</code>” rather than “<code>&lt; x, y &gt;</code>”, but “<code>1 &lt; 2</code>” instead of “<code>1&lt;2</code>”. Once we fix the transformation rules, implementation would be quite straightforward in Haskell with <code>Text.Regex</code>, if we want a pandoc filter to handle these substitutions for everything embedded in TeX math mode:</p>
<div class="sourceCode"><pre class="sourceCode hs"><code class="sourceCode haskell"><div class="sourceLine" id="1" href="#1" data-line-number="1">subLAngle s <span class="fu">=</span></div>
<div class="sourceLine" id="2" href="#2" data-line-number="2">  subRegex (mkRegex <span class="st">&quot;&lt;([^[:blank:]])&quot;</span>) s <span class="fu">$</span> <span class="st">&quot;\\langle \\1&quot;</span></div>
<div class="sourceLine" id="3" href="#3" data-line-number="3">subRAngle s <span class="fu">=</span></div>
<div class="sourceLine" id="4" href="#4" data-line-number="4">  subRegex (mkRegex <span class="st">&quot;([^[:blank:]])&gt;&quot;</span>) s <span class="fu">$</span> <span class="st">&quot;\\1\\rangle &quot;</span></div>
<div class="sourceLine" id="5" href="#5" data-line-number="5"></div>
<div class="sourceLine" id="6" href="#6" data-line-number="6">semType m<span class="fu">@</span>(<span class="dt">Math</span> mathType s) <span class="fu">=</span> <span class="kw">do</span></div>
<div class="sourceLine" id="7" href="#7" data-line-number="7">  <span class="kw">let</span> s&#39; <span class="fu">=</span> subLAngle <span class="fu">$</span> subRAngle s</div>
<div class="sourceLine" id="8" href="#8" data-line-number="8">  return <span class="fu">$</span> <span class="dt">Math</span> mathType s&#39;</div>
<div class="sourceLine" id="9" href="#9" data-line-number="9">semType x <span class="fu">=</span> return x</div></code></pre></div>
<p>So, why bother implementing a pandoc filter rather than just defining some TeX macros? Two main reasons:</p>
<ol type="1">
<li>TeX code is nontrivial to write. And you can’t get rid of the overwhelming backslashes quite easily. It would be very hard, if not impossible, to achieve the same thing you’d do with a pandoc filter. (I would say that TeX is rather fine for <em>typesetting</em>, but <em>only</em> for typesetting; actual programming/manipulating macros in TeX seems very awkward for me.)</li>
<li>TeX macros are just… code in TeX, which means that they’re not designated to work for non-DVI originated publishing formats, such as HTML with MathJax. So if you do a lot of heavy lifting with TeX macros, they won’t play well for display on the Web (unless you’re willing to convert math formulas into pictures).</li>
</ol>
<p>Surely you can write pandoc filters in <em>any</em> language as you like (officially Haskell and Python). Nothing like plain TeX (despite the fact that it’s Turing-complete): filters are easier to implement, unrestricted in the way that they interact with structured documents, with a great deal of cool supporting libraries even to blow up the earth. Although here in this case, we are only using some no-brainer pattern matching to release our fingers from heavy math typing.</p>
<p>A proof-of-concept of this is in <a href="https://github.com/soimort/pandoc-filters/blob/aad6033dfbd7d460d22aa627e04ca388f72af020/semtype.hs">semtype.hs</a>. While undocumented, all tricks it does should be clear from the comments and regular expressions themselves.</p>

]]>
    </content>
  </entry>
  <entry>
    <title>An Intuitive Exposition of<br />
“Proof by Contradiction vs. Proof of Negation”</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/notes/170306" />
    <id>tag:www.soimort.org,2017:/notes/170306</id>
    <published>2017-03-06T00:00:00+01:00</published>
    <updated>2017-03-06T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="logic" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p style="text-align:center !important;text-indent:0 !important">(Inspired by <a href="https://existentialtype.wordpress.com/2017/03/04/a-proof-by-contradiction-is-not-a-proof-that-derives-a-contradiction/"><em>A “proof by contradiction” is not a proof that ends with a contradiction</em></a> by Robert Harper.)</p>
<p>There seem to be some common misconceptions about “proof by contradiction”. Recently I came across Robert’s <a href="https://existentialtype.wordpress.com/2017/03/04/a-proof-by-contradiction-is-not-a-proof-that-derives-a-contradiction/">blog post</a> on this issue, and couldn’t help reviewing it in my own way. I prefer a more formal treatment, nevertheless, without sacrificing its intuitive comprehensibility.</p>
<p>What is a “proof by contradiction”, exactly? When a classical mathematician (probably you) talks about proving by contradiction, they can mean either one of these two (syntactically) different things:</p>
<ol type="1">
<li>Prove <span class="math inline">\(P\)</span>: Assume <span class="math inline">\(\lnot P\)</span>, we derive a contradiction (<span class="math inline">\(\bot\)</span>). Therefore, we have <span class="math inline">\(P\)</span>.</li>
<li>Prove <span class="math inline">\(\lnot P\)</span>: Assume <span class="math inline">\(P\)</span>, we derive a contradiction (<span class="math inline">\(\bot\)</span>). Therefore, we have <span class="math inline">\(\lnot P\)</span>.</li>
</ol>
<p>Both syntactic forms have some kind of <em>reductio ad absurdum</em> (reduction to / derivation of contradiction) argument. However, the first form provides an <em>indirect proof</em> for <span class="math inline">\(P\)</span>; this is what we call a genuine “proof by contradiction”. The second form provides a <em>direct proof</em> for <span class="math inline">\(\lnot P\)</span> which is just the negation of <span class="math inline">\(P\)</span>; this is preferably called a “proof of negation”, as it’s not a proof of <span class="math inline">\(P\)</span> itself, but rather a proof of the negation of <span class="math inline">\(P\)</span>.</p>
<p>But how are they any different? You may ask. In a classical setting, there is no semantic difference. Notice that in the first form (“proof by contradiction”), we could rewrite <span class="math inline">\(P\)</span> as <span class="math inline">\(\lnot Q\)</span>, then we get</p>
<ol start="3" type="1">
<li>Prove <span class="math inline">\(\lnot Q\)</span>: Assume <span class="math inline">\(Q\)</span>, we derive a contradiction (<span class="math inline">\(\bot\)</span>). Therefore, we have <span class="math inline">\(\lnot Q\)</span>.</li>
</ol>
<p>which is just the second form, i.e., a “proof of negation”. Likewise, if we rewrite <span class="math inline">\(P\)</span> as <span class="math inline">\(\lnot Q\)</span> in the second form, we get</p>
<ol start="4" type="1">
<li>Prove <span class="math inline">\(Q\)</span>: Assume <span class="math inline">\(\lnot Q\)</span>, we derive a contradiction (<span class="math inline">\(\bot\)</span>). Therefore, we have <span class="math inline">\(Q\)</span>.</li>
</ol>
<p>which is just the first form, i.e., a “proof by contradiction”. That’s the very reason people often misconceive them – classically, a proof by contradiction and a proof of negation can be simply converted to the form of one another, without losing their semantic validity.</p>
<p>From a constructivist perspective, things are quite different. In the above rewritten forms, we introduced a new term <span class="math inline">\(\lnot Q := P\)</span>. For the rewrite in the 3rd form to be valid, the new assumption <span class="math inline">\(Q\)</span> must be as strong as the original assumption <span class="math inline">\(\lnot P\)</span>, which is just <span class="math inline">\(\lnot \lnot Q\)</span>. Formally, there must be <span class="math inline">\(\lnot \lnot Q \implies Q\)</span>. For the rewrite in the 4th form to be valid, the new statement <span class="math inline">\(Q\)</span> must be not any stronger than the original statement <span class="math inline">\(\lnot P\)</span>, so formally there must also be <span class="math inline">\(Q \implies \lnot \lnot Q\)</span>. In intuitionistic logic, although we can derive a “double negation introduction” (thus complete the rewrite in the 4th form), there is no way to derive a “double negation elimination” as required to get the 3rd form. So technically, while we can soundly rewrite from a “proof of negation” to a “proof by contradiction”, the other direction is impossible. Indeed, we must make a clear distinction between a “proof by contradiction” and a “proof of negation” here: Semantically, they are not even dual and should not be fused with each other.</p>
<p>Why is this distinction important? Because in intuitionistic logic, the second form (proof of negation) is a valid proof; the first form (proof by contradiction) is not. Take a look at the negation introduction rule: <span class="math display">\[\lnot_\mathsf{I} : \frac{\Gamma, P \vdash \bot}{\Gamma \vdash \lnot P}\]</span> which justifies the validity of “proof of negation”. However, there is no such rule saying that <span class="math display">\[\mathsf{PBC} : \frac{\Gamma, \lnot P \vdash \bot}{\Gamma \vdash P}\]</span> In classical logic, where a rule like <span class="math inline">\(\mathsf{PBC}\)</span> is allowed, one can easily derive the double negation elimination which we begged for before: Given <span class="math inline">\(\Gamma \vdash \lnot \lnot P\)</span>, the only rule that ever introduces a negation is <span class="math inline">\(\lnot_\mathsf{I}\)</span>, so we must also have <span class="math inline">\(\Gamma, \lnot P \vdash \bot\)</span>. Then by <span class="math inline">\(\mathsf{PBC}\)</span>, we get a derivation of <span class="math inline">\(\Gamma \vdash P\)</span>, as desired. <span class="math display">\[\mathsf{DNE} : \frac{\Gamma \vdash \lnot \lnot P}{\Gamma \vdash P}\]</span> If we adopt <span class="math inline">\(\mathsf{PBC}\)</span>, then we will also have adopted <span class="math inline">\(\mathsf{DNE}\)</span>; if we have <span class="math inline">\(\mathsf{DNE}\)</span>, then it would be perfectly valid to rewrite a “proof by contradiction” into the form of a “proof of negation”, or the other way around, as is already shown before. Since constructivists do not want to accept rules like <span class="math inline">\(\mathsf{PBC}\)</span> or <span class="math inline">\(\mathsf{DNE}\)</span> at all, they claim that a “proof by contradiction” and a “proof of negation” are essentially different, in that the latter is a valid proof but the former is doubtful, while their distinction is purely syntactical for classical mathematicians as the semantic equivalence would be trivial with <span class="math inline">\(\mathsf{PBC}\)</span> or <span class="math inline">\(\mathsf{DNE}\)</span>.</p>
<p>The rationale behind constructivists’ choice of ruling out indirect proofs by rejecting derivation rules like <span class="math inline">\(\mathsf{PBC}\)</span> and <span class="math inline">\(\mathsf{DNE}\)</span> comes into view when talking about first-order theories, where existential quantifiers are used. Say, if we wish to prove that there exists some <span class="math inline">\(x\)</span> with a property <span class="math inline">\(P\)</span>, we must specify an example <span class="math inline">\(t\)</span> which makes this property holds: <span class="math display">\[\exists_{\mathsf{I}_t} : \frac{\Gamma \vdash P[t/x]}{\Gamma \vdash \exists x : S.P}\]</span></p>
<p style="text-align:center !important;text-indent:0 !important">(<span class="math inline">\(t\)</span> is a term of sort <span class="math inline">\(S\)</span>)</p>
<p>This is something called a <em>constructive proof</em>, in the sense that in order to derive an existentialization, one must construct such a term explicitly, directly.</p>
<p>What if we allow <span class="math inline">\(\mathsf{PBC}\)</span> in our proofs then? We will be given enough power to utilize an alternate approach: Assume to the contrary that for all <span class="math inline">\(x\)</span> in <span class="math inline">\(S\)</span>, <span class="math inline">\(\lnot P\)</span> holds. Then we derive a contradiction. Thus, by <span class="math inline">\(\mathsf{PBC}\)</span>, there must exist some <span class="math inline">\(x\)</span> such that <span class="math inline">\(P\)</span> holds (since <span class="math inline">\(\lnot (\exists x : S.P) \equiv \forall x : S.\lnot P\)</span>). Formally, <span class="math display">\[\frac{\Gamma, \forall x : S.\lnot P \vdash \bot}{\Gamma \vdash \exists x : S.P}\]</span> Note that a term <span class="math inline">\(t\)</span> is nowhere to be evident in this form of proof. The downside of this classical approach of <em>existence proof</em> is that it is non-constructive, so even if you can derive a proof that some mathematical object exists, you can’t claim that you necessarily know what it is, since you have not concretely constructed such an object yet. Its existence is just “principally provable”, but not necessarily constructible or witnessable.</p>
<p>I would say it’s too much of a philosophical choice between classical logic and intuitionistic logic – at least for old school mathematicians who don’t practically mechanize their proofs at all. But one with some logic maturity should be able to draw a semantic distinction between a “proof by contradiction” that <span class="math inline">\(\vdash P\)</span> and a “proof of negation” that <span class="math inline">\(\vdash \lnot P\)</span>, bewaring of how their treatments can diverge in non-classical logic settings. It is still questionable to me whether every theorem provable in classical logic can be proved constructively, whatsoever, a constructive proof almost always makes more sense: <strong>If you claim that you have an apple, just show me the apple, instead of arguing to me sophistically that it can’t be the case that you do not have an apple.</strong></p>
<section id="references" class="level2">
<h2>References</h2>
<p>[1] Andrzej Filinski, “Course Notes for Semantics and Types, Lecture 1: Logic”.</p>
<p>[2] Robert Harper, “A ‘proof by contradiction’ is not a proof that ends with a contradiction”. <a href="https://existentialtype.wordpress.com/2017/03/04/a-proof-by-contradiction-is-not-a-proof-that-derives-a-contradiction/" class="uri">https://existentialtype.wordpress.com/2017/03/04/a-proof-by-contradiction-is-not-a-proof-that-derives-a-contradiction/</a></p>
<p>[3] Andrej Bauer, “Proof of negation and proof by contradiction”. <a href="http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/" class="uri">http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/</a></p>
<p>[4] Timothy Gowers, “When is proof by contradiction necessary?”. <a href="https://gowers.wordpress.com/2010/03/28/when-is-proof-by-contradiction-necessary/" class="uri">https://gowers.wordpress.com/2010/03/28/when-is-proof-by-contradiction-necessary/</a></p>
<p>[5] Terence Tao, “The ‘no self-defeating object’ argument”. <a href="https://terrytao.wordpress.com/2009/11/05/the-no-self-defeating-object-argument/" class="uri">https://terrytao.wordpress.com/2009/11/05/the-no-self-defeating-object-argument/</a></p>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>What I Wish I Knew When Learning Boolean Circuits</title>
    <link rel="alternate" type="text/html" href="https://blog.soimort.org/comp/c/boolean-circuit/" />
    <id>tag:www.soimort.org,2017:/posts/170306</id>
    <published>2017-03-06T00:00:00+01:00</published>
    <updated>2017-03-06T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="blog" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[


]]>
    </content>
  </entry>
  <entry>
    <title>The Fundamental Justification</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/5" />
    <id>tag:www.soimort.org,2017:/mst/5</id>
    <published>2017-02-06T00:00:00+01:00</published>
    <updated>2017-02-06T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>I don’t have much to write about this week. My plan was to finish the sections on statistics and machine learning in <a href="https://wiki.soimort.org/">my wiki</a> before I can move on to more rigorous mathematics and logic, but that turned out to be an impossible task (shortly after the exam week a new, incredibly tenser semester is right under the nose). I wanted to write more about cryptography and information theory as a brush-up account of previous courses I’ve taken, and that’s infeasible too (although the very introductory parts are done in <a href="/mst/3/">#3</a> and <a href="/mst/4/">#4</a>).</p>
<p>Still, I have thought of many fun things I could do with statistical learning: image classification and face recognition (I always want a photo management application to be like that! I take and collect so many photos each day), a CBIR tool (like “Search by Image” in Google Images, but works locally on my computer), a GIMP Script-Fu that does tricks like image synthesis via ConvNets, etc. The good part about applying learning methods in image processing is that, once you can extract feature descriptors, everything becomes statistically learnable (and you as a programmer don’t really need to have prior knowledge about what an object visually is: an apple, or a pen?).</p>
<p>Cryptography, from what I learned, is a differently fun story. For a perfectly secure encryption scheme: <span class="math display">\[\Pr[M=m\,|\,C=c] = \Pr[M=m]\]</span> that is, knowing the ciphertext <span class="math inline">\(c\)</span> will not update any attacker’s belief about whatever the underlying message <span class="math inline">\(m\)</span> is. Even if you have a statistical training model, it cannot learn anything from purely observations of ciphertexts. But this unbreakable level of security comes with a price: The key space must expose substantial entropy that is as high as the message space, thus the key length can be no shorter than the message length (given by Shannon’s theorem). In practice, the messages we sent do not usually have the highest entropy possible, and we can safely assume that the attackers’ computation ability is bounded by polynomial-time algorithms, thus, we as the cryptosystem designers need only to make up schemes that are assumed to be unbreakable (i.e., breakable with only a negligible probability) for any polynomial-time attackers. As we don’t know yet if there actually are any polynomial unsolvable cases (e.g., is P ≠ NP?), the proof of security would eventually rely on some unproven computational hardness assumptions: one-way functions exist, integer factorization is hard, discrete logarithm is hard, etc. If one can construct a provably secure scheme, it is guaranteed that statistical cryptanalysis would be theoretically impossible within polynomial time (except for side-channel attacks); of course, if the hardness assumption we made is proved invalid, then nothing but the one-time pad can be secure.</p>
<p>I might be writing one or two blog posts about cryptographic security from an information-theoretic perspective and some basic cryptanalysis on insecure schemes, but now is the time to move on with my unfinished courses about logic and programming. Before that, I feel that I should add a little bit <a href="https://wiki.soimort.org/philosophy/">philosophy</a> to my wiki so as to refresh my viewpoint and methodology. And here it is.</p>
<p>(Philosophy is a sophisticated, highly arguable subject, so pardon me if there’s any inconsistency with your textbook.)</p>
<ul>
<li><strong>Metaphysics</strong> is about the fundamental nature of being and the world. <strong>Ontology</strong>, as a branch of metaphysics, studies about the basic categories of being of entities and their relations.</li>
<li><a href="https://wiki.soimort.org/philosophy/epistemology/">Epistemology</a> is the study of knowledge.
<ul>
<li>Where does knowledge come form?
<ul>
<li><strong>Empiricism</strong> claims that knowledge comes from empirical evidence.</li>
<li><strong>Rationalism</strong> claims that knowledge requires justification by reasoning.</li>
<li><strong>Skepticism</strong> rejects the certainty in knowledge and claims that it is impossible to have an adequate justification of knowledge.</li>
</ul></li>
<li>How to resolve the <em>regress problem</em> that the justification of knowledge is questioned ad infinitum?
<ul>
<li>In <strong>foundationalism</strong>, a statement is inferred from a basis of unprovably sound premises.
<ul>
<li>This approach leads to the (axiomatic) foundations of mathematics and the constructivism.</li>
<li>The fundamentals of modern <em>mathematics</em> are the <em>axiomatic set theory</em> (which has several variations with different sets of axioms).</li>
<li>The fundamentals of modern <em>probability theory</em> are the <em>Kolmogorov axioms</em>.</li>
<li>The computational hardness assumptions in cryptography may also be seen as a practice of foundationalism.</li>
</ul></li>
<li>In <strong>coherentism</strong>, a statement holds true as long as it coheres with all other justified beliefs, including itself.</li>
<li>In <strong>infinitism</strong>, a justification chain is allowed to be infinite, thus there could never be adequate justification for any statement in the chain (which is the point that it is often criticized for).</li>
</ul></li>
<li>So, what is knowledge, actually?
<ul>
<li><strong>Knowledge</strong> is <em>justified true belief</em>. (though questioned by the <em>Gettier problem</em> <span class="citation" data-cites="gettier1963justified">[1]</span>)</li>
</ul></li>
<li>How do we categorize our knowledge?
<ul>
<li><strong>A priori</strong> knowledge is independent of empirical evidence. Examples: knowledge deduced by logical deductions or mathematical proofs.</li>
<li><strong>A posteriori</strong> knowledge is dependent of empirical evidence. Examples: knowledge induced by statistical inference or learning (either human or machine learning).</li>
</ul></li>
<li><strong>Science</strong> is the approach to filter out unreliable knowledge and gather together reliable knowledge as a <strong>theory</strong>.</li>
<li>What qualifies as a science?
<ul>
<li>See the <em>demarcation problem</em>.</li>
</ul></li>
<li>Scientific discovery is made of <strong>hypotheses</strong>. A hypothesis is a proposed explanation or prediction method for a phenomenon.
<ul>
<li>Only formally proved or statistically tested hypotheses qualify as reliable knowledge.</li>
<li><strong>Epicurus’ principle of multiple explanations</strong>: If multiple hypotheses are consistent with the observations, one should retain them all.</li>
<li><strong>Occam’s razor</strong>: Among all hypotheses consistent with the observations, choose the simplest.</li>
<li>Epicurus’ principle of multiple explanations and Occam’s razor find their uses in the learning theory, e.g., Occam’s razor bound.</li>
</ul></li>
</ul></li>
<li><a href="https://wiki.soimort.org/philosophy/logic/">Logic</a> is the study of reasoning and argument, which plays an essential part in gaining knowledge.
<ul>
<li>How to reason / argue by inference?
<ul>
<li><strong>Deduction</strong> is to infer a conclusion by deriving a logical consequence (“a priori”) from some premises using rules of inference in a formal system. Examples: proofs, a priori probabilities.</li>
<li><strong>Induction</strong> is to infer a probable conclusion (“a posteriori”) from the generalization of some observations. Examples: statistical inference and learning, inductive programming.</li>
<li><strong>Abduction</strong> is to infer a probable explanation from some observations. Examples: deep learning.</li>
</ul></li>
</ul></li>
</ul>
<p>We can talk about the philosophy of science (particularly, philosophy of mathematics and statistics) with the understanding of epistemology and logic: Are you a logician or a statistician? If a logician, does set theory or type theory suit you the best? If a statistician, are you a Bayesian or a frequentist?</p>
<p>(As a personally opinionated note, I often find myself subscribe to the skepticism the most. But that doesn’t mean that logical reasoning and statistical inference aren’t useful to me; they are. Extremely. So as not to go too far with this subjective topic, I’ll be focusing more on classical / modal logic in the next few weeks.)</p>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Papers:</strong></p>
<div id="refs" class="references">
<div id="ref-gettier1963justified">
<p>[1] E. L. Gettier, “Is justified true belief knowledge?” <em>analysis</em>, vol. 23, no. 6, pp. 121–123, 1963. </p>
</div>
</div>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>The Measurable Entropy</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/4" />
    <id>tag:www.soimort.org,2017:/mst/4</id>
    <published>2017-01-11T00:00:00+01:00</published>
    <updated>2017-01-14T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (quantitative properties about real-world data including central tendency, dispersion and shape). The maximization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known).</p>
<ul>
<li>What is the measure for <strong>information</strong>?
<ul>
<li>Intuitively, if a sample appears to be more naturally “random”, then it may contain more “information” of interest since it takes a greater size of data (more bits) to describe. But how to measure this quantitatively?</li>
<li>Probability-theoretic view: <em>Shannon entropy</em>.</li>
<li>Algorithmic view: <em>Kolmogorov complexity</em>. (TBD in February or March 2017)</li>
</ul></li>
<li><a href="https://wiki.soimort.org/info/">Basic information theory</a>
<ul>
<li><strong>Shannon entropy</strong>
<ul>
<li>For discrete random variable <span class="math inline">\(X\)</span> with pmf <span class="math inline">\(p(x)\)</span>: <span class="math display">\[\operatorname{H}(X) = -\sum_{x\in\mathcal{X}} p(x) \log p(x).\]</span></li>
<li>For continuous random variable <span class="math inline">\(X\)</span> with pdf <span class="math inline">\(f(x)\)</span>: <span class="math display">\[\operatorname{H}(X) = -\int_\mathcal{X} f(x) \log f(x) dx.\]</span> (also referred to as <em>differential entropy</em>)</li>
<li>The notion of entropy is an extension of the one in statistical thermodynamics (Gibbs entropy) and the <a href="https://wiki.soimort.org/math/dynamical-systems/ergodic/">measure-theoretic entropy of dynamical systems</a>.</li>
<li>Obviously, the entropy is determined by the pmf/pdf, which depends on the parameters of the specific probability distribution.</li>
<li>In the context of Computer Science, the logarithm in the formula is often taken to the base <span class="math inline">\(2\)</span>. Assume that we take a uniform binary string of length <span class="math inline">\(\ell\)</span>, then <span class="math display">\[p(x) = 2^{-\ell}\]</span> Thus, the entropy of the distribution is <span class="math display">\[\operatorname{H}(X) = -\sum_{x\in\mathcal{X}} p(x) \log p(x) = - (2^\ell \cdot 2^{-\ell} \log_2 2^{-\ell}) = \ell\]</span> which is just the length of this (<span class="math inline">\(\ell\)</span>-bit) binary string. Therefore, the unit of information (when applying binary logarithm) is often called a <em>bit</em> (also <em>shannon</em>).</li>
<li>For the <strong>joint entropy</strong> <span class="math inline">\(\operatorname{H}(X,Y)\)</span> and the <strong>conditional entropy</strong> <span class="math inline">\(\operatorname{H}(X\,|\,Y)\)</span>, the following equation holds: <span class="math display">\[\operatorname{H}(X,Y) = \operatorname{H}(X\,|\,Y) + \operatorname{H}(Y) = \operatorname{H}(Y\,|\,X) + \operatorname{H}(X)\]</span> Notice that if <span class="math inline">\(\operatorname{H}(X\,|\,Y) = \operatorname{H}(X)\)</span>, then <span class="math inline">\(\operatorname{H}(X,Y) = \operatorname{H}(X) + \operatorname{H}(Y)\)</span> and <span class="math inline">\(\operatorname{H}(Y\,|\,X) = \operatorname{H}(Y)\)</span>. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be independent of each other in this case.</li>
<li><strong>Mutual information</strong> <span class="math inline">\(\operatorname{I}(X;Y) = \operatorname{H}(X) + \operatorname{H}(Y) - \operatorname{H}(X,Y) \geq 0\)</span> (equality holds iff <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent). Unlike the joint entropy or the conditional entropy, this notion does not reflect an actual probabilistic event thus it is referred to as <em>information</em> (sometimes <em>correlation</em>) rather than entropy.</li>
</ul></li>
<li><strong>Kullback-Leibler divergence (relative entropy)</strong> <span class="math display">\[\operatorname{D}_\mathrm{KL}(p\|q) = \sum_{x\in\mathcal{X}} p(x) \log \frac{p(x)}{q(x)}\]</span> KL divergence is a measurement of the distance of two probability distributions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>.
<ul>
<li>If <span class="math inline">\(p = q\)</span>, <span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) = 0\)</span>. (Any distribution has a KL divergence of 0 with itself.)</li>
<li><span class="math inline">\(\operatorname{I}(X;Y) = \operatorname{D}_\mathrm{KL}(p(x,y)\|p(x)p(y))\)</span>.</li>
<li><strong>Cross entropy</strong> <span class="math display">\[\operatorname{H}(p,q) = \operatorname{H}(p) + \operatorname{D}_\mathrm{KL}(p\|q)\]</span> Notice that cross entropy is defined on two distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> rather than two random variables taking one distribution <span class="math inline">\(p\)</span> (given by joint entropy <span class="math inline">\(\operatorname{H}(X,Y)\)</span>).</li>
</ul></li>
</ul></li>
<li>Basic probability theory
<ul>
<li><a href="https://wiki.soimort.org/math/probability/distributions/normal/">Normal (Gaussian) distribution</a>.
<ul>
<li>(Univariate) <span class="math inline">\(X \sim \mathcal{N}(\mu,\sigma^2)\)</span> <span class="math display">\[f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span> where <span class="math inline">\(\mu\)</span> is the mean, <span class="math inline">\(\sigma^2\)</span> is the variance of the distribution.<br />
(Multivariate) <span class="math inline">\(\boldsymbol x \sim \mathcal{N}_k(\boldsymbol\mu,\mathbf\Sigma)\)</span> <span class="math display">\[f(\boldsymbol x) = (2\pi)^{-k/2} |\mathbf\Sigma|^{-1/2} e^{-\frac{1}{2} (\boldsymbol x - \boldsymbol\mu)^\mathrm{T} \Sigma^{-1}(\boldsymbol x - \boldsymbol\mu)}\]</span> where <span class="math inline">\(\boldsymbol\mu\)</span> is the mean vector, <span class="math inline">\(\mathbf\Sigma\)</span> is the covariance matrix of the distribution.</li>
<li><em>Maximum entropy</em>: normal distribution is the probability distribution that maximizes the entropy when the mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span> are fixed.</li>
<li>The normal distribution does not have any shape parameter. Moreover, its skewness and excess kurtosis are always 0.</li>
</ul></li>
</ul></li>
<li><a href="https://wiki.soimort.org/math/statistics/">Basic descriptive statistics</a>
<ul>
<li><em>Descriptive statistics</em> describe the properties of data sets quantitatively, without making any further inference.</li>
<li><em>Population</em> vs. <em>sample</em>.</li>
<li>Three major descriptive statistics:
<ol type="1">
<li><em>Central tendency</em>: sample means (<strong>arithmetic mean</strong> <span class="math inline">\(\mu\)</span>, geometric, harmonic), <strong>median</strong>, <strong>mode</strong>, mid-range.
<ul>
<li>Arithmetic mean is an unbiased estimator of the population mean (expectation).</li>
<li>Median and mode are most robust in the presence of outliers.</li>
</ul></li>
<li><em>Dispersion (or variability)</em>: minimum, maximum, range, IQR (interquartile range), maximum absolute deviation, MAD (mean absolute deviation), <strong>sample variance</strong> <span class="math inline">\(s^2\)</span> with Bessel’s correction, <strong>CV (coefficient of variance)</strong>, <strong>VMR (index of dispersion)</strong>.
<ul>
<li>IQR and MAD are robust in the presence of outliers.</li>
<li>Sample variance (with Bessel’s correction) is an unbiased estimator of the population variance.</li>
<li>CV and VMR are sample standard deviation and sample variance normalized by the mean respectively, thus they are sometimes called <em>relative standard deviation</em> and <em>relative variance</em>; they are <em>not</em> unbiased though.</li>
</ul></li>
<li><em>Shape</em>: sample skewness, sample excess kurtosis.
<ul>
<li>These statistics show how a sample deviates from normality, since the skewness and the excess kurtosis of a normal distribution are 0. The estimators could vary under different circumstances.</li>
</ul></li>
</ol></li>
</ul></li>
</ul>
<section id="entropy-as-a-measure" class="level2">
<h2>Entropy as a measure<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></h2>
<p>For random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we define sets <span class="math inline">\(\tilde X\)</span> and <span class="math inline">\(\tilde Y\)</span>. Then the information entropy <span class="math inline">\(\operatorname{H}\)</span> may be viewed as a signed measure <span class="math inline">\(\mu\)</span> over <a href="https://wiki.soimort.org/math/set/">sets</a>: <span class="math display">\[\begin{align*}
\operatorname{H}(X) &amp;= \mu(\tilde X) \\
\operatorname{H}(Y) &amp;= \mu(\tilde Y) \\
\operatorname{H}(X,Y) &amp;= \mu(\tilde X \cup \tilde Y) \qquad \text{(Joint entropy is the measure of a set union)} \\
\operatorname{H}(X\,|\,Y) &amp;= \mu(\tilde X \setminus \tilde Y) \qquad \text{(Conditional entropy is the measure of a set difference)} \\
\operatorname{I}(X;Y) &amp;= \mu(\tilde X \cap \tilde Y) \qquad \text{(Mutual information is the measure of a set intersection)}
\end{align*}\]</span> The inclusion–exclusion principle: <span class="math display">\[\begin{align*}
\operatorname{H}(X,Y) &amp;= \operatorname{H}(X) + \operatorname{H}(Y) - \operatorname{I}(X;Y) \\
\mu(\tilde X \cup \tilde Y) &amp;= \mu(\tilde X) + \mu(\tilde Y) - \mu(\tilde X \cap \tilde Y)
\end{align*}\]</span> Bayes’ theorem: <span class="math display">\[\begin{align*}
\operatorname{H}(X\,|\,Y) &amp;= \operatorname{H}(Y\,|\,X) + \operatorname{H}(X) - \operatorname{H}(Y) \\
\mu(\tilde X \setminus \tilde Y) &amp;= \mu(\tilde Y \setminus \tilde X) + \mu(\tilde X) - \mu(\tilde Y)
\end{align*}\]</span></p>
</section>
<section id="entropy-and-data-coding" class="level2">
<h2>Entropy and data coding</h2>
<p><em>Absolute entropy (Shannon entropy)</em> quantifies how much information is contained in some data. For data compression, the entropy gives the minimum size that is needed to reconstruct original data (losslessly). Assume that we want to store a random binary string of length <span class="math inline">\(\ell\)</span> (by “random”, we do not have yet any prior knowledge on what data to be stored). Under the <em>principle of maximum entropy</em>, the entropy of its distribution <span class="math inline">\(p(x)\)</span> should be maximized: <span class="math display">\[\max \operatorname{H}(X) = \max \left\{ -\sum_{x\in\mathcal{X}} p(x) \log p(x) \right\}\]</span> given the only constraint <span class="math display">\[\sum_{x\in\mathcal{X}} p(x) = 1\]</span> Let <span class="math inline">\(\lambda\)</span> be the Lagrange multiplier, set <span class="math display">\[\mathcal{L} = - \sum_{x\in\mathcal{X}} p(x) \log p(x) - \lambda\left( \sum_{x\in\mathcal{X}} p(x) - 1 \right)\]</span> We get <span class="math display">\[\begin{align*}
\frac{\partial\mathcal{L}}{\partial x} = -p(x)(\log p(x) + 1 + \lambda) &amp;= 0 \\
\log p(x) &amp;= - \lambda - 1 \\
p(x) &amp;= c \qquad \text{(constant)}
\end{align*}\]</span> That is, the <a href="https://wiki.soimort.org/math/probability/#discrete-uniform-distribution">discrete uniform distribution</a> maximizes the entropy for a random string. Since <span class="math inline">\(|\mathcal{X}| = 2^\ell\)</span>, we have <span class="math inline">\(p(x) = 2^{-\ell}\)</span> and <span class="math inline">\(\operatorname{H}(X) = -\sum_{x\in\mathcal{X}} 2^{-\ell} \log_2 2^{-\ell} = \ell\)</span> (bits). We conclude that the information that can be represented in a <span class="math inline">\(\ell\)</span>-bit string is at most <span class="math inline">\(\ell\)</span> bits. Some practical results include</p>
<ul>
<li>In general, pseudorandom data (assume no prior knowledge) cannot be losslessly compressed, e.g., the uniform key used in one-time pad must have <span class="math inline">\(\log_2 |\mathcal{M}|\)</span> bits (lower bound) so as not to compromise the perfect secrecy. (Further topic: <em>Shannon’s source coding theorem</em>)</li>
<li>Fully correct encoding/decoding of data, e.g., <span class="math inline">\(\mathsf{Enc}(m)\)</span> and <span class="math inline">\(\mathsf{Dec}(c)\)</span> algorithms in a private-key encryption scheme, must ensure that the probability distributions of <span class="math inline">\(m \in \mathcal{M}\)</span> and <span class="math inline">\(c \in \mathcal{C}\)</span> have the same entropy.</li>
<li>An algorithm with finite input cannot generate randomness infinitely. Consider a circuit that takes the encoded algorithm with some input (<span class="math inline">\(\ell\)</span> bits in total) and outputs some randomness, the entropy of the output data is at most <span class="math inline">\(\ell\)</span> bits. (Further topic: <em>Kolmogorov complexity</em>)</li>
</ul>
<p><em>Relative entropy (KL divergence)</em> quantifies how much information diverges between two sets of data. For data differencing, the KL divergence gives the minimum patch size that is needed to reconstruct target data (with distribution <span class="math inline">\(p(x)\)</span>) given source data (with distribution <span class="math inline">\(q(x)\)</span>).</p>
<p>In particular, if <span class="math inline">\(p(x) = q(x)\)</span>, which means that the two distributions are identical, we have <span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) = 0\)</span>. This follows our intuition that no information is gained or lost during data encoding/decoding. If <span class="math inline">\(p(x_0) = 0\)</span> at <span class="math inline">\(x=x_0\)</span>, we take <span class="math inline">\(p(x) \log \frac{p(x)}{q(x)} = 0\)</span>, to justify the fact that the target data is trivial to reconstruct at this point, no matter how much information <span class="math inline">\(q(x)\)</span> contains. However, if <span class="math inline">\(q(x_0) = 0\)</span> at <span class="math inline">\(x=x_0\)</span>, we should take <span class="math inline">\(p(x) \log \frac{p(x)}{q(x)} = \infty\)</span>, so that the target data is impossible to reconstruct if we have only trivial <span class="math inline">\(q(x)\)</span> at some point (unless <span class="math inline">\(p(x_0) = 0\)</span>).</p>
<p><strong>Lemma 4.1. (Gibbs’ inequality)</strong><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> <em>The KL divergence is always non-negative: <span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) \geq 0\)</span>.</em></p>
<p>Informally, Lemma 4.1 simply states that in order to reconstruct target data from source data, either more information (<span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) &gt; 0\)</span>) or no further information (<span class="math inline">\(\operatorname{D}_\mathrm{KL}(p\|q) = 0\)</span>) is needed.</p>
</section>
<section id="maximum-entropy-and-normality" class="level2">
<h2>Maximum entropy and normality</h2>
<p><strong>Theorem 4.2.</strong> <em>Normal distribution <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span> maximizes the differential entropy for given mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</em></p>
<p><strong>Proof.</strong><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Let <span class="math inline">\(g(x)\)</span> be a pdf of the normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(f(x)\)</span> be an arbitrary pdf with the same mean and variance.</p>
<p>Consider the KL divergence between <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span>. By Lemma 4.1 (Gibbs’ inequality): <span class="math display">\[\operatorname{D}_\mathrm{KL}(f\|g) = \int_{-\infty}^\infty f(x) \log \frac{f(x)}{g(x)} dx = \operatorname{H}(f,g) - \operatorname{H}(f) \geq 0\]</span></p>
<p>Notice that <span class="math display">\[\begin{align*}
\operatorname{H}(f,g) &amp;= - \int_{-\infty}^\infty f(x) \log g(x) dx \\
&amp;= - \int_{-\infty}^\infty f(x) \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \right) dx \\
&amp;= \frac{1}{2} \left( \log(2\pi\sigma^2) + 1 \right) \\
&amp;= \operatorname{H}(g)
\end{align*}\]</span> Therefore, <span class="math display">\[\operatorname{H}(g) \geq \operatorname{H}(f)\]</span> That is, the distribution of <span class="math inline">\(g(x)\)</span> (Gaussian) always has the maximum entropy. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>It is also possible to derive the normal distribution directly from the principle of maximum entropy, under the constraint such that <span class="math inline">\(\int_{-\infty}^\infty (x-\mu)^2f(x)dx = \sigma^2\)</span>.</p>
<p>The well-known central limit theorem (CLT) which states that the sum of independent random variables <span class="math inline">\(\{X_1,\dots,X_n\}\)</span> tends toward a normal distribution may be alternatively expressed as the monotonicity of the entropy of the normalized sum: <span class="math display">\[\operatorname{H}\left( \frac{\sum_{i=1}^n X_i}{\sqrt{n}} \right)\]</span> which is an increasing function of <span class="math inline">\(n\)</span>. <span class="citation" data-cites="artstein2004solution">[1]</span></p>
</section>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Books:</strong></p>
<p>T. M. Cover and J. A. Thomas. <em>Elements of Information Theory</em>, 2nd ed.</p>
<p><strong>Papers:</strong></p>
<div id="refs" class="references">
<div id="ref-artstein2004solution">
<p>[1] S. Artstein, K. Ball, F. Barthe, and A. Naor, “Solution of shannon’s problem on the monotonicity of entropy,” <em>Journal of the American Mathematical Society</em>, vol. 17, no. 4, pp. 975–982, 2004. </p>
</div>
</div>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Information_theory_and_measure_theory" class="uri">https://en.wikipedia.org/wiki/Information_theory_and_measure_theory</a><a href="#fnref1" class="footnoteBack">↩</a></p></li>
<li id="fn2"><p><a href="https://en.wikipedia.org/wiki/Gibbs&#39;_inequality" class="uri">https://en.wikipedia.org/wiki/Gibbs'_inequality</a><a href="#fnref2" class="footnoteBack">↩</a></p></li>
<li id="fn3"><p><a href="https://en.wikipedia.org/wiki/Differential_entropy#Maximization_in_the_normal_distribution" class="uri">https://en.wikipedia.org/wiki/Differential_entropy#Maximization_in_the_normal_distribution</a><a href="#fnref3" class="footnoteBack">↩</a></p></li>
</ol>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>The Adversarial Computation</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/3" />
    <id>tag:www.soimort.org,2017:/mst/3</id>
    <published>2017-01-01T00:00:00+01:00</published>
    <updated>2017-01-01T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p><a href="https://wiki.soimort.org/comp/">Theory of computation</a> (computability and complexity) forms the basis for modern cryptography:</p>
<ul>
<li>What is an <a href="https://wiki.soimort.org/comp/algorithm/">algorithm</a>?
<ul>
<li>An algorithm is a <em>computational method</em> for solving an <em>abstract problem</em>.</li>
<li>An algorithm takes as input a set <span class="math inline">\(I\)</span> of <em>problem instances</em>, and outputs a solution from the set <span class="math inline">\(S\)</span> of <em>problem solutions</em>.</li>
<li>An algorithm is represented in a well-defined <a href="https://wiki.soimort.org/comp/language/">formal language</a>.</li>
<li>An algorithm must be able to be represented within a finite amount of time and space (otherwise it cannot be actually used for solving any problem).</li>
<li>An algorithm can be simulated by any <em>model of computation</em>:
<ul>
<li><em>Turing machine</em> is a model implemented through internal <em>states</em>.</li>
<li><em>λ-calculus</em> is a model based on pure <em>functions</em>.</li>
<li>All Turing-complete models are equivalent in their computational abilities.</li>
</ul></li>
<li>Computability: Not every abstract problem is solvable. Notably, there exists a decision problem for which some instances can neither be accepted nor rejected by any algorithm. (<em>Undecidable problem</em>)</li>
<li>Complexity:
<ul>
<li>The complexity class P is closed under polynomial-time reductions. Hence, proof by reduction can be a useful technique in provable security of cryptosystems.</li>
<li>If one can prove that P = NP, then one-way functions do not exist. This would invalidate the construction of cryptographically secure pseudorandom generators (PRG). (<em>Pseudorandom generator theorem</em>)</li>
</ul></li>
<li>In many scenarios, we assume that an algorithm acts as a stateless computation and takes independent and identically distributed inputs. It differs from a computer program conceptually.</li>
<li>An algorithm can be either <em>deterministic</em> or <em>probabilistic</em>.
<ul>
<li>For probabilistic algorithms, the source of randomness may be from:
<ul>
<li>External (physical) input of high entropy.</li>
<li>Pseudorandomness: Since everything computational is deterministic, the existence of pseudorandomness relies on the (assumed) existence of one-way functions and PRGs.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>Generally, pseudorandom generators used in probabilistic algorithms yield random bits according to the uniform distribution, so it is worth mentioning:</p>
<ul>
<li>Basic probability theory
<ul>
<li><a href="https://wiki.soimort.org/math/probability/#discrete-uniform-distribution">Discrete uniform distribution</a>. <span class="math inline">\(\mathcal{U}\{a,b\}\)</span>. The probability distribution where a finite number of values are equally likely to be observed (with probability <span class="math inline">\(\frac{1}{b-a+1}\)</span>).</li>
</ul></li>
</ul>
<p>Cryptographic schemes are defined as tuples of deterministic or probabilistic algorithms:</p>
<ul>
<li><a href="https://wiki.soimort.org/crypto/intro/">Principles of modern cryptography</a>
<ul>
<li>Formal description of a <em>private-key encryption scheme</em> <span class="math inline">\(\Pi=(\mathsf{Gen},\mathsf{Enc},\mathsf{Dec})\)</span> with message space <span class="math inline">\(\mathcal{M}\)</span>.
<ul>
<li><span class="math inline">\(\mathsf{Gen}\)</span>, <span class="math inline">\(\mathsf{Enc}\)</span>, <span class="math inline">\(\mathsf{Dec}\)</span> are three algorithms.</li>
<li>Correctness: <span class="math inline">\(\mathsf{Dec}_k(\mathsf{Enc}_k(m)) = m\)</span>.</li>
<li>For the correctness equality to hold, <span class="math inline">\(\mathsf{Dec}\)</span> should be deterministic.</li>
<li>Assume that we have access to a source of randomness, <span class="math inline">\(\mathsf{Gen}\)</span> should choose a key at random thus is probabilistic. If <span class="math inline">\(\mathsf{Gen}\)</span> is deterministic and always generate the same key, such an encryption scheme is of no practical use and easy to break.</li>
<li><span class="math inline">\(\mathsf{Enc}\)</span> can be either deterministic (e.g., as in one-time pads) or probabilistic. Later we will see that for an encryption scheme to be CPA-secure, <span class="math inline">\(\mathsf{Enc}\)</span> should be probabilistic.</li>
</ul></li>
<li><strong>Kerchhoffs’ principle (Shannon’s maxim)</strong> claims that a cryptosystem should be secure even if the scheme <span class="math inline">\((\mathsf{Gen},\mathsf{Enc},\mathsf{Dec})\)</span> is known to the adversary. That is, security should rely solely on the secrecy of the private key.</li>
<li>Provable security of cryptosystems requires:
<ol type="1">
<li>Formal definition of security;</li>
<li>Minimal assumptions;</li>
<li>Rigorous proofs of security.</li>
</ol></li>
<li>Common attacks and notions of security:
<ul>
<li><strong>Ciphertext-only attack</strong>.
<ul>
<li>A cryptosystem is said to be <em>perfectly secret</em> if it is theoretically unbreakable under ciphertext-only attack.</li>
<li>A cryptosystem is said to be <em>computationally secure</em> if it is resistant to ciphertext-only attack (by any polynomial-time adversary).</li>
</ul></li>
<li><strong>Known-plaintext attack (KPA)</strong>. A cryptosystem is <em>KPA-secure</em> if it is resistant to KPA.
<ul>
<li>KPA-security implies ciphertext-only security.</li>
</ul></li>
<li><strong>Chosen-plaintext attack (CPA)</strong>. A cryptosystem is <em>CPA-secure</em> (or <em>IND-CPA</em>) if it is resistant to CPA.
<ul>
<li>IND-CPA implies KPA-security.</li>
</ul></li>
<li><strong>Chosen-ciphertext attack (CCA)</strong>. A cryptosystem is <em>CCA-secure</em> (or <em>IND-CCA1</em>) if it is resistant to CCA; furthermore, a cryptosystem is <em>IND-CCA2</em> if it is resistant to adaptive CCA (where the adversary may make further calls to the oracle, but may not submit the challenge ciphertext).
<ul>
<li>IND-CCA1 implies IND-CPA.</li>
<li>IND-CCA2 implies IND-CCA1. Thus, IND-CCA2 is the strongest of above mentioned definitions of security.</li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="https://wiki.soimort.org/crypto/perfect-secrecy/">Perfect secrecy</a>
<ul>
<li>Two equivalent definitions: (proof of equivalence uses Bayes’ theorem)
<ul>
<li><span class="math inline">\(\Pr[M=m\,|\,C=c] = \Pr[M=m]\)</span>. (Observing a ciphertext <span class="math inline">\(c\)</span> does not leak any information about the underlying message <span class="math inline">\(m\)</span>)</li>
<li><span class="math inline">\(\Pr[\mathsf{Enc}_K(m)=c] = \Pr[\mathsf{Enc}_K(m&#39;)=c]\)</span>. (The adversary has no bias when distinguishing two messages if given only the ciphertext <span class="math inline">\(c\)</span>)</li>
</ul></li>
<li><strong>Perfect indistinguishability</strong> defined on adversarial indistinguishability experiment <span class="math inline">\(\mathsf{PrivK}_{\mathcal{A},\Pi}^\mathsf{eav}\)</span>:
<ul>
<li><span class="math inline">\(\Pr[\mathsf{PrivK}_{\mathcal{A},\Pi}^\mathsf{eav} = 1] = \frac{1}{2}\)</span>. (No adversary can win the indistinguishability game with a probability better than random guessing)</li>
</ul></li>
<li>Perfect indistinguishability is equivalent to the definition of perfect secrecy.</li>
<li>The adversarial indistinguishability experiment is a very useful setting in defining provable security, e.g., the definition of computational indistinguishability: (for arbitrary input size <span class="math inline">\(n\)</span>)
<ul>
<li><span class="math inline">\(\Pr[\mathsf{PrivK}_{\mathcal{A},\Pi}^\mathsf{eav}(n) = 1] \leq \frac{1}{2} + \mathsf{negl}(n)\)</span> where <span class="math inline">\(\mathsf{negl}(n)\)</span> is a negligible function.</li>
</ul></li>
<li>Perfect secrecy implies that <span class="math inline">\(|\mathcal{K}| \geq |\mathcal{M}|\)</span>, i.e., the key space must be larger than the message space. If <span class="math inline">\(|\mathcal{K}| &lt; |\mathcal{M}|\)</span>, then the scheme cannot be perfectly secure.</li>
<li><strong>Shannon’s theorem</strong>: If <span class="math inline">\(|\mathcal{K}| = |\mathcal{M}| = |\mathcal{C}|\)</span>, an encryption scheme is perfectly secret iff:
<ul>
<li><span class="math inline">\(k \in \mathcal{K}\)</span> is chosen uniformly.</li>
<li>For every <span class="math inline">\(m \in \mathcal{M}\)</span> and <span class="math inline">\(c \in \mathcal{C}\)</span>, there exists a unique <span class="math inline">\(k \in \mathcal{K}\)</span> such that <span class="math inline">\(\mathsf{Enc}_k(m) = c\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<p>A brief, formalized overview of some classical ciphers, and their security:</p>
<ul>
<li><a href="https://wiki.soimort.org/crypto/one-time-pad/">One-time pad (Vernam cipher)</a>: XOR cipher when <span class="math inline">\(|\mathcal{K}| = |\mathcal{M}|\)</span>.
<ul>
<li>One-time pad is perfectly secret. The proof simply follows from Bayes’ theorem. (Also verified by Shannon’s theorem. While one-time pad was initially introduced in the 19th century and patented by G. Vernam in 1919, it was not until many years later Claude Shannon gave a formal definition of information-theoretical security and proved that one-time pad is a perfectly secret scheme in his groundbreaking paper. <a href="#ref-shannon1949communication"><span class="citation" data-cites="shannon1949communication">[1]</span></a>)</li>
<li>One-time pad is deterministic. Moreover, it is a reciprocal cipher (<span class="math inline">\(\mathsf{Enc} = \mathsf{Dec}\)</span>).</li>
<li>One-time pad is <em>not</em> secure when the same key is applied in multiple encryptions, and it is <em>not</em> CPA-secure. In fact, an adversary can succeed in such indistinguishability experiments with probability 1.</li>
</ul></li>
<li>Insecure historical ciphers:
<ul>
<li><a href="https://wiki.soimort.org/crypto/classical/shift/">Shift cipher</a>: Defined with key space <span class="math inline">\(\mathcal{K}=\{0,\dots,n-1\}\)</span>. (<span class="math inline">\(n=|\Sigma|\)</span>)
<ul>
<li><span class="math inline">\(|\mathcal{K}|=n\)</span>, <span class="math inline">\(|\mathcal{M}|=n^\ell\)</span>.</li>
<li>Cryptanalysis using frequency analysis.</li>
</ul></li>
<li><a href="https://wiki.soimort.org/crypto/classical/substitution/">Substitution cipher</a>: Defined with key space <span class="math inline">\(\mathcal{K} = \mathfrak{S}_\Sigma\)</span> (symmetric group on <span class="math inline">\(\Sigma\)</span>).
<ul>
<li><span class="math inline">\(|\mathcal{K}|=n!\)</span>, <span class="math inline">\(|\mathcal{M}|=n^\ell\)</span>.</li>
<li>Cryptanalysis using frequency analysis.</li>
</ul></li>
<li><a href="https://wiki.soimort.org/crypto/classical/vigenere/">Vigenère cipher (poly-alphabetic shift cipher)</a>: Like (mono-alphabetic) shift cipher, but the key length is an (unknown) integer <span class="math inline">\(t\)</span>.
<ul>
<li><span class="math inline">\(|\mathcal{K}|=n^t\)</span>, <span class="math inline">\(|\mathcal{M}|=n^\ell\)</span>. (Typically <span class="math inline">\(t \ll \ell\)</span>)</li>
<li>Cryptanalysis using Kasiski’s method, index of coincidence method and frequency analysis.</li>
</ul></li>
</ul></li>
</ul>
<p>Lessons learned from these classical ciphers: While perfect secrecy is easy to achieve (one-time pads), designing practical cryptographic schemes (with shorter keys, and computationally hard to break) can be difficult.</p>
<section id="where-do-random-bits-come-from" class="level2">
<h2>Where do random bits come from?</h2>
<p>The construction of private-key encryption schemes involves probabilistic algorithms. We simply assume that an unlimited supply of independent, unbiased random bits is available for these cryptographic algorithms. But in practice, this is a non-trivial issue, as the source of randomness must provide high-entropy data so as to accommodate cryptographically secure random bits.</p>
<p>In the perfectly secret scheme of one-time pads, the key generation algorithm <span class="math inline">\(\mathsf{Gen}\)</span> requires the access to a source of randomness in order to choose the uniformly random key <span class="math inline">\(k \in \mathcal{K}\)</span>. Practically, high-entropy data may be collected via physical input or even fully written by hand with human labor.</p>
<p>Theoretically, without external intervention, we have:</p>
<p><strong>Conjecture 3.1.</strong> <em>Pseudorandom generators exist.</em></p>
<p><strong>Theorem 3.2. (Pseudorandom generator theorem)</strong> <em>Pseudorandom generators exist if and only if one-way functions exist.</em></p>
<p>Pseudorandomness is also a basic construction in CPA-secure encryption algorithms (<span class="math inline">\(\mathsf{Enc}\)</span>), e.g., in stream ciphers and block ciphers.</p>
<p>So what is an acceptable level of pseudorandomness, if we are not sure whether such generators theoretically exist? Intuitively, if one cannot distinguish between a “pseudorandom” string (generated by a PRG) and a truly random string (chosen according to the uniform distribution), we have confidence that the PRG is a good one. Various statistical tests have been designed for testing the randomness of PRGs.</p>
</section>
<section id="pseudorandomness-and-ind-cpa" class="level2">
<h2>Pseudorandomness and IND-CPA</h2>
<p>It holds true that:</p>
<p><strong>Corollary 3.3.</strong> By redefining the key space, we can assume that any encryption scheme <span class="math inline">\(\Pi=(\mathsf{Gen},\mathsf{Enc},\mathsf{Dec})\)</span> satisfies</p>
<ol type="1">
<li><span class="math inline">\(\mathsf{Gen}\)</span> chooses a uniform key.</li>
<li><span class="math inline">\(\mathsf{Enc}\)</span> is deterministic.</li>
</ol>
<p>If so, why do we still need probabilistic <span class="math inline">\(\mathsf{Enc}\)</span> in CPA-secure encryptions? Can’t we just make <span class="math inline">\(\mathsf{Enc}\)</span> deterministic while still being CPA-secure?</p>
<p>The first thing to realize is that chosen-plaintext attacks are geared towards multiple encryptions (with the same secret key <span class="math inline">\(k\)</span>), so when the adversary obtains a pair <span class="math inline">\((m_0, c_0)\)</span> such that <span class="math inline">\(\Pr[C=c_0\,|\,M=m_0] = 1\)</span>, <em>the key is already leaked</em>. (Recall that the adversary knows the <em>deterministic</em> algorithm <span class="math inline">\(\mathsf{Enc}_k\)</span>, thus reversing <span class="math inline">\(k\)</span> from known <span class="math inline">\(m_0\)</span> and <span class="math inline">\(c_0\)</span> can be quite feasible; e.g., in a one-time pad, <span class="math inline">\(k = m_0 \oplus c_0\)</span>.) The only way to get around this is make <span class="math inline">\(\mathsf{Enc}_k\)</span> <em>probabilistic</em> (constructed from a <em>pseudorandom function</em>), such that an adversary cannot reverse the key efficiently within polynomial time.</p>
<p>Note that perfect secrecy is not possible under CPA, since there is a small possibility that the adversary will reverse the key (by, for example, traversing an exponentially large lookup table of all random bits) and succeed in the further indistinguishability experiment with a slightly higher (but negligible) probability.</p>
</section>
<section id="historical-exploits-of-many-time-pad" class="level2">
<h2>Historical exploits of many-time pad</h2>
<p>One-time pad is one of the most (provably) secure encryption schemes, and its secrecy does not rely on any computational hardness assumptions. However, it requires that <span class="math inline">\(|\mathcal{K}| \geq |\mathcal{M}|\)</span> (which in fact is a necessary condition for any perfectly secret scheme), thus its real-world use is limited.</p>
<p>The one-time key <span class="math inline">\(k\)</span> (uniformly chosen from the key space <span class="math inline">\(\mathcal{K}\)</span>) may <em>not</em> be simply reused in multiple encryptions. Assume that <span class="math inline">\(|\mathcal{K}| = |\mathcal{M}|\)</span>, for encryptions of <span class="math inline">\(n\)</span> messages, the message space is expanded to size <span class="math inline">\(|\mathcal{M}|^n\)</span>, while the key space remains <span class="math inline">\(\mathcal{K}\)</span>, thus we have <span class="math inline">\(|\mathcal{K}| &lt; |\mathcal{M}|^n\)</span>. Such a degraded scheme (many-time pad) is theoretically insecure and vulnerable to several practical cryptanalyses.</p>
<p>A historical exploit of the vulnerability of many-time pad occurred in the VENONA project, where the U.S. Army’s Signal Intelligence Service (later the NSA) aimed at decrypting messages sent by the USSR intelligence agencies (KGB) over a span of 4 decades. As the KGB mistakenly reused some portions of their one-time key codebook, the SIS was able to break a good amount of the messages.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
</section>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Books:</strong></p>
<p>J. Katz and Y. Lindell, <em>Introduction to Modern Cryptography</em>, 2nd ed.</p>
<p><strong>Papers:</strong></p>
<div id="refs" class="references">
<div id="ref-shannon1949communication">
<p>[1] C. E. Shannon, “Communication theory of secrecy systems,” <em>Bell system technical journal</em>, vol. 28, no. 4, pp. 656–715, 1949. </p>
</div>
</div>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>R. L. Benson, “The Venona story.” <a href="https://www.nsa.gov/about/cryptologic-heritage/historical-figures-publications/publications/coldwar/assets/files/venona_story.pdf" class="uri">https://www.nsa.gov/about/cryptologic-heritage/historical-figures-publications/publications/coldwar/assets/files/venona_story.pdf</a><a href="#fnref1" class="footnoteBack">↩</a></p></li>
</ol>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>The Probable Outcome</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/2" />
    <id>tag:www.soimort.org,2017:/mst/2</id>
    <published>2016-12-23T00:00:00+01:00</published>
    <updated>2016-12-23T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>A refresher of basic probability theory, which is just common knowledge but plays a supporting role in information theory, statistical methods, and consequently, computer science.</p>
<ul>
<li><a href="https://wiki.soimort.org/math/probability/">Basic probability theory</a>
<ul>
<li>An <strong>experiment</strong> has various <strong>outcomes</strong>. The set of all probable outcomes constitute the <strong>sample space</strong> of that experiment.</li>
<li>Any <em>measurable</em> subset of the sample space <span class="math inline">\(\Omega\)</span> is known as an <strong>event</strong>.</li>
<li>A <strong>probability measure</strong> is a real-valued function defined on a set of events <span class="math inline">\(\mathcal{F}\)</span> in a probability space <span class="math inline">\((\Omega,\mathcal{F},\Pr)\)</span> that satisfies measure properties such as countable additivity. (See <strong>Kolmogorov’s axioms</strong>.)</li>
<li>The <strong>union bound</strong> (Boole’s inequality) follows from the fact that a probability measure is σ-sub-additive.</li>
<li><em>Events</em> can be <em>independent</em>. The following conditions hold equivalently for any independent events:
<ul>
<li><span class="math inline">\(\Pr[A_1 \cap A_2] = \Pr[A_1] \cdot \Pr[A_2]\)</span></li>
<li><span class="math inline">\(\Pr[A_1|A_2] = \Pr[A_1]\)</span></li>
</ul></li>
<li><strong>Bayes’ theorem</strong> and the <strong>law of total probability</strong> describe the basic properties of conditional probability.</li>
<li>A <strong>random variable</strong> is a mapping that maps a <em>value</em> to an <em>event</em>. Hence, we have probability measure defined on random variables, such as <span class="math inline">\(\Pr[X=x]\)</span>.
<ul>
<li>For <em>discrete</em> random variables, a <strong>probability mass function (pmf)</strong> determines a <strong>discrete probability distribution</strong>.</li>
<li>For <em>continuous</em> random variables, a <strong>probability density function (pdf)</strong> determines a <strong>continuous probability distribution</strong>.</li>
</ul></li>
<li><em>Random variables</em> can be <em>uncorrelated</em>. (<span class="math inline">\(\operatorname{Cov}(X,Y)=0 \iff \operatorname{E}[XY] = \operatorname{E}[X] \cdot \operatorname{E}[Y]\)</span>.)
<ul>
<li><em>Independent</em> random variables are uncorrelated.</li>
<li>However, uncorrelated random variables are not necessarily independent.</li>
</ul></li>
<li>A <em>distribution</em> can be presented using <strong>moments</strong>:
<ul>
<li><strong>Expectation (mean)</strong> <span class="math inline">\(\operatorname{E}[X]\)</span>: first raw moment.</li>
<li><strong>Variance</strong> <span class="math inline">\(\operatorname{Var}(X)\)</span>: second central moment.</li>
<li><strong>Skewness</strong> <span class="math inline">\(\operatorname{Skew}(X)\)</span>: third standardized moment.</li>
<li><strong>Kurtosis</strong> <span class="math inline">\(\operatorname{Kurt}(X)\)</span>: fourth standardized moment.</li>
<li>For a bounded distribution of probability, the collection of all the moments (of all orders) uniquely determines the distribution.</li>
<li>Some distributions, notably Cauchy distributions, do not have their moments defined.</li>
</ul></li>
<li><strong>Concentration inequalities</strong> provide bounds on how a random variable deviates from some value (usually one of its <em>moments</em>).
<ul>
<li><strong>Markov’s inequality</strong> is the simplest and weakest probability bound.</li>
<li><strong>Chebyshev’s inequality</strong> provides an upper bound on the probability that a random variable deviates from its expectation.</li>
<li><strong>Chernoff bound</strong> is stronger than Markov’s inequality.</li>
<li><strong>Hoeffding’s inequality</strong> provides an upper bound on the probability that the sum of random variables deviates from its expectation. It’s also useful for analyzing the number of required samples needed to obtain a confidence interval.</li>
</ul></li>
<li>Some common <em>discrete probability distributions</em>:
<ul>
<li><strong>Bernoulli distribution</strong>. Special case of Binomial distribution: <span class="math inline">\(\text{B}(1,p)\)</span>.</li>
<li><strong>Binomial distribution</strong> <span class="math inline">\(\text{B}(n,p)\)</span>. Given number of draws <span class="math inline">\(n\)</span>, the distribution of the number of successes.</li>
<li><strong>Geometric distribution</strong> <span class="math inline">\(\text{Geom}(p)\)</span>. Special case of negative binomial distribution: <span class="math inline">\(\text{NB}(1,1-p)\)</span>.</li>
<li><strong>Negative binomial distribution</strong> <span class="math inline">\(\text{NB}(r,p)\)</span>. Given number of failures <span class="math inline">\(r\)</span>, the distribution of the number of successes.</li>
</ul></li>
</ul></li>
</ul>
<section id="probability-measure-distribution-and-generalized-function" class="level2">
<h2>Probability measure, distribution and generalized function</h2>
<p>Intuitively, probability is a measure of uncertainty. Mathematically, probability is a real-valued function defined on a set of events in a probability space that satisfies measure properties such as countable additivity (or simply, <em>measure</em> on probability space).</p>
<p>Typically, a probability density function (pdf) or a probability mass function (pmf) determines a distribution in the probability space.</p>
<p><strong>Example 2.1.</strong> Consider the wave function of a particle: <span class="math display">\[\Psi(x,t)\]</span> where <span class="math inline">\(x\)</span> is position and <span class="math inline">\(t\)</span> is time.</p>
<p>If the particle’s position is measured, its location cannot be determined but is described by a probability distribution: The probability that the particle is found in <span class="math inline">\([x, x+\Delta x]\)</span> is <span class="math display">\[\Delta\Pr = |\Psi(x,t)|^2 \Delta x\]</span></p>
<p>The square modulus of the wave function (which is real-valued, non-negative) <span class="math display">\[\left|\Psi(x, t)\right|^2 = {\Psi(x, t)}^{*}\Psi(x, t) = \rho(x, t)\]</span> is interpreted as the pdf.</p>
<p>Since the particle must be found somewhere, we have the normalization condition: (by the assumption of unit measure) <span class="math display">\[\int\limits_{-\infty}^\infty |\Psi(x,t)|^2 dx = 1\]</span></p>
<p>Distributions are also called generalized functions in analysis. It expands the notion of functions to functions whose derivatives may not exist in the classical sense. Thus, it is not uncommon that many probability distributions cannot be described using classical (differentiable) functions. The Dirac delta function <span class="math inline">\(\delta\)</span> (which is a generalized function) is often used to represent a discrete distribution, or a partially discrete, partially continuous distribution, using a pdf.</p>
</section>
<section id="bayes-theorem-and-common-fallacies" class="level2">
<h2>Bayes’ theorem and common fallacies</h2>
<p>Bayes’ theorem forms the basis for <em>Bayesian inference</em>, which is an important method of statistical inference that updates the probability for a <em>hypothesis</em> as more evidence or information becomes available.</p>
<p>Hypotheses can also be fallacies. In Bayesian inference, if one can make the assumption that every event occurs independently and the probability is identically distributed throughout lasting trials, it is clear to see that some common beliefs are mistaken.</p>
<p><strong>Gambler’s fallacy (Monte Carlo fallacy).</strong> If an outcome occurs more frequently than normal during some period, it will happen less frequently in the future; contrariwise, if an outcome happens less frequently than normal during some period, it will happen more frequently in the future. This is presumed to be a means of <em>balancing</em> nature.</p>
<p>Gambler’s fallacy is considered a fallacy if the probability of outcomes is known to be independently, identically distributed. Assume that the future (the probability of event <span class="math inline">\(A_2\)</span>) has no effect on the past (the probability of event <span class="math inline">\(A_1\)</span>), we have <span class="math inline">\(\Pr[A_1|A_2] = \Pr[A_1]\)</span>. From Bayes’ theorem, it holds true that <span class="math display">\[\Pr[A_2|A_1] = \Pr[A_2]\]</span> That is, past events should not increase or decrease our confidence in a future event.</p>
<p><strong>Hot-hand fallacy.</strong> A person who has experienced success with a seemingly random event has a greater chance of further success in additional attempts. That is, if an outcome occurs more frequently than normal during some period, it will also happen frequently in the future.</p>
<p>If psychological factors can be excluded, then hot-hand fallacy is a fallacy caused by people’s confirmation bias. Like the gambler’s fallacy, if we can’t assume that the probability of outcomes is independently, identically distributed, we can’t simply conclude that this belief is mistaken.</p>
<p><strong>Inverse gambler’s fallacy.</strong> If an unlikely outcome occurs, then the trials must have been repeated many times before.</p>
<p>Assume that the past (the probability of event <span class="math inline">\(A_1\)</span>) has no effect on the future (the probability of event <span class="math inline">\(A_2\)</span>), we have <span class="math inline">\(\Pr[A_2|A_1] = \Pr[A_2]\)</span>. From Bayes’ theorem, it holds true that <span class="math display">\[\Pr[A_1|A_2] = \Pr[A_1]\]</span> That is, our confidence in <span class="math inline">\(A_1\)</span> should remain unchanged after we observe <span class="math inline">\(A_2\)</span>.</p>
</section>
<section id="lln-and-chebyshevs-inequality" class="level2">
<h2>LLN and Chebyshev’s inequality</h2>
<p><strong>Fallacies of hasty generalization and slothful induction (law of small numbers).</strong> Informal fallacies reaching an inductive generalization based on insufficient evidence, or denying a reasonable conclusion of an inductive argument.</p>
<p>Statistically saying, sampling from a small group can lead to misbeliefs that fail to hold for the entire population, if hypothesis testing is not carefully conducted.</p>
<p><strong>Theorem 2.2. (Law of large numbers)</strong> Let <span class="math inline">\(X_1, \dots, X_n\)</span> be an infinite sequence of i.i.d. Lebesgue integrable random variables with fixed expectation <span class="math inline">\(\operatorname{E}[X_1] = \cdots = \operatorname{E}[X_n] = \mu\)</span>. Define the sample average <span class="math display">\[\overline{X}_n = \frac{1}{n}(X_1 + \dots + X_n)\]</span></p>
<ol type="1">
<li><strong>(Weak law of large numbers; Khintchine’s law)</strong> The sample average converges in probability towards the expectation: <span class="math display">\[\lim_{n\to\infty} \Pr[|\overline{X}_n - \mu| &gt; \varepsilon] = 0\]</span></li>
<li><strong>(Strong law of large numbers)</strong> The sample average converges <em>almost surely</em> to the expectation: <span class="math display">\[\Pr[\lim_{n\to\infty} \overline{X}_n = \mu] = 1\]</span></li>
</ol>
<p>Chebyshev’s inequality provides an upper bound on the probability that a random variable deviates from its expected value. Thus, it may be used as a proof for the weak law of large numbers.</p>
</section>
<section id="how-is-mathematical-expectation-only-mathematical" class="level2">
<h2>How is mathematical expectation only “mathematical”?</h2>
<p>The expected value of a random variable <span class="math inline">\(X\)</span>: <span class="math display">\[\operatorname{E}[X] = \sum_{x \in \mathcal{X}} x \Pr[X=x]\]</span> While it seemingly gives an estimate on how people would “expect” a random variable to take its value, it can sometimes lead to counterintuitive results, as shown by the following paradox.</p>
<p><strong>St. Petersburg Paradox.</strong><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> A casino offers a game of chance for a gambler to flip a fair coin until it comes up tails. The initial stake starts at <span class="math inline">\(2\)</span> dollars and is doubled every time heads appears. The first time tails appears, the game ends and the gambler wins whatever is in the pot. Thus if the coin comes up tails the first time, the gambler wins <span class="math inline">\(2^1=2\)</span> dollars, and the game ends. If the coin comes up heads, the coin is flipped again. If the coin comes up tails the second time, the gambler wins <span class="math inline">\(2^2=4\)</span> dollars, and the game ends. If the coin comes up heads again, the coin is flipped again. If the coin comes up tails the third time, the gambler wins <span class="math inline">\(2^3=8\)</span> dollars, and the game ends. So on and so like. Eventually the gambler wins <span class="math inline">\(2^k\)</span> dollars, where <span class="math inline">\(k\)</span> is the number of coin flips until tails appears. (It is easy to see that <span class="math inline">\(k\)</span> satisfies the geometric distribution.) What would be a fair price to pay the casino for entering such a game? (Assume that there is no house edge)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(k\)</span>th coin flip</th>
<th style="text-align: center;"><span class="math inline">\(\Pr[\text{Tails}]\)</span></th>
<th style="text-align: center;">Stake</th>
<th style="text-align: center;">Expected payoff</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{2}\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{4}\)</span></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{16}\)</span></td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{32}\)</span></td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
</tr>
<tr class="odd">
<td style="text-align: center;">k</td>
<td style="text-align: center;"><span class="math inline">\((1/2)^k\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2^k\)</span></td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>The price should be made equal to the expected value that a gambler wins the stake, which is <span class="math display">\[\operatorname{E}[\text{Payoff}]
= \sum_{k=1}^{+\infty} \left(\frac{1}{2}\right)^k \cdot 2^k
= \sum_{k=1}^{+\infty} 1
= +\infty\]</span></p>
<p>If a rational gambler pays for entering a game if and only if its average payoff is larger than its price, then he would pay any price to enter this game (since the expected payoff of this game is infinitely large). But in reality, few of us are willing to pay even tens of dollars to enter such a game. What went wrong? Furthermore, if <em>mathematical</em> expectation does not reflect correctly what people expect from a game, how to quantify the “<em>true</em>” expectation?</p>
<p>The St. Petersburg paradox was initially stated by Nicolas Bernoulli in 1713. There are several proposed approaches for solving the paradox, including the <a href="https://en.wikipedia.org/wiki/Expected_utility_hypothesis">expected utility</a> theory with the hypothesis of diminishing marginal utility <a href="#ref-sep-paradox-stpetersburg"><span class="citation" data-cites="sep-paradox-stpetersburg">[1]</span></a>, and the cumulative prospect theory. However, none of them is purely probability theoretical, as they require the use of hypothesized economic/behavioral models.</p>
</section>
<section id="bias-of-sample-variance-and-bessels-correction" class="level2">
<h2>Bias of sample variance and Bessel’s correction</h2>
<p>In probability theory, the variance of a random variable <span class="math inline">\(X\)</span> is defined as <span class="math display">\[\operatorname{Var}(X) = \operatorname{E}[(X-\mu)^2]
= \frac{1}{N} \sum_{i=1}^N (X_i-\bar{X})^2\]</span></p>
<p>In statistics, when calculating the sample variance in order to give an estimation of the population variance, and the population mean is unknown, Bessel’s correction<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> (use of <span class="math inline">\(N-1\)</span> instead of <span class="math inline">\(N\)</span>) is often preferred: <span class="math display">\[s^2 = \frac{1}{N-1} \sum_{i=1}^N (X_i-\bar{X})^2\]</span></p>
<p>A few remarks and caveats:</p>
<ol type="1">
<li>Bessel’s correction is only necessary when the population mean is unknown and estimated as the sample mean.</li>
<li>Without Bessel’s correction, the estimated variance would be <em>biased</em>; the biased sample variance <span class="math inline">\(s_n^2\)</span> tends to be much smaller than the population variance <span class="math inline">\(\sigma^2\)</span>, whether the sample mean is smaller or larger than the population mean.</li>
<li>Bessel’s correction does not yield an unbiased estimator of standard deviation, only variance and covariance.</li>
<li>The corrected estimator often has a larger mean squared error (MSE).</li>
</ol>
</section>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Books:</strong></p>
<p>M. Mitzenmacher and E. Upfal, <em>Probability and Computing: Randomized Algorithms and Probabilistic Analysis</em>.</p>
<p>M. Baron, <em>Probability and Statistics for Computer Scientists</em>, 2nd ed.</p>
<p><strong>Articles:</strong></p>
<div id="refs" class="references">
<div id="ref-sep-paradox-stpetersburg">
<p>[1] R. Martin, “The st. Petersburg paradox,” in <em>The stanford encyclopedia of philosophy</em>, Summer 2014., E. N. Zalta, Ed. <a href="https://plato.stanford.edu/archives/sum2014/entries/paradox-stpetersburg/" class="uri">https://plato.stanford.edu/archives/sum2014/entries/paradox-stpetersburg/</a>; Metaphysics Research Lab, Stanford University, 2014. </p>
</div>
</div>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox" class="uri">https://en.wikipedia.org/wiki/St._Petersburg_paradox</a><a href="#fnref1" class="footnoteBack">↩</a></p></li>
<li id="fn2"><p><a href="https://en.wikipedia.org/wiki/Bessel&#39;s_correction" class="uri">https://en.wikipedia.org/wiki/Bessel's_correction</a><a href="#fnref2" class="footnoteBack">↩</a></p></li>
</ol>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>Remember My Last Tabs, File Manager</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/notes/161208" />
    <id>tag:www.soimort.org,2017:/notes/161208</id>
    <published>2016-12-08T00:00:00+01:00</published>
    <updated>2016-12-13T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="tooling" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>It’s 2016, and I can’t believe that there is still no “Continue where you left off” option in most dominant GUI file managers (as far as I know)!</p>
<p>Yes, it bugs me when I can’t restore my last open tabs and I want my old session so badly. Remembering last tabs, if I get the history right, was a feature first introduced by Google Chrome, and soon it started to play an indispensable part in my daily workflow. I’m a multitasker, but the computing resource of my laptop is very limited – Say, if I have a session in which I am working on a homework report, having loads of folders, web pages and editor buffers open and those can fill up gigabytes of RAM easily, then I realize that I will need to compile something really hard-core, or maybe just take a short rest and do some random surfing on the web, certainly I would rather close all those engrossing processes for the time being, hoping that they could continue with all the open tabs I left off.</p>
<p>It’s mainly four types of applications that account for so-called “work sessions” for me:</p>
<ul>
<li>Terminal emulator</li>
<li>File manager</li>
<li>Web browser</li>
<li>Text editor</li>
</ul>
<p>Terminals don’t take up a lot of memory, so I wouldn’t mind leaving them open. Typical browsers, including Chromium and Firefox, do support session resuming (and there are even <a href="https://chrome.google.com/webstore/detail/session-buddy/edacconmaakjimmfgnblocblbcdcpbko">extensions</a> which allow you to save current tabs and recover them at any later time). Any decent text editor (or IDE) may also be configured to remember open tabs / sessions. After all, average file managers fail to meet my basic needs of productivity.</p>
<section id="file-managers-the-unremembered-ux" class="level2">
<h2>File managers: The unremembered UX</h2>
<p>I’m on GNOME 3, but currently using the <a href="https://github.com/mate-desktop/caja">Caja</a> file manager – ever since Nautilus 3.6 decided to remove two or three features I found important to me (<a href="https://bugzilla.gnome.org/show_bug.cgi?id=676842">compact mode</a>, <a href="https://bugzilla.gnome.org/show_bug.cgi?id=692852">backspace navigation</a>) and introduced an awkward, smug “search-whatever-shit-as-you-type” feature.</p>
<p>File managers I’ve tried so far:</p>
<ul>
<li>Nautilus (GNOME). As said, already rendered unusable for me.</li>
<li>Pantheon. Like Nautilus, it doesn’t feature a compact mode either.</li>
<li>Nemo (Cinnamon). Nope, segfaults too often.</li>
<li>Caja (MATE). It’s OK, just what I’m using right now.
<ul>
<li>Open issue for saving sessions: <a href="https://github.com/mate-desktop/caja/issues/523" class="uri">https://github.com/mate-desktop/caja/issues/523</a></li>
</ul></li>
<li>Dolphin (KDE). OK, unless it’s from the foreign land of KDE.
<ul>
<li>Open issue for saving sessions: <a href="https://bugs.kde.org/show_bug.cgi?id=246028" class="uri">https://bugs.kde.org/show_bug.cgi?id=246028</a></li>
</ul></li>
<li>Konqueror (KDE). It’s both a web browser and a file manager, and it’s the only one I know that can save / restore open tabs. Unfortunately it has only limited file management functionality. (sufficient as a <em>file viewer</em>, though?)</li>
</ul>
<p>Among all above, I settled down with Caja, simply because there was no reasonably good alternative. Still, I’m wishing for something that can save session states for me. After doing a little research, I realized that:</p>
<ol type="1">
<li>There is no commonly adopted protocol addressing this issue. <a href="https://wiki.gnome.org/Projects/SessionManagement/SavingState">Not even on GNOME</a>.</li>
<li>There is <a href="https://wiki.gnome.org/Projects/SessionManagement/EggSMClient">EggSMClient</a>, but state saving is implemented on the X (desktop) session level thus only available on the <a href="https://www.x.org/releases/X11R7.7/doc/libSM/xsmp.html">XSMP</a> backend. It works when you logout your desktop session and login, but not when you close the window and restart the application again.</li>
<li>It is ultimately the application itself which must maintain its session states and restore them when required.</li>
</ol>
</section>
<section id="a-quick-working-patch-for-caja" class="level2">
<h2>A quick working patch for Caja</h2>
<p>Let’s take the issue into more technical details. On Caja (or other similarly GTK+/GLib-based file managers), one need to implement:</p>
<ul>
<li>On the <code>destroy</code> callback of the main <code>GtkObject</code>, all last remaining session data (e.g., internal states about open tabs, windows) must be saved to disk. (after the main loop ends there’s no more chance to get this information back)</li>
<li>On GUI initialization, read last session data (if exist) from disk, and reopen saved tabs as well as windows.</li>
<li>On the event of changing state (e.g., creating or closing tab/window, repositioning tabs), session data are updated respectively and, optionally, saved to disk.</li>
</ul>
<p>With <code>caja_application_get_session_data()</code>, making a quick workaround that enables Caja to save and restore a session is somewhat trivial labor; however, it seems Caja doesn’t record the correct (spatial) ordering of tabs in its session data – so I wouldn’t consider this as a satisfying solution to the issue, and I have no intent to send such an incomplete patch to Caja. Nevertheless, it’s better than nothing, and, if ordering of tabs really matters, it would be feasible to write a <a href="https://github.com/soimort/dotfiles/blob/b721e42238a90e88c83d1feb20682d0605367b11/Scripts/Open-Folders">wrapper script</a> that manipulates the XML file in <code>$HOME/.config/caja/last-session</code>.</p>
<p>And here goes the patch: (Applied to Caja 1.16.1; definitely UNWARRANTED)</p>
<script src="https://gist.github.com/soimort/73c75266d1610ff0af68b40e7b07d939.js"></script>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>Boilerplating Pandoc for Academic Writing</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/notes/161117" />
    <id>tag:www.soimort.org,2017:/notes/161117</id>
    <published>2016-11-17T00:00:00+01:00</published>
    <updated>2016-11-17T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    <category term="tooling" />
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p>For starters, this is how you might want to turn your well-written Markdown file (with common metadata fields like <code>title</code>, <code>author</code> and <code>date</code>) into a properly typeset PDF document:</p>
<pre><code>$ pandoc src.md -o out.pdf</code></pre>
<p>However, Markdown is not TeX. <em>Not even close.</em> Once you need to have some bleeding edge control over the typesetting outcome, or perhaps just a little refinement on its LaTeX templating, you’ll soon notice that Pandoc has its quirks and gotchas. I’ve been utilizing Pandoc in all my serious academic writing (incl. homework reports) for years, ever since I gave up on learning more about the overwhelmingly sophisticated TeX ecosystem and turned to something that “just works”. Pandoc fits my needs well. And when it doesn’t, there’s almost always a workaround that achieves the same thing neatly. And this is what this write-up is mostly about.</p>
<section id="tweaking-default.latex-bad-idea." class="level2">
<h2>Tweaking <code>default.latex</code>? Bad idea.</h2>
<p>You could, of course, modify the default template (<a href="https://github.com/jgm/pandoc-templates/blob/master/default.latex"><code>default.latex</code></a>) provided by Pandoc, as long as you’re no stranger to LaTeX. In this way, you can achieve anything you want – in <em>pure</em> LaTeX.</p>
<pre><code>$ pandoc <span class="do">--template my-default.latex</span> src.md -o out.pdf</code></pre>
<p>There are, however, a few problems with this naïve approach:</p>
<ol type="1">
<li>If you are tweaking the template just for something you’re currently working on, you will end up with some highly document-specific, hardly reusable template. Also this won’t give you any good for using Pandoc – you could just write plain LaTeX anyway.</li>
<li>If Pandoc improves its default template for a newer version, your home-brewed template won’t benefit from this (unless you’re willing to merge the diffs and resolve any conflicts by hand).</li>
</ol>
<p>I’m conservative about changing the templates. If it’s a general issue that needs to be fixed in the default template, sending a pull request to <a href="https://github.com/jgm/pandoc-templates">pandoc-templates</a> might be a better idea. Of course, if there’s a certain submission format you have to stick with (given LaTeX templates for conference papers), then you will fall back on your own.</p>
</section>
<section id="separating-the-formatting-stuff" class="level2">
<h2>Separating the formatting stuff</h2>
<p>I wouldn’t claim that I know the best practice of using Pandoc, but there’s such a common idiom that cannot be overstressed: <em>Separate presentation and content!</em></p>
<p>In the YAML front matter of <code>src.md</code> (the main Markdown file you’re writing), put only things that matter to your potential readers:</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><div class="sourceLine" id="1" href="#1" data-line-number="1"><span class="ot">---</span></div>
<div class="sourceLine" id="2" href="#2" data-line-number="2"><span class="fu">title:</span><span class="at"> Boilerplating Pandoc for Academic Writing</span></div>
<div class="sourceLine" id="3" href="#3" data-line-number="3"><span class="fu">subtitle:</span><span class="at"> or How I Learned to Stop Typesetting and Concentrate on the Math</span></div>
<div class="sourceLine" id="4" href="#4" data-line-number="4"><span class="fu">author:</span><span class="at"> Mort Yao</span></div>
<div class="sourceLine" id="5" href="#5" data-line-number="5"><span class="fu">date:</span><span class="at"> 17 November 2016</span></div>
<div class="sourceLine" id="6" href="#6" data-line-number="6"><span class="fu">abstract:</span><span class="at"> |</span></div>
<div class="sourceLine" id="7" href="#7" data-line-number="7">  Lorem ipsum dolor sit amet, consectetur adipiscing elit,</div>
<div class="sourceLine" id="8" href="#8" data-line-number="8">  sed do eiusmod tempor incididunt ut labore et dolore magna</div>
<div class="sourceLine" id="9" href="#9" data-line-number="9">  aliqua. Ut enim ad minim veniam, quis nostrud exercitation</div>
<div class="sourceLine" id="10" href="#10" data-line-number="10">  ullamco laboris nisi ut aliquip ex ea commodo consequat.</div>
<div class="sourceLine" id="11" href="#11" data-line-number="11"><span class="ot">---</span></div></code></pre></div>
<p>And in a separate YAML file (let’s call it <code>default.yaml</code>), here goes the formatting stuff:</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><div class="sourceLine" id="1" href="#1" data-line-number="1"><span class="ot">---</span></div>
<div class="sourceLine" id="2" href="#2" data-line-number="2"><span class="fu">geometry:</span><span class="at"> margin=1.5in</span></div>
<div class="sourceLine" id="3" href="#3" data-line-number="3"><span class="fu">indent:</span><span class="at"> true</span></div>
<div class="sourceLine" id="4" href="#4" data-line-number="4"><span class="fu">header-includes:</span><span class="at"> |</span></div>
<div class="sourceLine" id="5" href="#5" data-line-number="5">  \usepackage<span class="kw">{</span>tcolorbox<span class="kw">}</span></div>
<div class="sourceLine" id="6" href="#6" data-line-number="6">  \newcommand\qed<span class="kw">{</span>\hfill\rule{1em<span class="kw">}{</span>1em<span class="kw">}</span>}</div>
<div class="sourceLine" id="7" href="#7" data-line-number="7"><span class="ot">---</span></div></code></pre></div>
<p>Above is my personal default, and it’s worth a few words to explain:</p>
<ul>
<li><code>geometry</code> is where you control the geometric settings of your document. For example, you may narrow down the page margin to <code>margin=1.5in</code>, and this is equivalent to raw LaTeX:</li>
</ul>
<pre><code>\usepackage[margin=1.5in]{geometry}</code></pre>
<ul>
<li>Set <code>indent</code> to any value other than <code>false</code> if paragraph indentation is desired. (And it is often desired in formal publications.)</li>
<li><code>header-includes</code> is where you define your own macros, configure existing ones, or claim <code>\usepackage</code> in case you want to use a package not enabled by Pandoc (e.g., <a href="https://www.ctan.org/pkg/tcolorbox"><code>tcolorbox</code></a>). Although you might as well define those in other places (e.g., in the content of a Markdown file), <em>don’t do that</em>.
<ul>
<li>This decent Q.E.D. tombstone: <code>\newcommand\qed{\hfill\rule{1em}{1em}}</code> is my favorite of all time. It doesn’t require the <code>amsthm</code> package.</li>
</ul></li>
</ul>
<p>With a separate <code>default.yaml</code>, now here we are:</p>
<pre><code>$ pandoc <span class="do">default.yaml</span> src.md -o out.pdf</code></pre>
</section>
<section id="separating-header-includes" class="level2">
<h2>Separating <code>header-includes</code></h2>
<p>You might have already noticed that the <code>subtitle</code> field won’t display in the produced PDF file. As far as I’m concerned (in Pandoc 1.18), this is the expected behavior. See <a href="http://pandoc.org/MANUAL.html#fn1">here in README</a>:</p>
<blockquote>
<p>To make <code>subtitle</code> work with other LaTeX document classes, you can add the following to <code>header-includes</code>:</p>
<div class="sourceCode"><pre class="sourceCode tex"><code class="sourceCode latex"><div class="sourceLine" id="1" href="#1" data-line-number="1"><span class="fu">\providecommand</span>{<span class="fu">\subtitle</span>}[1]{<span class="co">%</span></div>
<div class="sourceLine" id="2" href="#2" data-line-number="2">  <span class="bu">\usepackage</span>{<span class="ex">titling</span>}</div>
<div class="sourceLine" id="3" href="#3" data-line-number="3">  <span class="fu">\posttitle</span>{<span class="co">%</span></div>
<div class="sourceLine" id="4" href="#4" data-line-number="4">    <span class="fu">\par\large</span>#1<span class="kw">\end</span>{<span class="ex">center</span>}}</div>
<div class="sourceLine" id="5" href="#5" data-line-number="5">}</div></code></pre></div>
</blockquote>
<p>Unfortunately, this won’t work (until <a href="https://github.com/jgm/pandoc/issues/2139">Issue #2139</a> is resolved) since Pandoc parses the <code>header-includes</code> metadata field as Markdown, and the bracketed <code>[1]</code> is misinterpreted as literals rather than a part of LaTeX control sequence. So the workaround is: Instead of embedding <code>header-includes</code> as a metadata field in YAML, we should separate it into another file for this dedicated purpose (it’s simply raw LaTeX anyway), and include it using <code>--include-in-header/-H</code>:</p>
<pre><code>$ pandoc <span class="do">-H header.tex</span> default.yaml src.md -o out.pdf</code></pre>
<p>Note that you can’t have two <code>header-includes</code> for one document. So the <code>header-includes</code> field specified in YAML metadata will be overridden by the content of <code>header.tex</code>.</p>
</section>
<section id="citing-sources" class="level2">
<h2>Citing sources</h2>
<p>While the Markdown syntax for citing is rather easy (<code>[@id]</code>), it takes effort to make things right, especially if you have a certain preferred citation format (APA, MLA, Chicago, IEEE, etc.).</p>
<p>The suggestion is: Use <a href="https://hackage.haskell.org/package/pandoc-citeproc">pandoc-citeproc</a>. Once you have a list of references you’re interested in, you need two things to typeset those nicely in your document:</p>
<ul>
<li>A CSL (Citation Style Language) file (<code>.csl</code>), to specify the citation format you want to use.
<ul>
<li>You can preview (and download) many common citation styles in the <a href="https://www.zotero.org/styles">Zotero Style Repository</a>.</li>
</ul></li>
<li>A BibTeX file (<code>.bib</code>), which is a list of all entries you might cite.
<ul>
<li>Citation entries in BibTeX format may be found easily on the Internet, through academic search engines and databases. Concatenate them one by one.</li>
</ul></li>
</ul>
<p>As part of the YAML metadata: (Assume you have <code>ieee.csl</code> and <code>references.bib</code>)</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><div class="sourceLine" id="1" href="#1" data-line-number="1"><span class="fu">csl:</span><span class="at"> ieee.csl</span></div>
<div class="sourceLine" id="2" href="#2" data-line-number="2"><span class="fu">bibliography:</span><span class="at"> references.bib</span></div></code></pre></div>
<p>Using <code>pandoc-citeproc</code> as a filter, generate the document with citations:</p>
<pre><code>$ pandoc <span class="do">--filter pandoc-citeproc</span> -H header.tex default.yaml src.md -o out.pdf</code></pre>
<p>The list of references is appended to the end of the document. It is often desirable to give the references an obvious title (“References”), start from a new page and avoid any further indentation, so the following comes in the end of the Markdown source:</p>
<div class="sourceCode"><pre class="sourceCode tex"><code class="sourceCode latex"><div class="sourceLine" id="1" href="#1" data-line-number="1"><span class="fu">\newpage</span></div>
<div class="sourceLine" id="2" href="#2" data-line-number="2"><span class="fu">\setlength\parindent</span>{0pt}</div>
<div class="sourceLine" id="3" href="#3" data-line-number="3"></div>
<div class="sourceLine" id="4" href="#4" data-line-number="4"># References</div></code></pre></div>
</section>
<section id="putting-it-all-together" class="level2">
<h2>Putting it all together!</h2>
<p>Basically, we need 5 files in total:</p>
<ul>
<li>For content:
<ul>
<li><code>src.md</code> (Markdown + possibly LaTeX mixed format): Main text.</li>
<li><code>references.bib</code> (BibTeX/BibLaTeX format): List of references.</li>
</ul></li>
<li>For presentation:
<ul>
<li><code>default.yaml</code> (YAML format): Format-related metadata.</li>
<li><code>header.tex</code> (LaTeX format): Content of <code>header-includes</code>; package imports and macro definitions.</li>
<li><code>ieee.csl</code> (CSL XML format): Citation style.</li>
</ul></li>
</ul>
<p>And one command:</p>
<pre><code>$ pandoc --filter pandoc-citeproc -H <span class="do">header.tex</span> <span class="do">default.yaml</span> <span class="do">src.md</span> -o out.pdf</code></pre>
</section>
<section id="open-question-lightweight-replacement-for-amsthm" class="level2">
<h2>Open question: Lightweight replacement for <code>amsthm</code>?</h2>
<p>Pandoc doesn’t provide native support for <a href="https://www.ctan.org/pkg/amsthm"><code>amsthm</code></a> (and I wonder if there will ever be). You can still have the same thing in Pandoc Markdown:</p>
<div class="sourceCode"><pre class="sourceCode tex"><code class="sourceCode latex"><div class="sourceLine" id="1" href="#1" data-line-number="1"><span class="fu">\newtheorem</span>{definition}{Definition}</div>
<div class="sourceLine" id="2" href="#2" data-line-number="2"></div>
<div class="sourceLine" id="3" href="#3" data-line-number="3"><span class="kw">\begin</span>{<span class="ex">definition</span>}</div>
<div class="sourceLine" id="4" href="#4" data-line-number="4">Man is a rational animal.</div>
<div class="sourceLine" id="5" href="#5" data-line-number="5"><span class="kw">\end</span>{<span class="ex">definition</span>}</div></code></pre></div>
<p>However, everything in between <code>\begin</code> and <code>\end</code> will be treated as raw LaTeX, and the expressiveness of Markdown is lost there. More importantly, this is purely a LaTeX-specific thing, so there’s no way for Pandoc to convert this to HTML or any other format (unless you have a filter that does the trick). Consequently, I tend to write all definitions / theorems (lemmas, claims, corollaries, propositions…) in simple Markdown:</p>
<pre><code>**Definition 1.** *Man is a rational animal.*</code></pre>
<p>It does have some advantages over <code>amsthm</code>:</p>
<ul>
<li>Using <code>amsthm</code>, you cannot see the numbering of each theorem (definition, etc.) in the text editor (well, you can’t without a dedicated plugin at least). This is inconvenient when you need to refer to a prior one later. By numbering them explicitly, you can clearly see these ordinals in the Markdown source.</li>
<li>It is perfectly valid Markdown, so it converts to any format as you wish (HTML, for example).</li>
</ul>
<p>This also has some drawbacks compared to using <code>amsthm</code>, though:</p>
<ul>
<li>It doesn’t have theorem counters. You need to number things explicitly, manually. (Clearly you can’t have implicit numbering and explicit numbering at the same time, so here’s the trade-off.)</li>
<li>It doesn’t have automatic formatting. That is, you could possibly get the style for a certain entry (plain, definition, remark) wrong.</li>
<li>Semantically, they are not recognized as theorems, just normal text paragraphs. This is problematic if you want to prevent definitions and theorems from being indented, since there’s no way for LaTeX to tell them from a normal text.</li>
</ul>
<p>(Probably) The best solution is to write a filter that (conventionally) converts any plain text like <code>Definition 1</code> (and <code>Lemma 2</code>, <code>Theorem 3</code>, etc.) in the beginning of a paragraph to proper Markdown (for HTML target) or corresponding <code>amsthm</code> block (for LaTeX target). Even better, it should be able to do cross-references accordingly (Remember <code>Lemma 10.42</code>? Let’s put an anchored link on that!). This is yet to be done, but would be very helpful to someone who does a lot of theorems and proofs thus wants to avoid the kludge of mixing raw LaTeX with semantically evident Markdown.</p>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>Bloom Filters in Adversarial Environments</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/reports/bloom-filters-in-adversarial-environments" />
    <id>tag:www.soimort.org,2017:/reports/bloom-filters-in-adversarial-environments</id>
    <published>2016-11-15T00:00:00+01:00</published>
    <updated>2016-11-15T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p><p style='background-color:yellow'> This is an expository reading summary of a selected <a href="https://eprint.iacr.org/2015/543.pdf">CRYPTO 2015 paper</a> I did as an assignment in KU’s <a href="http://kurser.ku.dk/course/nscphd1080/2016-2017">Introduction to Modern Cryptography</a> course. Adversarial-resilient Bloom filters are the counterparts of cryptographically secure hash functions in an adversarial setting, where adaptive adversaries that have access to a deterministic or non-deterministic query oracle may challenge the data structure in a way that intentionally increases the false positive rate of querying. As a preliminary result, this paper shows that the resistance of Bloom filters against computationally bounded adversaries requires that <a href="/mst/1/#p-versus-np-problem-and-one-way-functions">one-way functions exist</a>; furthermore, such constructions are possible using pseudorandom permutations. I do find the proof a bit involved, but the notions of security introduced for Bloom filters are new and appealing (which I haven’t read previously anywhere else).</p></p>
<p>Original paper:</p>
<ul>
<li><strong>M. Naor and E. Yogev, “Bloom filters in adversarial environments,” in Annual Cryptology Conference, 2015.</strong> <a href="https://arxiv.org/abs/1412.8356">[arXiv:1412.8356]</a></li>
</ul>
<hr />
<p style="text-align:center !important;text-indent:0 !important"><strong>Abstract</strong></p>
<p>Bloom filter is a hash-based probabilistic data structure which is space-efficient for set membership querying, with a small probability of false positives. Naor and Yogev’s 2015 paper introduces the adversarial model and formally proposes a strong notion of security for Bloom filters, i.e., <em>adversarial resilience</em>, based on an adversarial game under a cryptographic setting. This paper also discusses the correspondence between adversarial-resilient Bloom filters and the open assumption that one-way functions exist, thus enables theoretical constructions using pseudorandom permutations. We believe that such an understanding will help design practical Bloom filters that are safe from known attacks in software systems.</p>
<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<p>Probabilistic data structures are data structures that employ randomness in their designs to enable more efficient approaches of storing and querying data, compared to deterministic ones. Traditionally, the algorithmic probabilistic analysis of such data structures assumes the model where all inputs and queries are <em>independent</em> of the internal randomness of data structures. In this work, we consider an adversarial environment, where a computationally bounded adversary<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> may adaptively chooses inputs and queries with the intention of degrading the efficiency of the underlying data structure of some computer system. By introducing the adversarial model, we analyze the behavior of such data representations under the cryptographic notion of computational security against adversaries; furthermore, it enables us to construct more efficient, provably secure schemes of probabilistic data structures.</p>
<p>As a concrete example, a Bloom filter is a probabilistic data structure that holds a set <span class="math inline">\(S\)</span> of elements approximately, using significantly fewer bits of storage and allowing for faster access than a complete representation. As a trade-off between efficiency and preciseness, for any query of <span class="math inline">\(x \in S\)</span>, a Bloom filter always outputs a <em>yes</em>-answer, and for any query of <span class="math inline">\(x \not\in S\)</span>, it should output a <em>yes</em>-answer only with small probability. In other words, a <em>no</em>-answer given by a Bloom filter indicates unambiguously that <span class="math inline">\(x \not\in S\)</span>, while a <em>yes</em>-answer indicates that <span class="math inline">\(x \in S\)</span> probably holds<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>, that is, it allows false positives. Ideally, the error probability that a Bloom filter returns a false positive should be as small as possible.</p>
<p>Approaching the commonly-seen set membership problem, Bloom filters have been implemented widely in real-world applications, specifically as internal data representations for optimizing large-scale software systems. For example, Akamai’s CDN<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> servers maintain Bloom filters in their memories to decide whether to lookup the disk cache for a requested resource, and a false positive of the Bloom filter causes a cache miss, which means that the server has to make an unnecessary disk lookup at an expense of time and system workload; if an attacker exploits the behaviors of the Bloom filter, it is possible for them to cast queries that degrade the disk cache hit rate of the CDN servers, and consequently, perform a Denial-of-Service (DoS) attack.<span class="citation" data-cites="maggs2015algorithmic">[1]</span> On another scenario, where Bitcoin clients apply Bloom filters in the Simplified Payment Verification (SPV) mode to increase the overall performance of wallet synchronization, an adversary may perform a DoS attack on an SPV node by learning from the responses of Bloom filters they have access to.<span class="citation" data-cites="benjaminattacks">[2]</span></p>
<p>As discussed above, the adversarial model addresses some security issues, thus the necessity of defining security in adversarial environments and constructing provably secure Bloom filters arises. Essentially, it is desirable for a well-constructed Bloom filter to maintain its small error probability in an adversarial environment; we say that such a Bloom filter is <em>adversarial resilient</em> (or just <em>resilient</em>). In an adversarial game, where an adversary has oracle access to the Bloom filter and is allowed to make a number of <span class="math inline">\(t\)</span> queries before it outputs a certain <span class="math inline">\(x^*\)</span> (that has not been queried before) which is believed to be a false positive, and if it is, the adversary wins the game. We say that a Bloom filter is <span class="math inline">\((n,t,\varepsilon)\)</span>-<em>adversarial resilient</em> if when initialized over sets of size <span class="math inline">\(n\)</span> then after <span class="math inline">\(t\)</span> queries the probability of <span class="math inline">\(x^*\)</span> being a false positive is at most <span class="math inline">\(\varepsilon\)</span>. A Bloom filter that is resilient for any polynomially many queries is said to be <em>strongly resilient</em>.</p>
<p>Clearly, a trivial construction of a strongly resilient Bloom filter would be a deterministic lookup table that stores <span class="math inline">\(S\)</span> precisely, so that there is no false positive which an adversary can find. However, such a construction does not take advantage of the space and time efficiency as a normal Bloom filter would do, since it stores every element in the memory. In the following, we consider only non-trivial Bloom filters, and we show that for a non-trivial Bloom filter to be adversarial-resilient, one-way functions must exist; that is, if one-way functions do not exist, then any non-trivial Bloom filter can be attacked with a non-negligible probability by an efficient adversary. Furthermore, under the assumption that one-way functions exist, a pseudorandom permutation (PRP) can be used to construct a strongly resilient Bloom filter which has a reasonable memory consumption.</p>
<p>The construction of a Bloom filter consists of two algorithms: an initialization algorithm that gets a set <span class="math inline">\(S\)</span> and outputs a memory-efficient representation of <span class="math inline">\(S\)</span>; a query algorithm that gets a representation of <span class="math inline">\(S\)</span> and an <span class="math inline">\(x\)</span> to be checked and outputs <span class="math inline">\(1\)</span> if <span class="math inline">\(x \in S\)</span>, otherwise <span class="math inline">\(0\)</span>. Typically, the initialization algorithm is randomized but the query algorithm is deterministic, that is, a query operation does not amend the existing representation. We say that such a Bloom filter has a <em>steady representation</em>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
</section>
<section id="definitions" class="level1">
<h1>2. Definitions</h1>
<p>In the following model, we consider a universal set <span class="math inline">\(U\)</span> and a subset <span class="math inline">\(S \subset U\)</span> to be stored in a Bloom filter. We denote that <span class="math inline">\(u=|U|\)</span> and <span class="math inline">\(n=|S|\)</span>.</p>
<p><strong>Definition 1. (Steady-representation Bloom filter)</strong> <em>Let <span class="math inline">\(\mathbf{B}=(\mathbf{B}_1,\mathbf{B}_2)\)</span> be a pair of polynomial-time algorithms, where <span class="math inline">\(\mathbf{B}_1\)</span> is a randomized algorithm that gets as input a set <span class="math inline">\(S\)</span> and outputs a representation <span class="math inline">\(M\)</span>, and <span class="math inline">\(\mathbf{B}_2\)</span> is a deterministic algorithm that gets as input a representation <span class="math inline">\(M\)</span> and a query element <span class="math inline">\(x \in U\)</span>. We say that <span class="math inline">\(\mathbf{B}\)</span> is an <span class="math inline">\((n,\varepsilon)\)</span>-Bloom filter (with a steady representation) if for any set <span class="math inline">\(S \subset U\)</span> of size <span class="math inline">\(n\)</span> it holds that:</em></p>
<ol type="1">
<li><p><span class="math inline">\(\forall x \in S, \Pr[\mathbf{B}_2(\mathbf{B}_1(S), x) = 1] = 1\)</span> <strong>(Completeness)</strong></p></li>
<li><p><span class="math inline">\(\forall x \not\in S, \Pr[\mathbf{B}_2(\mathbf{B}_1(S), x) = 1] \leq \varepsilon\)</span> <strong>(Soundness)</strong></p></li>
</ol>
<p><em>where the probability is taken over the randomness used by the algorithm <span class="math inline">\(\mathbf{B}_1\)</span>.</em></p>
<p>Intuitively, the first property (completeness) says that for all elements in the set <span class="math inline">\(S\)</span>, the Bloom filter is guaranteed to output a <em>yes</em>-answer correctly; the second property (soundness) gives the upper bound that the Bloom filter outputs a false positive, that is, the query algorithm returns <span class="math inline">\(1\)</span> when an element does not actually belong to the set <span class="math inline">\(S\)</span>. Formally,</p>
<p><strong>False positive and error rate.</strong> Given a representation <span class="math inline">\(M\)</span> of <span class="math inline">\(S\)</span>, if <span class="math inline">\(x \not\in S\)</span> and <span class="math inline">\(\mathbf{B}_2(M,x)=1\)</span>, we say that <span class="math inline">\(x\)</span> is a <em>false positive</em>. And we say that the probability bound <span class="math inline">\(\varepsilon\)</span> of outputting false positives is the <em>error rate</em> of <span class="math inline">\(\mathbf{B}\)</span>.</p>
<p>In an adversarial environment, consider the following experiment for any Bloom filter <span class="math inline">\(\mathbf{B}=(\mathbf{B}_1,\mathbf{B}_2)\)</span>, adversary <span class="math inline">\(\mathcal{A}\)</span>, value <span class="math inline">\(t\)</span> as the bound of the amount of queries which <span class="math inline">\(\mathcal{A}\)</span> can make, and value <span class="math inline">\(\lambda\)</span> as the security parameter.</p>
<p><strong>The Bloom filter resilience challenge experiment <span class="math inline">\(\mathsf{Challenge}_{\mathcal{A},\mathbf{B},t}(\lambda)\)</span>:</strong></p>
<ol type="1">
<li><span class="math inline">\(M \leftarrow \mathbf{B}_1(1^\lambda,S)\)</span>.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></li>
<li><span class="math inline">\(x^* \leftarrow \mathcal{A}^{\mathbf{B}_2(M,\cdot)}(1^\lambda,S)\)</span>, where <span class="math inline">\(\mathcal{A}\)</span> performs at most <span class="math inline">\(t\)</span> queries <span class="math inline">\(x_1,\dots,x_t\)</span> to the oracle <span class="math inline">\(\mathbf{B}_2(M,\cdot)\)</span>. Note that <span class="math inline">\(\mathcal{A}\)</span> has only oracle access to the Bloom filter and cannot see the representation <span class="math inline">\(M\)</span>.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></li>
<li>The output of the experiment is defined to be <span class="math inline">\(1\)</span>, if <span class="math inline">\(x^* \not\in S \cup \{x_1,\dots,x_t\}\)</span> and <span class="math inline">\(\mathbf{B}_2(M,x^*)=1\)</span>, and <span class="math inline">\(0\)</span> otherwise. If the output of the experiment is <span class="math inline">\(1\)</span>, we say that <span class="math inline">\(\mathcal{A}\)</span> succeeds.</li>
</ol>
<p><strong>Definition 2. (Adversarial-resilient Bloom filter)</strong> <em>Let <span class="math inline">\(\mathbf{B}=(\mathbf{B}_1,\mathbf{B}_2)\)</span> be an <span class="math inline">\((n,\varepsilon)\)</span>-Bloom filter (with a steady representation). We say that <span class="math inline">\(\mathbf{B}\)</span> is an <span class="math inline">\((n,t,\varepsilon)\)</span>-adversarial resilient Bloom filter if for any set <span class="math inline">\(S\)</span> of size <span class="math inline">\(n\)</span>, for all sufficiently large <span class="math inline">\(\lambda \in \mathbb{N}\)</span> and for all probabilistic polynomial-time adversaries <span class="math inline">\(\mathcal{A}\)</span>, it holds that</em> <span class="math display">\[\Pr[\mathsf{Challenge}_{\mathcal{A},\mathbf{B},t}(\lambda) = 1] \leq \varepsilon\]</span></p>
<p><em>where the probability is taken over the randomness used by the algorithm <span class="math inline">\(\mathbf{B}_1\)</span> and <span class="math inline">\(\mathcal{A}\)</span>.</em></p>
<p>To define the non-triviality of a Bloom filter formally, notice that it is always desirable to minimize the memory use of the Bloom filter. Let <span class="math inline">\(\mathbf{B}\)</span> be an <span class="math inline">\((n,\varepsilon)\)</span>-Bloom filter that uses <span class="math inline">\(m\)</span> bits of memory. It is shown<span class="citation" data-cites="carter1978exact">[3]</span> that the lower bound of <span class="math inline">\(m\)</span> is <span class="math inline">\(m \geq n \log \frac{1}{\varepsilon}\)</span>. Thus, we define</p>
<p><strong>Definition 3. (Minimal error)</strong> <em>Let <span class="math inline">\(\mathbf{B}\)</span> be an <span class="math inline">\((n,\varepsilon)\)</span>-Bloom filter. We say that <span class="math inline">\(\varepsilon_0 = 2^{-\frac{m}{n}}\)</span> is the minimal error of <span class="math inline">\(\mathbf{B}\)</span>.</em></p>
<p>As mentioned previously, a trivial construction of Bloom filters is a lookup table that stores <span class="math inline">\(S\)</span> precisely, in which case, the memory use <span class="math inline">\(m=\log \binom{u}{n} \approx n \log(\frac{u}{n})\)</span>, thus by using the bound <span class="math inline">\(m \geq n \log \frac{1}{\varepsilon}\)</span>, a construction is trivial if <span class="math inline">\(\varepsilon &gt; \frac{n}{u}\)</span>. On the other hand, if <span class="math inline">\(u\)</span> is super-polynomial in <span class="math inline">\(n\)</span>, then <span class="math inline">\(\varepsilon\)</span> is negligible in <span class="math inline">\(n\)</span> and every polynomial-time adversary has only negligible probability to find any false positive, therefore for such <span class="math inline">\(S \subset U\)</span>, the Bloom filter must be trivial. Notice that <span class="math inline">\(\varepsilon_o \leq \varepsilon\)</span>, we then define</p>
<p><strong>Definition 4. (Non-trivial Bloom filter)</strong> <em>Let <span class="math inline">\(\mathbf{B}\)</span> be an <span class="math inline">\((n,\varepsilon)\)</span>-Bloom filter and let <span class="math inline">\(\varepsilon_0\)</span> be the minimal error of <span class="math inline">\(\mathbf{B}\)</span>. We say that <span class="math inline">\(\mathbf{B}\)</span> is non-trivial if there exists a constant <span class="math inline">\(c \geq 1\)</span> such that <span class="math inline">\(\varepsilon_0 &gt; \max\{\frac{n}{u},\frac{1}{n^c}\}\)</span>.</em></p>
</section>
<section id="resilient-bloom-filters-and-one-way-functions" class="level1">
<h1>3. Resilient Bloom Filters and One-Way Functions</h1>
<p>We now show that the existence of adversarial resilient Bloom filters depends on the existence of one-way functions, that is, if any non-trivial, strongly resilient Bloom filter exists, then one-way functions also exist.</p>
<p><strong>Theorem 5.</strong> <em>Let <span class="math inline">\(\mathbf{B}=(\mathbf{B}_1,\mathbf{B}_2)\)</span> be any non-trivial Bloom filter (with a steady representation) of <span class="math inline">\(n\)</span> elements that uses <span class="math inline">\(m\)</span> bits of memory, and let <span class="math inline">\(\varepsilon_0\)</span> be the minimal error of <span class="math inline">\(\mathbf{B}\)</span>. If <span class="math inline">\(\mathbf{B}\)</span> is <span class="math inline">\((n,t,\varepsilon)\)</span>-adversarial resilient for <span class="math inline">\(t=\mathcal{O}(\frac{m}{\varepsilon_0^2})\)</span>, then one-way functions exist.</em></p>
<p><strong>Proof.</strong> First we assume that one-way functions do not exist, then we show that we can construct a polynomial-time adversary <span class="math inline">\(\mathcal{A}\)</span> such that <span class="math display">\[\Pr[\mathsf{Challenge}_{\mathcal{A},\mathbf{B},t}(\lambda) = 1] &gt; \varepsilon\]</span> given a fixed value <span class="math inline">\(\varepsilon\)</span>. That is, <span class="math inline">\(\mathbf{B}\)</span> cannot be <span class="math inline">\((n,t,\varepsilon)\)</span>-adversarial resilient.</p>
<p>Define the following function <span class="math inline">\(f\)</span>: <span class="math display">\[f(S,r,x_1,\dots,x_t) = (x_1,\dots,x_t,\mathbf{B}_2(M,x_1),\dots,\mathbf{B}_2(M,x_t))\]</span> where <span class="math inline">\(S\)</span> is a set of size <span class="math inline">\(n\)</span>, <span class="math inline">\(r\)</span> is the number of bits used by the randomness of <span class="math inline">\(\mathbf{B}_1\)</span>, <span class="math inline">\(M\)</span> is a representation of <span class="math inline">\(S\)</span> generated by <span class="math inline">\(\mathbf{B}_1\)</span>, and <span class="math inline">\(t=\frac{200m}{\varepsilon_0}\)</span>. Clearly, <span class="math inline">\(f\)</span> is polynomial-time computable.</p>
<p>Since <span class="math inline">\(f\)</span> is not a one-way function (under the assumption that one-way functions do not exist), there is also an algorithm that can invert <span class="math inline">\(f\)</span> efficiently. Thus we have,</p>
<p><strong>Claim 6.</strong> <em>Assume that one-way functions do not exist, there exists a polynomial-time algorithm <span class="math inline">\(\mathcal{A}\)</span> that inverts <span class="math inline">\(f\)</span> with a failure probability of at most <span class="math inline">\(\frac{1}{100}\)</span>:</em> <span class="math display">\[\Pr[f(\mathcal{A}(f(S,r,x_1,\dots,x_t))) \neq f(S,r,x_1,\dots,x_t)] &lt; \frac{1}{100}\]</span></p>
<p><strong>Proof.</strong> Because <span class="math inline">\(f\)</span> is not a one-way function, there exists<span class="citation" data-cites="katz2014introduction">[4]</span> an algorithm <span class="math inline">\(\mathcal{A}&#39;\)</span> such that <span class="math inline">\(\Pr[\mathsf{Invert}_{\mathcal{A}&#39;,f}(n) = 1] \geq \frac{1}{p(n)}\)</span>, where <span class="math inline">\(p(n)\)</span> is polynomial in <span class="math inline">\(n\)</span>. Construct an algorithm <span class="math inline">\(\mathcal{A}\)</span> that runs <span class="math inline">\(\mathcal{A}&#39;\)</span> individually for <span class="math inline">\(\lceil\frac{\log 100}{\log(p(n)) - \log(p(n)-1)}\rceil\)</span> times, so we have the total failure probability <span class="math inline">\(\Pr[f(\mathcal{A}(f(S,r,x_1,\dots,x_t))) \neq f(S,r,x_1,\dots,x_t)] &lt; \left(1-\frac{1}{p(n)}\right)^{\lceil\frac{\log 100}{\log(p(n)) - \log(p(n)-1)}\rceil} \leq \frac{1}{100}\)</span> <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>Using <span class="math inline">\(\mathcal{A}\)</span>, construct the following probabilistic polynomial-time algorithm <span class="math inline">\(\mathsf{Attack}\)</span>:</p>
<blockquote style="background:gainsboro; border-radius:1em; padding:.25em .5em;">
<p><strong>The Algorithm <span class="math inline">\(\mathsf{Attack}\)</span></strong></p>
<p><span class="math inline">\(\mathsf{Attack}\)</span> is given oracle access to the query algorithm <span class="math inline">\(\mathbf{B}_2(M,\cdot)\)</span>, and gets <span class="math inline">\(1^\lambda\)</span> as input.</p>
<ol type="1">
<li>For <span class="math inline">\(i \in \{1,\dots,t\}\)</span>, sample <span class="math inline">\(x_i \in U\)</span> uniformly, and query <span class="math inline">\(y_i = \mathbf{B}_2(M,x_i)\)</span>.</li>
<li>Run <span class="math inline">\(\mathcal{A}\)</span> (the inverter of <span class="math inline">\(f\)</span>) and get <span class="math inline">\((S&#39;,r&#39;,x_1,\dots,x_t) \leftarrow \mathcal{A}(x_1,\dots,x_t,y_1,\dots,y_t)\)</span>.</li>
<li>Compute <span class="math inline">\(M&#39; \overset{r&#39;}{\leftarrow} \mathbf{B}_1(1^\lambda,S&#39;)\)</span>, using <span class="math inline">\(r&#39;\)</span> as the randomness bits in the initialization.</li>
<li>For <span class="math inline">\(k=1,\dots,\frac{100}{\varepsilon_0}\)</span>, do:
<ol type="a">
<li>Sample <span class="math inline">\(x^* \in U\)</span> uniformly.</li>
<li>If <span class="math inline">\(\mathbf{B}_2(M&#39;,x^*)=1\)</span> and <span class="math inline">\(x^* \not\in \{x_1,\dots,x_t\}\)</span>, output <span class="math inline">\(x^*\)</span> and HALT.</li>
</ol></li>
<li>Sample <span class="math inline">\(x^* \in U\)</span> uniformly, and output <span class="math inline">\(x^*\)</span>.</li>
</ol>
</blockquote>
<p><strong>Claim 7.</strong> <em>Assume that <span class="math inline">\(\mathcal{A}\)</span> inverts <span class="math inline">\(f\)</span> successfully. For any representation <span class="math inline">\(M\)</span>, the probability such that there exists a representation <span class="math inline">\(M&#39;\)</span> that for <span class="math inline">\(i \in \{1,\dots,t\}\)</span>, <span class="math inline">\(\mathbf{B}_2(M,x_i)=\mathbf{B}_2(M&#39;,x_i)\)</span>, and that the error rate <span class="math inline">\(\Pr[\mathbf{B}_2(M,x) \neq \mathbf{B}_2(M&#39;,x)] &gt; \frac{\varepsilon_0}{100}\)</span> is at most <span class="math inline">\(\frac{1}{100}\)</span>.</em></p>
<p><strong>Proof.</strong> From the error rate of any <span class="math inline">\(x\)</span> and the independence of the choice of <span class="math inline">\(x_i\)</span>, we get <span class="math display">\[\Pr[\forall i \in \{1,\dots,t\} : \mathbf{B}_2(M,x_i) = \mathbf{B}_2(M&#39;,x_i)] \leq \left(1 - \frac{\varepsilon_0}{100}\right)^t\]</span></p>
<p>Since the Bloom filter uses <span class="math inline">\(m\)</span> bits of memory, there are <span class="math inline">\(2^m\)</span> possible representations as candidates for <span class="math inline">\(M&#39;\)</span>. Thus, by union bound, <span class="math display">\[\Pr[\exists M&#39; \ \forall i \in \{1,\dots,t\} : \mathbf{B}_2(M,x_i) = \mathbf{B}_2(M&#39;,x_i)] \leq 2^m \left(1 - \frac{\varepsilon_0}{100}\right)^t \leq \frac{1}{100}\]</span></p>
<p>Since <span class="math inline">\(\mathcal{A}\)</span> is assumed to invert <span class="math inline">\(f\)</span> successfully, it must output a representation <span class="math inline">\(M&#39;\)</span> such that for <span class="math inline">\(i \in \{1,\dots,t\}\)</span>, <span class="math inline">\(\mathbf{B}_2(M,x_i)=\mathbf{B}_2(M&#39;,x_i)\)</span>. Therefore, the above bound holds. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>Define <span class="math inline">\(\mu(M)=\Pr_{x \in U}[\mathbf{B}_2(M,x)=1]\)</span> as the positive rate over <span class="math inline">\(U\)</span>, we now show that for almost all possible representations <span class="math inline">\(M\)</span> generated from set <span class="math inline">\(S\)</span> and randomness <span class="math inline">\(r\)</span>, it holds true that <span class="math inline">\(\mu(M) &gt; \frac{\varepsilon_0}{8}\)</span>, with only a negligible probability of error:</p>
<p><strong>Claim 8.</strong> <span class="math inline">\(\Pr_S[\exists r : \mu(M_r^S) \leq \frac{\varepsilon}{8}] \leq 2^{-n}\)</span>.</p>
<p><strong>Proof.</strong> Let <span class="math inline">\(\mathsf{BAD}\)</span> be the set of all sets <span class="math inline">\(S\)</span> such that there exists an <span class="math inline">\(r\)</span> such that <span class="math inline">\(\mu(M_r^S) \leq \frac{\varepsilon_0}{8}\)</span>. Given <span class="math inline">\(S \in \mathsf{BAD}\)</span>, let <span class="math inline">\(\hat{S}\)</span> be the set of all elements <span class="math inline">\(x\)</span> such that <span class="math inline">\(\mathbf{B}_2(M_r^S,x)=1\)</span>, then <span class="math inline">\(|\hat{S}| \leq \frac{\varepsilon_0}{8} \cdot u\)</span>. Notice that we can encode the set <span class="math inline">\(S\)</span> using the representation <span class="math inline">\(M_r^S\)</span> while specifying <span class="math inline">\(S\)</span> from all subsets of <span class="math inline">\(\hat{S}\)</span> of size <span class="math inline">\(n\)</span>, and the encoding bits must be no less than <span class="math inline">\(\log|\mathsf{BAD}|\)</span> (which is the number of bits required to encode <span class="math inline">\(|\mathsf{BAD}|\)</span>): <span class="math display">\[\log|\mathsf{BAD}| \leq m + \log\binom{\frac{\varepsilon_0 u}{8}}{n}
\leq m + n \log\left(\frac{\varepsilon_0 u}{8}\right) - n \log n + 2n
\leq -n + \log\binom{u}{n}
\]</span> thus <span class="math inline">\(|\mathsf{BAD}| \leq 2^{-n}\binom{u}{n}\)</span>. Since the number of sets <span class="math inline">\(S\)</span> is <span class="math inline">\(\binom{u}{n}\)</span>, <span class="math inline">\(\Pr_S[\exists r : \mu(M_r^S) \leq \frac{\varepsilon}{8}] \leq 2^{-n}\)</span>. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p><strong>Claim 9.</strong> <em>Assume that <span class="math inline">\(\Pr[\mathbf{B}_2(M,x) \neq \mathbf{B}_2(M&#39;,x)] \leq \frac{\varepsilon_0}{100}\)</span> and that <span class="math inline">\(\mu(M) &gt; \frac{\varepsilon_0}{8}\)</span>. The probability that <span class="math inline">\(\mathsf{Attack}\)</span> does not halt on Step 4 is at most <span class="math inline">\(\frac{1}{100}\)</span>.</em></p>
<p><strong>Proof.</strong> It follows directly from the assumptions that <span class="math inline">\(\mu(M&#39;) &gt; \frac{\varepsilon_0}{8} - \frac{\varepsilon_0}{100} &gt; \frac{\varepsilon_0}{10}\)</span>. For convenience, let <span class="math inline">\(\mathcal{X} = \{x_1,\dots,x_t\}\)</span> and <span class="math inline">\(\hat{S&#39;} = \{x : \mathbf{B}_2(M&#39;,x)=1\}\)</span>. We have that</p>
<p><span class="math display">\[E[|\hat{S&#39;} \cap \mathcal{X}|] = t \cdot \mu(M&#39;) &gt; \frac{200m}{\varepsilon_0} \cdot \frac{\varepsilon_0}{10} = 20m\]</span></p>
<p>By Chernoff bound with a probability of at least <span class="math inline">\((1-e^{-\Omega(m)})\)</span> we have that <span class="math inline">\(|\hat{S&#39;} \cap \mathcal{X}| &lt; 40m\)</span>, <span class="math display">\[|\hat{S&#39;} \backslash \mathcal{X}|=|\hat{S&#39;}|-|\hat{S&#39;} \cap \mathcal{X}| &gt; |\hat{S&#39;}| - 40m \geq \frac{\varepsilon_0 u}{10} - 40m\]</span></p>
<p><em>Case 1.</em> <span class="math inline">\(u=n^d\)</span> (<span class="math inline">\(d\)</span> is a constant). We show that <span class="math inline">\(|\hat{S&#39;} \backslash \mathcal{X}| \geq 1\)</span>, so that an exhaustive search over the universal set <span class="math inline">\(U\)</span> is efficient and guaranteed to find an element <span class="math inline">\(x^*\)</span> in <span class="math inline">\(\hat{S&#39;} \backslash \mathcal{X}\)</span>. Let <span class="math inline">\(c\)</span> be a constant such that <span class="math inline">\(\varepsilon_0 &gt; \frac{1}{n^c}\)</span>, <span class="math inline">\(\varepsilon_0 &lt; \frac{1}{n^{c-1}}\)</span>. We have <span class="math inline">\(\frac{u}{n} = n^{d-1} \geq \frac{1}{\varepsilon_0} &gt; n^{c-1}\)</span>, then <span class="math inline">\(d-c&gt;1\)</span>. Moreover, <span class="math inline">\(m \leq n \log\frac{u}{n} \leq nd \log n\)</span>, thus, <span class="math display">\[|\hat{S&#39;} \backslash \mathcal{X}| \geq \frac{\varepsilon_0 u}{10} - 40m
\geq \frac{n^{d-c}}{10} - 40nd \log n &gt; 1\]</span></p>
<p><em>Case 2.</em> <span class="math inline">\(u\)</span> is super-polynomial in <span class="math inline">\(n\)</span>. We show that the fraction of <span class="math inline">\(|\hat{S&#39;} \backslash \mathcal{X}|\)</span> is large enough so that sampling can find an <span class="math inline">\(x^*\)</span>, with only a small failure probability. Since <span class="math inline">\(\frac{\varepsilon_0}{20}\)</span> is polynomial in <span class="math inline">\(\frac{1}{n}\)</span> but <span class="math inline">\(\frac{40m}{u} \leq \frac{40n \log u}{u}\)</span> is negligible, we get that <span class="math inline">\(\frac{\varepsilon_0}{20} &gt; \frac{40m}{u}\)</span>. It follow from <span class="math inline">\(\frac{|\hat{S&#39;} \backslash \mathcal{X}|}{u} &gt; \frac{\varepsilon_0}{10} - \frac{40m}{u}\)</span> that <span class="math inline">\(\frac{|\hat{S&#39;} \backslash \mathcal{X}|}{u} &gt; \frac{\varepsilon_0}{20}\)</span>. Thus, the probability of sampling <span class="math inline">\(x \not\in \hat{S&#39;} \backslash \mathcal{X}\)</span> in all <span class="math inline">\(k\)</span> attempts is bounded by <span class="math display">\[\left(1-\frac{\varepsilon_0}{20}\right)^k
= \left(1-\frac{\varepsilon_0}{20}\right)^\frac{100}{\varepsilon_0}
&lt; \frac{1}{100}
\]</span></p>
<p>In both cases, the probability that <span class="math inline">\(\mathsf{Attack}\)</span> fails to find <span class="math inline">\(x^*\)</span> and halt on Step 4 is less than <span class="math inline">\(\frac{1}{100}\)</span>. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p><strong>Claim 10.</strong> <span class="math inline">\(\Pr[\mathbf{B}(M&#39;,x^*)=1; \mathbf{B}_2(M,x^*)=0] \leq \frac{1}{100}\)</span></p>
<p><strong>Proof.</strong> This follows from the assumption that <span class="math inline">\(\Pr[\mathbf{B}_2(M,x) \neq \mathbf{B}_2(M&#39;,x)] \leq \frac{\varepsilon}{100}\)</span>. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p>Consider <strong>Claim 6, 7, 8 &amp; 10</strong> which cover all cases that <span class="math inline">\(\mathsf{Attack}\)</span> fails, each happens only if the respective assumptions hold, so they provide an upper bound of failure probability. Taking a union bound, we have the total failure probability is at most <span class="math inline">\(\frac{4}{100}\)</span>. Thus, we have constructed a polynomial-time adversary <span class="math inline">\(\mathsf{Attack}\)</span> such that <span class="math display">\[\Pr[\mathsf{Challenge}_{\mathsf{Attack},\mathbf{B},t}(\lambda) = 1] &gt; \varepsilon \geq 1-\frac{4}{100}\]</span> therefore <span class="math inline">\(\mathbf{B}\)</span> cannot be <span class="math inline">\((n,t,\varepsilon)\)</span>-adversarial resilient, which is a contradiction implying that such adversarial resilient Bloom filters do not exist, under the assumption that one-way functions do not exist. By modus tollens, we have that if non-trivial <span class="math inline">\((n,t,\varepsilon)\)</span>-adversarial resilient Bloom filters exist, then one-way functions exist. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
</section>
<section id="further-reading" class="level1">
<h1>4. Further Reading</h1>
<p>In <strong>Theorem 5</strong> we showed that the existence of adversarial resilient Bloom filters implies that one-way functions exist. Furthermore, assume that adversarial resilient Bloom filters exist (thus one-way functions exist), it can be shown that pseudorandom permutations may be used to construct non-trivial, strongly adversarial resilient Bloom filters. We have</p>
<p><strong>Proposition 11. (Construction using pseudorandom permutations)</strong><a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> <em>Let <span class="math inline">\(\mathbf{B}\)</span> be an <span class="math inline">\((n,\varepsilon)\)</span>-Bloom filter using <span class="math inline">\(m\)</span> bits of memory. If pseudorandom permutations exist, then for any security parameter <span class="math inline">\(\lambda\)</span>, there exists an <span class="math inline">\((n,\varepsilon+\mathsf{negl}(\lambda))\)</span>-strongly resilient Bloom filter that uses <span class="math inline">\(m&#39;=m+\lambda\)</span> bits of memory.</em></p>
<p><strong>Unsteady representation and computationally unbounded adversary.</strong> The above discussion about Bloom filters assumes that steady representation is used, that is, the query algorithm <span class="math inline">\(\mathbf{B}_2\)</span> is deterministic. Some implementations allow <span class="math inline">\(\mathbf{B}_2\)</span> to change the internal representation thus querying can also be probabilistic. Further results regarding <em>unsteady representations</em> may be found in <span class="citation" data-cites="naor2015bloom">[5]</span>, with the ACD<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> framework proposed in <span class="citation" data-cites="naor2006learning">[6]</span>. Moreover, some results are shown to hold even for <em>computationally unbounded adversaries</em>.</p>
<p><strong>Bloom filters and secrecy.</strong> Bloom filters, like hash functions, are not designed as encryption schemes. Thus, even adversarial resilient Bloom filters may leak considerable information in an unintended way. As a concrete example, in Bitcoin lightweight SPV clients which rely on Bloom filters to store users’ Bitcoin addresses, an adversary can efficiently distinguish information about these addresses. See <span class="citation" data-cites="gervais2014privacy">[7]</span> for a discussion on this interesting scenario.</p>
</section>
<section id="references" class="level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-maggs2015algorithmic">
<p>[1] B. M. Maggs and R. K. Sitaraman, “Algorithmic nuggets in content delivery,” <em>ACM SIGCOMM Computer Communication Review</em>, vol. 45, no. 3, pp. 52–66, 2015. </p>
</div>
<div id="ref-benjaminattacks">
<p>[2] R. Benjamin and E. K. Yasmine, “Attacks on bitcoin,” 2015. </p>
</div>
<div id="ref-carter1978exact">
<p>[3] L. Carter, R. Floyd, J. Gill, G. Markowsky, and M. Wegman, “Exact and approximate membership testers,” in <em>Proceedings of the tenth annual acm symposium on theory of computing</em>, 1978, pp. 59–65. </p>
</div>
<div id="ref-katz2014introduction">
<p>[4] J. Katz and Y. Lindell, “Introduction to modern cryptography,” 2014. </p>
</div>
<div id="ref-naor2015bloom">
<p>[5] M. Naor and E. Yogev, “Bloom filters in adversarial environments,” in <em>Annual cryptology conference</em>, 2015, pp. 565–584. </p>
</div>
<div id="ref-naor2006learning">
<p>[6] M. Naor and G. N. Rothblum, “Learning to impersonate,” in <em>Proceedings of the 23rd international conference on machine learning</em>, 2006, pp. 649–656. </p>
</div>
<div id="ref-gervais2014privacy">
<p>[7] A. Gervais, S. Capkun, G. O. Karame, and D. Gruber, “On the privacy provisions of bloom filters in lightweight bitcoin clients,” in <em>Proceedings of the 30th annual computer security applications conference</em>, 2014, pp. 326–335. </p>
</div>
</div>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>We will only consider polynomial-time adversaries here.<a href="#fnref1" class="footnoteBack">↩</a></p></li>
<li id="fn2"><p>For this reason, a <em>yes</em>-answer is also referred to as a <em>maybe</em>-answer in some texts.<a href="#fnref2" class="footnoteBack">↩</a></p></li>
<li id="fn3"><p>A CDN (Content Delivery Network) is a globally distributed network of proxy servers deployed in multiple data centers, in order to serve cached content to end-users with high availability and high performance.<a href="#fnref3" class="footnoteBack">↩</a></p></li>
<li id="fn4"><p>In this work, we will consider only steady representations.<a href="#fnref4" class="footnoteBack">↩</a></p></li>
<li id="fn5"><p>The security parameter <span class="math inline">\(\lambda\)</span> is implied in the initialization algorithm <span class="math inline">\(\mathbf{B}_1\)</span>, thus we denote it as <span class="math inline">\(\mathbf{B}_1(1^\lambda,S)\)</span>.<a href="#fnref5" class="footnoteBack">↩</a></p></li>
<li id="fn6"><p>To strengthen our definition, we assume that the adversary also gets the set <span class="math inline">\(S\)</span>, and it is important that the adversary can hardly find any false positive even if given <span class="math inline">\(S\)</span>.<a href="#fnref6" class="footnoteBack">↩</a></p></li>
<li id="fn7"><p>A proof of this can be found in <span class="citation" data-cites="naor2015bloom">[5]</span>.<a href="#fnref7" class="footnoteBack">↩</a></p></li>
<li id="fn8"><p>Learning model of adaptively changing distributions (ACD).<a href="#fnref8" class="footnoteBack">↩</a></p></li>
</ol>
</section>

]]>
    </content>
  </entry>
  <entry>
    <title>The Decisional Hardness</title>
    <link rel="alternate" type="text/html" href="https://www.soimort.org/mst/1" />
    <id>tag:www.soimort.org,2017:/mst/1</id>
    <published>2016-11-01T00:00:00+01:00</published>
    <updated>2016-11-20T00:00:00+01:00</updated>
    <author>
      <name>Mort Yao</name>
    </author>
    
    <content type="html" xml:lang="en" xml:base="https://www.soimort.org/">
<![CDATA[
<p><p style='background-color:yellow'> <strong>(20 Nov 2016) Correction:</strong> P ≠ NP is not sufficient to imply that one-way functions exist. See <a href="#p-versus-np-problem-and-one-way-functions">P versus NP problem and one-way functions</a>.</p></p>
<hr />
<p><strong>Intro.</strong> Starting from November, I’ll summarize my study notes on <a href="https://wiki.soimort.org">wiki</a> into weekly blog posts. I always wanted to keep my study progress on track; I feel that it’s hard even to convince myself without visible footprints.</p>
<p>So here we have the first episode. (Hopefully it won’t be the last one)</p>
<hr />
<p>Asymptotic notation is an important tool in analyzing the time/space efficiency of algorithms.</p>
<ul>
<li><a href="https://wiki.soimort.org/math/calculus/limit/">Limit</a>
<ul>
<li>Formal definition of limit (the (ε, δ)-definition) in calculus. Note that limits involving infinity are closely related to asymptotic analysis. In addition to basic limit rules, L’Hôpital’s rule is also relevant.</li>
</ul></li>
<li><a href="https://wiki.soimort.org/algo/asymptotic-notation/">Asymptotic notation</a>
<ul>
<li>Introduction to the Bachmann–Landau notation family (among them are the most widely-used Big O notation and Big Theta notation).</li>
<li>Master theorem is used to find the asymptotic bound for recurrence. This is particularly helpful when analyzing recursive algorithms (e.g., binary search, merge sort, tree traversal).</li>
<li>Based on common orders of asymptotic running time using Big O notation, we can categorize algorithms into various classes of time complexities (among them are P, DLOGTIME, SUBEXP and EXPTIME). Note that we have not formally defined the word “algorithm” and “complexity class” yet.</li>
</ul></li>
</ul>
<p>For decision problems, we now formally define the time complexity classes P and NP, and propose the hardness of NP-complete problems, which plays an indispensable role in the study of algorithm design and modern cryptography.</p>
<ul>
<li><a href="https://wiki.soimort.org/comp/language/">Formal language</a>
<ul>
<li>Formal definition of language. We will revisit this when studying formal grammars like the context-free grammar and parsing techniques for compilers. For now, it suffices to know that binary string is a common encoding for all kinds of problems (especially, decision problems).</li>
</ul></li>
<li><a href="https://wiki.soimort.org/comp/decidability/">Decidability</a>
<ul>
<li>Among all abstract problems, we are mostly interested in decision problems.</li>
<li>The decidability of a language depends on whether there exists an algorithm that decides it.</li>
</ul></li>
<li><a href="https://wiki.soimort.org/comp/reducibility/">Reducibility</a>
<ul>
<li>Polynomial-time reduction is a commonly used technique that maps one language to another.</li>
<li>What is a hard language for a complexity class; what is a complete language for a complexity class.</li>
</ul></li>
<li><a href="https://wiki.soimort.org/comp/complexity/time/">Time complexity</a>
<ul>
<li>Encodings of concrete problems matter. Normally we would choose a “standard encoding” for our language of interest.</li>
<li>Polynomial-time algorithms are considered to be efficient and languages which have polynomial-time algorithms that decide them are considered tractable.</li>
<li>P is the time complexity class of all problems that are polynomial-time solvable.</li>
<li>NP is the time complexity class of all problems that are polynomial-time verifiable.</li>
</ul></li>
<li><a href="https://wiki.soimort.org/comp/complexity/time/npc/">NP-completeness</a>
<ul>
<li>The set of languages that are complete for the complexity class NP, that is, the “hardest problems” in NP.</li>
<li>NP-complete problems are central in answering the open question whether P = NP.</li>
<li>We (informally) show that every NP problem is polynomial-time reducible to CIRCUIT-SAT, and that CIRCUIT-SAT is NP-complete.</li>
<li>There are other problems (SAT, 3-CNF-SAT, CLIQUE, VERTEX-COVER, HAM-CYCLE, TSP, SUBSET-SUM) polynomial-time reducible from one to another, thus they are also shown to be NP-complete.</li>
</ul></li>
</ul>
<p><strong>Computational hardness assumption P ≠ NP.</strong> Although it is still an open proposition, many believe that P ≠ NP. Notably, if P ≠ NP holds true,</p>
<ol type="1">
<li>If a decision problem is polynomial-time unsolvable in general case, we should strive to find approximations or randomized algorithms; exact algorithms cannot be run in worst-case polynomial time thus may not be efficient. This applies to optimization problems too.</li>
<li><del>
One-way functions exist, which implies that pseudorandom generators and functions exist. Consequently, many cryptographic constructions (private-key encryption, MACs, etc.) are provably computationally secure.
</del></li>
</ol>
<p><p style='background-color:yellow'> (I stand corrected: There is no such a known proof showing that P ≠ NP implies the existence of one-way functions. However, reversely, the existence of one-way functions implies that P ≠ NP. There is an informal argument given by Peter Shor on StackExchange<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, rephrased in the section <a href="#p-versus-np-problem-and-one-way-functions">P versus NP problem and one-way functions</a>.)</p></p>
<p>Later we will cover the notion of security in cryptography, so there is a refresher of basic probability: (Probability is also considerably used in analyzing the behaviors of non-deterministic algorithms, hash functions, etc.)</p>
<ul>
<li><a href="https://wiki.soimort.org/math/probability/">Probability</a>
<ul>
<li>An intuitive introduction to basic probability theory based on Kolmogorov’s axioms, including the union bound (Boole’s inequality) and its generalized form Bonferroni inequalities, the conditional probability and Bayes’ theorem. We will revisit the notion of probability space when coming to measure theory.</li>
</ul></li>
</ul>
<p>Plan for next week:</p>
<ul>
<li><strong>(Algorithms)</strong> More involved NP-complete problems. Exact algorithms. Approximation algorithms. Probabilistic algorithms.</li>
<li><strong>(Cryptography)</strong> Information-theoretic/computational security (semantic security, IND, IND-CPA, IND-CCA). Private-key encryption. Message authentication codes. Hash functions. Theoretical constructions (one-way functions, pseudorandomness). Practical constructions (Feistel network, substitution-permutation network, DES, AES).</li>
</ul>
<section id="p-versus-np-problem-and-one-way-functions" class="level2">
<h2>P versus NP problem and one-way functions</h2>
<p>Consider the following map: <span class="math display">\[f : (x, r) \to s\]</span> where <span class="math inline">\(x\)</span> is an arbitrary bit string, <span class="math inline">\(r\)</span> is a string of random bits, and <span class="math inline">\(s\)</span> is an instance of a <span class="math inline">\(k\)</span>-SAT problem having <span class="math inline">\(x\)</span> as a planted solution, while the randomness of <span class="math inline">\(r\)</span> determines uniquely which <span class="math inline">\(k\)</span>-SAT problem to choose.</p>
<p>If we can invert the above function <span class="math inline">\(f\)</span> (in polynomial time), we must already have solved the corresponding <span class="math inline">\(k\)</span>-SAT problem <span class="math inline">\(s\)</span> with a planted solution <span class="math inline">\(x\)</span>. <span class="math inline">\(k\)</span>-SAT problems are known to be NP-complete, and inverting such a function would be as hard as solving a <span class="math inline">\(k\)</span>-SAT problem with a planted solution, that is, inverting <span class="math inline">\(f\)</span> <em>at one point</em> can be hard. Clearly, should we have a one-way function, then inverting it is guaranteed to be no easier than inverting <span class="math inline">\(f\)</span>.</p>
<p>So what does it mean if P ≠ NP? We know that <span class="math inline">\(k\)</span>-SAT problem is hard to solve in its <em>worst case</em>, so function <span class="math inline">\(f\)</span> can be made as hard to invert as solving a <span class="math inline">\(k\)</span>-SAT problem in its <em>worst case</em>. However, we don’t know whether it’s possible to have a class <span class="math inline">\(\mathcal{S}\)</span> of <span class="math inline">\(k\)</span>-SAT problems with planted solutions that are as hard as general-case <span class="math inline">\(k\)</span>-SAT problems. If such a class <span class="math inline">\(\mathcal{S}\)</span> exists, then given any <span class="math inline">\(s \in \mathcal{S}\)</span>, no probabilistic polynomial-time algorithm is able to get <span class="math inline">\(x\)</span> with a non-negligible probability, so we can conclude that <span class="math inline">\(f\)</span> is indeed a one-way function. <a href="#ref-selman1992survey"><span class="citation" data-cites="selman1992survey">[1]</span></a></p>
<p><strong>Problem 1.1.</strong> Does there exist a class <span class="math inline">\(\mathcal{S}\)</span> of <span class="math inline">\(k\)</span>-SAT problems with planted solutions, such that every <span class="math inline">\(L \in \mathcal{S}\)</span> is NP-hard?</p>
<p><strong>Conjecture 1.2.</strong> <em>If <span class="math inline">\(\mathrm{P} \neq \mathrm{NP}\)</span>, then one-way functions exist.</em></p>
<p>On the other hand, assume that <span class="math inline">\(f\)</span> is a one-way function, so that one-way functions do exist, then this implies that <span class="math inline">\(k\)</span>-SAT problem is hard to solve (in its worse case) by a polynomial-time algorithm, thus we have P ≠ NP. By modus tollens, if P = NP, then no one-way function exists. <a href="#ref-abadi1990generating"><span class="citation" data-cites="abadi1990generating">[2]</span></a></p>
<p><strong>Theorem 1.3.</strong> <em>If one-way functions exist, then <span class="math inline">\(\mathrm{P} \neq \mathrm{NP}\)</span>.</em></p>
<p><em>Proof.</em> <em>(Sketch)</em> Let <span class="math inline">\(f : \{0,1\}^*\to\{0,1\}^*\)</span> be a one-way function. There is a polynomial-time algorithm <span class="math inline">\(M_f\)</span> that computes <span class="math inline">\(y=f(x)\)</span> for all <span class="math inline">\(x\)</span>, thus, there exists a polynomial-time computable circuit that outputs <span class="math inline">\(y=f(x)\)</span> for all <span class="math inline">\(x\)</span>.</p>
<p>Since <span class="math inline">\(f\)</span> is a one-way function, that is, for every probabilistic polynomial-time algorithm <span class="math inline">\(\mathcal{A}\)</span>, there is a negligible function <span class="math inline">\(\mathsf{negl}\)</span> such that <span class="math inline">\(\Pr[\mathsf{Invert}_{\mathcal{A},f}(n) = 1] \leq \mathsf{negl}(n)\)</span>, so we know that no <span class="math inline">\(\mathcal{A}\)</span> can fully compute <span class="math inline">\(f^{-1}(x)\)</span> for any given <span class="math inline">\(x\)</span>. <span class="math inline">\(\mathcal{A}\)</span> fully computes <span class="math inline">\(f^{-1}\)</span> if and only if it solves the corresponding <code>CIRCUIT-SAT</code> problems of the circuit in all cases. Thus, there must exist some <code>CIRCUIT-SAT</code> problems that cannot be decided by a polynomial-time algorithm, therefore, <span class="math inline">\(\mathrm{P} \neq \mathrm{NP}\)</span>. <p style='text-align:right !important;text-indent:0 !important;position:relative;top:-1em'>&#9632;</p></p>
<p><em>Remark 1.4.</em> If one can come up with a construction of the one-way function or a proof that such functions exist, then it holds true that <span class="math inline">\(\mathrm{P} \neq \mathrm{NP}\)</span>.</p>
</section>
<section id="references-and-further-reading" class="level2">
<h2>References and further reading</h2>
<p><strong>Books:</strong></p>
<p>T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, <em>Introduction to Algorithms</em>, 3rd ed.</p>
<p>M. Sipser, <em>Introduction to the Theory of Computation</em>, 3rd ed.</p>
<p>J. Katz and Y. Lindell, <em>Introduction to Modern Cryptography</em>, 2nd ed.</p>
<p><strong>Papers:</strong></p>
<div id="refs" class="references">
<div id="ref-selman1992survey">
<p>[1] A. L. Selman, “A survey of one-way functions in complexity theory,” <em>Mathematical Systems Theory</em>, vol. 25, no. 3, pp. 203–221, 1992. </p>
</div>
<div id="ref-abadi1990generating">
<p>[2] M. Abadi, E. Allender, A. Broder, J. Feigenbaum, and L. A. Hemachandra, “On generating solved instances of computational problems,” in <em>Proceedings on advances in cryptology</em>, 1990, pp. 297–310. </p>
</div>
</div>
</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="http://cstheory.stackexchange.com/a/8843/21291" class="uri">http://cstheory.stackexchange.com/a/8843/21291</a><a href="#fnref1" class="footnoteBack">↩</a></p></li>
</ol>
</section>

]]>
    </content>
  </entry>
</feed>
